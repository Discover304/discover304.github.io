<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>【机器学习】第二部分上：线性回归 | Yang's Harbor</title><meta name="keywords" content="学习,记录,Python,笔记,机器学习"><meta name="author" content="✨YangSier✨,hobart.yang@qq.com"><meta name="copyright" content="✨YangSier✨"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="线性模型线性模型是自然界最简单的模型之一，它描述了一个（或多个）自变量对另一个因变量的影响是呈简单的比例、线性关系.例如：  住房每平米单价为1万元，100平米住房价格为100万元，120平米住房为120万元； 一台挖掘机每小时挖$100m^3$沙土，工作4小时可以挖掘$400m^3$沙土.  线性模型在二维空间内表现为一条直线，在三维空间内表现为一个平面，更高维度下的线性模型很难用几何图形来表示">
<meta property="og:type" content="article">
<meta property="og:title" content="【机器学习】第二部分上：线性回归">
<meta property="og:url" content="https://discover304.top/2021/11/25/2021q4/103-1-ml-lr/index.html">
<meta property="og:site_name" content="Yang&#39;s Harbor">
<meta property="og:description" content="线性模型线性模型是自然界最简单的模型之一，它描述了一个（或多个）自变量对另一个因变量的影响是呈简单的比例、线性关系.例如：  住房每平米单价为1万元，100平米住房价格为100万元，120平米住房为120万元； 一台挖掘机每小时挖$100m^3$沙土，工作4小时可以挖掘$400m^3$沙土.  线性模型在二维空间内表现为一条直线，在三维空间内表现为一个平面，更高维度下的线性模型很难用几何图形来表示">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://image.discover304.top/ai/AI-cover-black-white.webp">
<meta property="article:published_time" content="2021-11-25T06:27:22.000Z">
<meta property="article:modified_time" content="2021-12-25T06:48:56.000Z">
<meta property="article:author" content="✨YangSier✨">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="记录">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.discover304.top/ai/AI-cover-black-white.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://discover304.top/2021/11/25/2021q4/103-1-ml-lr/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta name="google-site-verification" content="ilqpfk3vkgzDNNikz_V37-DOvRyi5wv4Hoi_eyBqvTg"/><meta name="msvalidate.01" content="49D9A50CCF9744E17274791468EDB517"/><meta name="baidu-site-verification" content="code-V24KosyVh1"/><meta name="360-site-verification" content="bd8859c3d74dfa3e8aeee9db30c94bd2"/><meta name="yandex-verification" content="f28ec9bbd50c56f5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-1849044985266192',
  enable_page_level_ads: 'true'
});</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8030f6052f2fed6a4704d96619f090d6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="/css/font.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"bottom","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":200,"languages":{"author":"Author: ✨YangSier✨","link":"Link: ","source":"Source: Yang's Harbor","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#ffc910","bgDark":"#02c3f6","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-25 14:48:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Yang's Harbor" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">261</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">90</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">29</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-link"></i><span> Connection</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i><span> Friends</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://aierlab.com"><i class="fa-fw fas fa-sitemap"></i><span> GroupSite</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-wrench"></i><span> Tools</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/diary"><i class="fa-fw fas fa-file-text"></i><span> Diary</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wandb.ai/"><i class="fa-fw fas fa-newspaper"></i><span> WandB</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Article</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://image.discover304.top/ai/AI-cover-black-white.webp)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yang's Harbor</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-link"></i><span> Connection</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i><span> Friends</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://aierlab.com"><i class="fa-fw fas fa-sitemap"></i><span> GroupSite</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-wrench"></i><span> Tools</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/diary"><i class="fa-fw fas fa-file-text"></i><span> Diary</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wandb.ai/"><i class="fa-fw fas fa-newspaper"></i><span> WandB</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Article</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【机器学习】第二部分上：线性回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-11-25T06:27:22.000Z" title="Created 2021-11-25 14:27:22">2021-11-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-12-25T06:48:56.000Z" title="Updated 2021-12-25 14:48:56">2021-12-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NoteBook/">NoteBook</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NoteBook/PythonNote/">PythonNote</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>22min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p>线性模型是自然界最简单的模型之一，它描述了一个（或多个）自变量对另一个因变量的影响是呈简单的比例、线性关系.例如：</p>
<ul>
<li>住房每平米单价为1万元，100平米住房价格为100万元，120平米住房为120万元；</li>
<li>一台挖掘机每小时挖$100m^3$沙土，工作4小时可以挖掘$400m^3$沙土.</li>
</ul>
<p>线性模型在二维空间内表现为一条直线，在三维空间内表现为一个平面，更高维度下的线性模型很难用几何图形来表示（称为超平面）.如下图所示：</p>
<p><img src="https://image.discover304.top/ai/lieaner_1.png"></p>
<center><font size=2>二维空间下线性模型表现为一条直线</font></center>

<p><img src="https://image.discover304.top/ai/lieaner_2.png"></p>
<center><font size=2>三维空间下线性模型表现为一个平面</font></center>
线性回归是要根据一组输入值和输出值（称为样本），寻找一个线性模型，能最佳程度上拟合于给定的数值分布，从而再给定新的输入时预测输出.样本如下表所示：

<table>
<thead>
<tr>
<th>输入(x)</th>
<th>输出(y)</th>
</tr>
</thead>
<tbody><tr>
<td>0.5</td>
<td>5.0</td>
</tr>
<tr>
<td>0.6</td>
<td>5.5</td>
</tr>
<tr>
<td>0.8</td>
<td>6.0</td>
</tr>
<tr>
<td>1.1</td>
<td>6.8</td>
</tr>
<tr>
<td>1.4</td>
<td>6.8</td>
</tr>
</tbody></table>
<p>根据样本拟合的线性模型如下图所示：</p>
<p><img src="https://image.discover304.top/ai/linear_3.png" alt="linear_3"></p>
<h2 id="线性模型定义"><a href="#线性模型定义" class="headerlink" title="线性模型定义"></a>线性模型定义</h2><p>设给定一组属性$x, x&#x3D;(x_1;x_2;…;x_n)$，线性方程的一般表达形式为：<br>$$<br>y &#x3D; w_1x_1 + w_2x_2 + w_3x_3 + … + w_nx_n + b<br>$$<br>写成向量形式为：<br>$$<br>y &#x3D; w^Tx + b<br>$$<br>其中，$w&#x3D;(w_1;w_2;…;w_n), x&#x3D;(x_1;x_2;…;x_n)$，w和b经过学习后，模型就可以确定. 当自变量数量为1时，上述线性模型即为平面下的直线方程：<br>$$<br>y &#x3D; wx + b<br>$$<br>线性模型形式简单、易于建模，却蕴含着机器学习中一些重要的基本思想. 许多功能强大的非线性模型可以在线性模型基础上引入层级结构或高维映射而得. 此外，由于$w$直观表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性.例如，判断一个西瓜是否为好瓜，可以用如下表达式来判断：<br>$$<br>f_{\text{好瓜}}(x) &#x3D; 0.2x_{\text{色泽}} + 0.5x_{\text{根蒂}} + 0.3x_{\text{敲声}} + 1<br>$$<br>上述公式可以解释为，一个西瓜是否为好瓜，可以通过色泽、根蒂、敲声等因素共同判断，其中根蒂最重要(权重最高)，其次是敲声和色泽.</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>在二维平面中，给定两点可以确定一条直线.但在实际工程中，可能有很多个样本点，无法找到一条直线精确穿过所有样本点，只能找到一条与样本”足够接近“或”距离足够小“的直线，近似拟合给定的样本.如下图所示：</p>
<p><img src="https://image.discover304.top/ai/linear_4.png" alt="linear_4"></p>
<p>如何确定直线到所有样本足够近呢？可以使用损失函数来进行度量.</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数用来度量真实值（由样本中给出）和预测值（由模型算出）之间的差异.损失函数值越小，表明模型预测值和真实值之间差异越小，模型性能越好；损失函数值越大，模型预测值和真实值之间差异越大，模型性能越差.在回归问题中，均方差是常用的损失函数，其表达式如下所示：<br>$$<br>E &#x3D; \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y - y’)^2}<br>$$<br>其中，y为模型预测值，y’为真实值. 均方差具有非常好的几何意义，对应着常用的欧几里得距离（简称欧式距离）. 线性回归的任务是要寻找最优线性模型，是的损失函数值最小，即：<br>$$<br>(w^*, b^*) &#x3D; arg min \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y - y’)^2} \<br> &#x3D; arg min \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y’ - wx_i - b)^2}<br>$$<br>基于均方误差最小化来进行模型求解的方法称为“最小二乘法”. 线性回归中，最小二乘法就是试图找到一条直线，是所有样本到直线的欧式距离之和最小.  可以将损失函数对w和b分别求导，得到损失函数的导函数，并令导函数为0即可得到w和b的最优解.</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p><strong>为什么使用梯度下降</strong></p>
<p>在实际计算中，通过最小二乘法求解最优参数有一定的问题：</p>
<p>（1）最小二乘法需要计算逆矩阵，有可能逆矩阵不存在；</p>
<p>（2）当样本特征数量较多时，计算逆矩阵非常耗时甚至不可行. </p>
<p>所以，在实际计算中，通常采用梯度下降法来求解损失函数的极小值，从而找到模型的最优参数.</p>
<p><strong>什么是梯度下降</strong></p>
<p>梯度（gradient）是一个向量（矢量，有方向），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大.损失函数沿梯度相反方向收敛最快（即能最快找到极值点）.当梯度向量为零（或接近于零），说明到达一个极值点，这也是梯度下降算法迭代计算的终止条件.</p>
<p>这种按照负梯度不停地调整函数权值的过程就叫作“梯度下降法”.通过这样的方法，改变权重让损失函数的值下降得更快，进而将值收敛到损失函数的某个极小值.</p>
<p>通过损失函数，我们将“寻找最优参数”问题，转换为了“寻找损失函数最小值”问题.梯度下降法算法描述如下：</p>
<p>（1）损失是否足够小？如果不是，计算损失函数的梯度.<br>（2）按梯度的反方向走一小步，以缩小损失.<br>（3）循环到（1）.</p>
<p><img src="https://image.discover304.top/ai/gradent_decent.png" alt="gradent_decent"></p>
<p>梯度下降法中通过沿着梯度负方向不断调整参数，从而逐步接近损失函数极小值所在点. 如下图所示：</p>
<p><img src="https://image.discover304.top/ai/gradent_decent2.png" alt="gradent_decent2"></p>
<p><strong>参数更新法则</strong></p>
<p>在直线方程中，有两个参数需要学习，$w_0$和$w_1$，梯度下降过程中，分别对这两个参数单独进行调整，调整法则如下：<br>$$<br>w_0 &#x3D; w_0 + \Delta w_0\<br>w_1 &#x3D; w_1 + \Delta w_1<br>$$<br>$\Delta w_0$和$\Delta w_1$可表示为：<br>$$<br>\Delta w_0 &#x3D; -\eta \frac{\Delta loss}{\Delta w_0}\<br>\Delta w_1 &#x3D; -\eta \frac{\Delta loss}{\Delta w_1}\<br>$$<br>其中，$\eta$称为学习率，$\frac{\Delta loss}{\Delta w_i}$为梯度（即损失函数关于参数$w_i$的偏导数）. 损失函数表达式为：<br>$$<br>loss &#x3D;\frac{1}{2}\sum(y - y’)^2 &#x3D;  \frac{1}{2}\sum((y-(w_0+w_1x))^2)<br>$$<br>对损失函数求导（求导过程见补充知识），可得$w_0, w_1$的偏导数为：<br>$$<br>\frac{\Delta loss}{\Delta w_0} &#x3D; \sum((y - y’)(-1)) &#x3D; -\sum(y - y’)\<br>\frac{\Delta loss}{\Delta w_1} &#x3D; \sum((y - y’)(-x)) &#x3D; -\sum(x(y - y’))<br>$$</p>
<h2 id="实现线性回归"><a href="#实现线性回归" class="headerlink" title="实现线性回归"></a>实现线性回归</h2><h3 id="自己编码实现"><a href="#自己编码实现" class="headerlink" title="自己编码实现"></a>自己编码实现</h3><p>以下是实现线性回归的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性回归示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> axes3d</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">train_x = np.array([<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1.1</span>, <span class="number">1.4</span>])  <span class="comment"># 输入集</span></span><br><span class="line">train_y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">1000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">lrate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">epochs = []  <span class="comment"># 记录迭代次数</span></span><br><span class="line">losses = []  <span class="comment"># 记录损失值</span></span><br><span class="line"></span><br><span class="line">w0, w1 = [<span class="number">1</span>], [<span class="number">1</span>]  <span class="comment"># 模型初始值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">    epochs.append(i)  <span class="comment"># 记录第几次迭代</span></span><br><span class="line"></span><br><span class="line">    y = w0[-<span class="number">1</span>] + w1[-<span class="number">1</span>] * train_x  <span class="comment"># 取出最新的w0,w1计算线性方程输出</span></span><br><span class="line">    <span class="comment"># 损失函数(均方差)</span></span><br><span class="line">    loss = (((train_y - y) ** <span class="number">2</span>).<span class="built_in">sum</span>()) / <span class="number">2</span></span><br><span class="line">    losses.append(loss)  <span class="comment"># 记录每次迭代的损失值</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%d: w0=%f, w1=%f, loss=%f&quot;</span> % (i, w0[-<span class="number">1</span>], w1[-<span class="number">1</span>], loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算w0,w1的偏导数</span></span><br><span class="line">    d0 = -(train_y - y).<span class="built_in">sum</span>()</span><br><span class="line">    d1 = -(train_x * (train_y - y)).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新w0,w1</span></span><br><span class="line">    w0.append(w0[-<span class="number">1</span>] - (d0 * lrate))</span><br><span class="line">    w1.append(w1[-<span class="number">1</span>] - (d1 * lrate))</span><br></pre></td></tr></table></figure>

<p>程序执行结果：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 <span class="attribute">w0</span>=1.00000000 <span class="attribute">w1</span>=1.00000000 <span class="attribute">loss</span>=44.17500000</span><br><span class="line">2 <span class="attribute">w0</span>=1.20900000 <span class="attribute">w1</span>=1.19060000 <span class="attribute">loss</span>=36.53882794</span><br><span class="line">3 <span class="attribute">w0</span>=1.39916360 <span class="attribute">w1</span>=1.36357948 <span class="attribute">loss</span>=30.23168666</span><br><span class="line">4 <span class="attribute">w0</span>=1.57220792 <span class="attribute">w1</span>=1.52054607 <span class="attribute">loss</span>=25.02222743</span><br><span class="line">5 <span class="attribute">w0</span>=1.72969350 <span class="attribute">w1</span>=1.66296078 <span class="attribute">loss</span>=20.71937337</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line">996 <span class="attribute">w0</span>=4.06506160 <span class="attribute">w1</span>=2.26409126 <span class="attribute">loss</span>=0.08743506</span><br><span class="line">997 <span class="attribute">w0</span>=4.06518850 <span class="attribute">w1</span>=2.26395572 <span class="attribute">loss</span>=0.08743162</span><br><span class="line">998 <span class="attribute">w0</span>=4.06531502 <span class="attribute">w1</span>=2.26382058 <span class="attribute">loss</span>=0.08742820</span><br><span class="line">999 <span class="attribute">w0</span>=4.06544117 <span class="attribute">w1</span>=2.26368585 <span class="attribute">loss</span>=0.08742480</span><br><span class="line">1000 <span class="attribute">w0</span>=4.06556693 <span class="attribute">w1</span>=2.26355153 <span class="attribute">loss</span>=0.08742142</span><br></pre></td></tr></table></figure>

<p>可以给数据加上可视化，让结果更直观.添加如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###################### 训练过程可视化 ######################</span></span><br><span class="line"><span class="comment"># 训练过程可视化</span></span><br><span class="line"><span class="comment">## 损失函数收敛过程</span></span><br><span class="line">w0 = np.array(w0[:-<span class="number">1</span>])</span><br><span class="line">w1 = np.array(w1[:-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mp.figure(<span class="string">&quot;Losses&quot;</span>, facecolor=<span class="string">&quot;lightgray&quot;</span>)  <span class="comment"># 创建一个窗体</span></span><br><span class="line">mp.title(<span class="string">&quot;epoch&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&quot;:&quot;</span>)  <span class="comment"># 网格线：虚线</span></span><br><span class="line">mp.plot(epochs, losses, c=<span class="string">&quot;blue&quot;</span>, label=<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">mp.legend()  <span class="comment"># 图例</span></span><br><span class="line">mp.tight_layout()  <span class="comment"># 紧凑格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示模型直线</span></span><br><span class="line">pred_y = w0[-<span class="number">1</span>] + w1[-<span class="number">1</span>] * train_x  <span class="comment"># 根据x预测y</span></span><br><span class="line">mp.figure(<span class="string">&quot;Linear Regression&quot;</span>, facecolor=<span class="string">&quot;lightgray&quot;</span>)</span><br><span class="line">mp.title(<span class="string">&quot;Linear Regression&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&quot;x&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&quot;y&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&quot;:&quot;</span>)</span><br><span class="line">mp.scatter(train_x, train_y, c=<span class="string">&quot;blue&quot;</span>, label=<span class="string">&quot;Traing&quot;</span>)  <span class="comment"># 绘制样本散点图</span></span><br><span class="line">mp.plot(train_x, pred_y, c=<span class="string">&quot;red&quot;</span>, label=<span class="string">&quot;Regression&quot;</span>)</span><br><span class="line">mp.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示梯度下降过程(复制粘贴即可，不需要编写)</span></span><br><span class="line"><span class="comment"># 计算损失函数曲面上的点 loss = f(w0, w1)</span></span><br><span class="line">arr1 = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">500</span>)  <span class="comment"># 0~9间产生500个元素的均匀列表</span></span><br><span class="line">arr2 = np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">500</span>)  <span class="comment"># 0~3.5间产生500个元素的均匀列表</span></span><br><span class="line"></span><br><span class="line">grid_w0, grid_w1 = np.meshgrid(arr1, arr2)  <span class="comment"># 产生二维矩阵</span></span><br><span class="line"></span><br><span class="line">flat_w0, flat_w1 = grid_w0.ravel(), grid_w1.ravel()  <span class="comment"># 二维矩阵扁平化</span></span><br><span class="line">loss_metrix = train_y.reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 生成误差矩阵（-1,1）表示自动计算维度</span></span><br><span class="line">outer = np.outer(train_x, flat_w1)  <span class="comment"># 求外积（train_x和flat_w1元素两两相乘的新矩阵）</span></span><br><span class="line"><span class="comment"># 计算损失：((w0 + w1*x - y)**2)/2</span></span><br><span class="line">flat_loss = (((flat_w0 + outer - loss_metrix) ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">0</span>)) / <span class="number">2</span></span><br><span class="line">grid_loss = flat_loss.reshape(grid_w0.shape)</span><br><span class="line"></span><br><span class="line">mp.figure(<span class="string">&#x27;Loss Function&#x27;</span>)</span><br><span class="line">ax = mp.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Loss Function&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;w0&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;w1&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;loss&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=<span class="number">10</span>, cstride=<span class="number">10</span>, cmap=<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">ax.plot(w0, w1, losses, <span class="string">&#x27;o-&#x27;</span>, c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;BGD&#x27;</span>, zorder=<span class="number">5</span>)</span><br><span class="line">mp.legend(loc=<span class="string">&#x27;lower left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>数据可视化结果如下图所示：</p>
<p><img src="https://image.discover304.top/ai/linear_5.png"></p>
<center><font size=2>回归得到的线性模型</font></center>

<p><img src="https://image.discover304.top/ai/linear_loss.png"></p>
<center><font size=2>损失函数收敛过程</font></center>

<p><img src="https://image.discover304.top/ai/linear_6.png"></p>
<center><font size=2>梯度下降过程</font></center>

<h3 id="通过sklearn-API实现"><a href="#通过sklearn-API实现" class="headerlink" title="通过sklearn API实现"></a>通过sklearn API实现</h3><p>同样，可以使用sklearn库提供的API实现线性回归.代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用LinearRegression实现线性回归</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm  <span class="comment"># 线性模型# 线性模型</span></span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm  <span class="comment"># 模型性能评价模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line">train_x = np.array([[<span class="number">0.5</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>], [<span class="number">1.1</span>], [<span class="number">1.4</span>]])  <span class="comment"># 输入集</span></span><br><span class="line">train_y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归器</span></span><br><span class="line">model = lm.LinearRegression()</span><br><span class="line"><span class="comment"># 用已知输入、输出数据集训练回归器</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># 根据训练模型预测输出</span></span><br><span class="line">pred_y = model.predict(train_x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;coef_:&quot;</span>, model.coef_)  <span class="comment"># 系数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;intercept_:&quot;</span>, model.intercept_)  <span class="comment"># 截距</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Linear Regression&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Linear Regression&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本点</span></span><br><span class="line">mp.scatter(train_x, train_y, c=<span class="string">&#x27;blue&#x27;</span>, alpha=<span class="number">0.8</span>, s=<span class="number">60</span>, label=<span class="string">&#x27;Sample&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合直线</span></span><br><span class="line">mp.plot(train_x,  <span class="comment"># x坐标数据</span></span><br><span class="line">        pred_y,  <span class="comment"># y坐标数据</span></span><br><span class="line">        c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;Regression&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p><img src="https://image.discover304.top/ai/linear_10.png" alt="linear_10"></p>
<h2 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h2><p>（1）平均绝对误差（Mean Absolute Deviation）：单个观测值与算术平均值的偏差的绝对值的平均；</p>
<p>（2）均方误差：单个样本到平均值差值的平方平均值；</p>
<p>（3）MAD(中位数绝对偏差)：与数据中值绝对偏差的中值；</p>
<p><img src="https://image.discover304.top/ai/MAD.png"></p>
<p>（4）R2决定系数：趋向于1，模型越好；趋向于0，模型越差. </p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><h3 id="什么是多项式回归"><a href="#什么是多项式回归" class="headerlink" title="什么是多项式回归"></a>什么是多项式回归</h3><p>线性回归适用于数据呈线性分布的回归问题.如果数据样本呈明显非线性分布，线性回归模型就不再适用（下图左），而采用多项式回归可能更好（下图右）.例如：</p>
<p><img src="https://image.discover304.top/ai/poly_1.png" alt="poly_1"></p>
<h3 id="多项式模型定义"><a href="#多项式模型定义" class="headerlink" title="多项式模型定义"></a>多项式模型定义</h3><p>与线性模型相比，多项式模型引入了高次项，自变量的指数大于1，例如一元二次方程：<br>$$<br>y &#x3D; w_0 + w_1x + w_2x^2<br>$$<br>一元三次方程：<br>$$<br>y &#x3D; w_0 + w_1x + w_2x^2 + w_3x ^ 3<br>$$<br>推广到一元n次方程：<br>$$<br>y &#x3D; w_0 + w_1x + w_2x^2 + w_3x ^ 3 + … + w_nx^n<br>$$<br>上述表达式可以简化为：<br>$$<br>y &#x3D; \sum_{i&#x3D;1}^N w_ix^i<br>$$</p>
<h3 id="与线性回归的关系"><a href="#与线性回归的关系" class="headerlink" title="与线性回归的关系"></a>与线性回归的关系</h3><p>多项式回归可以理解为线性回归的扩展，在线性回归模型中添加了新的特征值.例如，要预测一栋房屋的价格，有$x_1, x_2, x_3$三个特征值，分别表示房子长、宽、高，则房屋价格可表示为以下线性模型：<br>$$<br>y &#x3D; w_1 x_1 + w_2 x_2 + w_3 x_3 + b<br>$$<br>对于房屋价格，也可以用房屋的体积，而不直接使用$x_1, x_2, x_3$三个特征：<br>$$<br>y &#x3D; w_0 + w_1x + w_2x^2 + w_3x ^ 3<br>$$<br>相当于创造了新的特征$x, x$ &#x3D; 长 * 宽 * 高.  以上两个模型可以解释为：</p>
<ul>
<li>房屋价格是关于长、宽、高三个特征的线性模型</li>
<li>房屋价格是关于体积的多项式模型</li>
</ul>
<p>因此，可以将一元n次多项式变换成n元一次线性模型.</p>
<h3 id="多项式回归实现"><a href="#多项式回归实现" class="headerlink" title="多项式回归实现"></a>多项式回归实现</h3><p>对于一元n次多项式，同样可以利用梯度下降对损失值最小化的方法，寻找最优的模型参数$w_0, w_1, w_2, …, w_n$.可以将一元n次多项式，变换成n元一次多项式，求线性回归.以下是一个多项式回归的实现.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多项式回归示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 线性模型</span></span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm</span><br><span class="line"><span class="comment"># 模型性能评价模块</span></span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"><span class="comment"># 管线模块</span></span><br><span class="line"><span class="keyword">import</span> sklearn.pipeline <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line">train_x, train_y = [], []   <span class="comment"># 输入、输出样本</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;poly_sample.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        data = [<span class="built_in">float</span>(substr) <span class="keyword">for</span> substr <span class="keyword">in</span> line.split(<span class="string">&quot;,&quot;</span>)]</span><br><span class="line">        train_x.append(data[:-<span class="number">1</span>])</span><br><span class="line">        train_y.append(data[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">train_x = np.array(train_x)  <span class="comment"># 二维数据形式的输入矩阵，一行一样本，一列一特征</span></span><br><span class="line">train_y = np.array(train_y)  <span class="comment"># 一维数组形式的输出序列，每个元素对应一个输入样本</span></span><br><span class="line"><span class="comment"># print(train_x)</span></span><br><span class="line"><span class="comment"># print(train_y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多项式特征扩展预处理，和一个线性回归器串联为一个管线</span></span><br><span class="line"><span class="comment"># 多项式特征扩展：对现有数据进行的一种转换，通过将数据映射到更高维度的空间中</span></span><br><span class="line"><span class="comment"># 进行多项式扩展后，我们就可以认为，模型由以前的直线变成了曲线</span></span><br><span class="line"><span class="comment"># 从而可以更灵活的去拟合数据</span></span><br><span class="line"><span class="comment"># pipeline连接两个模型</span></span><br><span class="line">model = pl.make_pipeline(sp.PolynomialFeatures(<span class="number">3</span>), <span class="comment"># 多项式特征扩展，扩展最高次项为3</span></span><br><span class="line">                         lm.LinearRegression())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用已知输入、输出数据集训练回归器</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># print(model[1].coef_)</span></span><br><span class="line"><span class="comment"># print(model[1].intercept_)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据训练模型预测输出</span></span><br><span class="line">pred_train_y = model.predict(train_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估指标</span></span><br><span class="line">err4 = sm.r2_score(train_y, pred_train_y)  <span class="comment"># R2得分, 范围[0, 1], 分值越大越好</span></span><br><span class="line"><span class="built_in">print</span>(err4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集之外构建测试集</span></span><br><span class="line">test_x = np.linspace(train_x.<span class="built_in">min</span>(), train_x.<span class="built_in">max</span>(), <span class="number">1000</span>)</span><br><span class="line">pre_test_y = model.predict(test_x.reshape(-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># 对新样本进行预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Polynomial Regression&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Polynomial Regression&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">mp.scatter(train_x, train_y, c=<span class="string">&#x27;dodgerblue&#x27;</span>, alpha=<span class="number">0.8</span>, s=<span class="number">60</span>, label=<span class="string">&#x27;Sample&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.plot(test_x, pre_test_y, c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;Regression&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">0</span>.<span class="number">9224401504764776</span></span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p><img src="https://image.discover304.top/ai/poly_2.png" alt="poly_2"></p>
<h2 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h2><h3 id="什么是欠拟合、过拟合"><a href="#什么是欠拟合、过拟合" class="headerlink" title="什么是欠拟合、过拟合"></a>什么是欠拟合、过拟合</h3><p>在上一小节多项式回归示例中，多项特征扩展器PolynomialFeatures()进行多项式扩展时，指定了最高次数为3，该参数为多项式扩展的重要参数，如果选取不当，则可能导致不同的拟合效果.下图显示了该参数分别设为1、20时模型的拟合图像：</p>
<p><img src="https://image.discover304.top/ai/poly_3.png" alt="poly_3"></p>
<p>这两种其实都不是好的模型. 前者没有学习到数据分布规律，模型拟合程度不够，预测准确度过低，这种现象称为“欠拟合”；后者过于拟合更多样本，以致模型泛化能力（新样本的适应性）变差，这种现象称为“过拟合”. **欠拟合模型一般表现为训练集、测试集下准确度都比较低；过拟合模型一般表现为训练集下准确度较高、测试集下准确度较低.  **一个好的模型，不论是对于训练数据还是测试数据，都有接近的预测精度，而且精度不能太低.</p>
<p>【思考1】以下哪种模型较好，哪种模型较差，较差的原因是什么？</p>
<table>
<thead>
<tr>
<th>训练集R2值</th>
<th>测试集R2值</th>
</tr>
</thead>
<tbody><tr>
<td>0.6</td>
<td>0.5</td>
</tr>
<tr>
<td>0.9</td>
<td>0.6</td>
</tr>
<tr>
<td>0.9</td>
<td>0.88</td>
</tr>
</tbody></table>
<p>【答案】第一个模型欠拟合；第二个模型过拟合；第三个模型适中，为可接受的模型.</p>
<p>【思考2】以下哪个曲线为欠拟合、过拟合，哪个模型拟合最好？</p>
<p><img src="https://image.discover304.top/ai/overfit.png" alt="overfit"></p>
<p>【答案】第一个模型欠拟合；第三个模型过拟合；第二个模型拟合较好.</p>
<h3 id="如何处理欠拟合、过拟合"><a href="#如何处理欠拟合、过拟合" class="headerlink" title="如何处理欠拟合、过拟合"></a>如何处理欠拟合、过拟合</h3><ul>
<li>欠拟合：提高模型复杂度，如增加特征、增加模型最高次幂等等；</li>
<li>过拟合：降低模型复杂度，如减少特征、降低模型最高次幂等等.</li>
</ul>
<h2 id="线性回归模型变种"><a href="#线性回归模型变种" class="headerlink" title="线性回归模型变种"></a>线性回归模型变种</h2><p>过拟合还有一个常见的原因，就是模型参数值太大，所以可以通过抑制参数的方式来解决过拟合问题.如下图所示，右图产生了一定程度过拟合，可以通过弱化高次项的系数（但不删除）来降低过拟合. </p>
<p><img src="https://image.discover304.top/ai/overfit2.png" alt="overfit2"></p>
<p>例如，可以通过在$\theta_3, \theta_4$上添加一定的系数，来压制这两个高次项的系数，这种方法称为正则化。但在实际问题中，可能有更多的系数，我们并不知道应该压制哪些系数，所以，可以通过<strong>收缩所有系数来避免过拟合</strong>.</p>
<h3 id="正则化定义"><a href="#正则化定义" class="headerlink" title="正则化定义"></a>正则化定义</h3><p>正则化是指，在目标函数（如损失函数）后面加上一个范数，来防止过拟合的手段，这个范数定义为：<br>$$<br>||x||<em>p &#x3D; (\sum</em>{i&#x3D;1}^N |x|^p)^{\frac{1}{p}}<br>$$<br>当p&#x3D;1时，称为L1范数（即所有系数绝对值之和）：<br>$$<br>||x||<em>1 &#x3D; (\sum</em>{i&#x3D;1}^N |x|)<br>$$<br>当p&#x3D;2是，称为L2范数（即所有系数平方之和再开方）：<br>$$<br>||x||<em>2 &#x3D; (\sum</em>{i&#x3D;1}^N |x|^2)^{\frac{1}{2}}<br>$$<br>通过对目标函数添加正则项，整体上压缩了参数的大小，从而防止过拟合.</p>
<h3 id="Lasso回归与岭回归"><a href="#Lasso回归与岭回归" class="headerlink" title="Lasso回归与岭回归"></a>Lasso回归与岭回归</h3><p>Lasso 回归和岭回归（Ridge Regression）都是在标准线性回归的基础上修改了损失函数的回归算法. Lasso回归全称为 Least absolute shrinkage and selection operator，又译“最小绝对值收敛和选择算子”、”套索算法”，其损失函数如下所示：<br>$$<br>E &#x3D; \frac{1}{n}(\sum_{i&#x3D;1}^N y_i - y_i’)^2 + \lambda ||w||<em>1<br>$$<br>岭回归损失函数为：<br>$$<br>E &#x3D; \frac{1}{n}(\sum</em>{i&#x3D;1}^N y_i - y_i’)^2 + \lambda ||w||_2<br>$$<br>从逻辑上说，Lasso回归和岭回归都可以理解为通过调整损失函数，减小函数的系数，从而避免过于拟合于样本，降低偏差较大的样本的权重和对模型的影响程度.</p>
<p>以下关于Lasso回归于岭回归的sklearn实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lasso回归和岭回归示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 线性模型</span></span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm</span><br><span class="line"><span class="comment"># 模型性能评价模块</span></span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line">x, y = [], []  <span class="comment"># 输入、输出样本</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;abnormal.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        data = [<span class="built_in">float</span>(substr) <span class="keyword">for</span> substr <span class="keyword">in</span> line.split(<span class="string">&quot;,&quot;</span>)]</span><br><span class="line">        x.append(data[:-<span class="number">1</span>])</span><br><span class="line">        y.append(data[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">x = np.array(x)  <span class="comment"># 二维数据形式的输入矩阵，一行一样本，一列一特征</span></span><br><span class="line">y = np.array(y)  <span class="comment"># 一维数组形式的输出序列，每个元素对应一个输入样本</span></span><br><span class="line"><span class="comment"># print(x)</span></span><br><span class="line"><span class="comment"># print(y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归器</span></span><br><span class="line">model = lm.LinearRegression()</span><br><span class="line"><span class="comment"># 用已知输入、输出数据集训练回归器</span></span><br><span class="line">model.fit(x, y)</span><br><span class="line"><span class="comment"># 根据训练模型预测输出</span></span><br><span class="line">pred_y = model.predict(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建岭回归器并进行训练</span></span><br><span class="line"><span class="comment"># Ridge: 第一个参数为正则强度，该值越大，异常样本权重就越小</span></span><br><span class="line">model_2 = lm.Ridge(alpha=<span class="number">200</span>, max_iter=<span class="number">1000</span>)  <span class="comment"># 创建对象, max_iter为最大迭代次数</span></span><br><span class="line">model_2.fit(x, y)  <span class="comment"># 训练</span></span><br><span class="line">pred_y2 = model_2.predict(x)  <span class="comment"># 预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># lasso回归</span></span><br><span class="line">model_3 = lm.Lasso(alpha=<span class="number">0.5</span>,  <span class="comment"># L1范数相乘的系数</span></span><br><span class="line">                   max_iter=<span class="number">1000</span>)  <span class="comment"># 最大迭代次数</span></span><br><span class="line">model_3.fit(x, y)  <span class="comment"># 训练</span></span><br><span class="line">pred_y3 = model_3.predict(x)  <span class="comment"># 预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Linear &amp; Ridge &amp; Lasso&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Linear &amp; Ridge &amp; Lasso&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">mp.scatter(x, y, c=<span class="string">&#x27;dodgerblue&#x27;</span>, alpha=<span class="number">0.8</span>, s=<span class="number">60</span>, label=<span class="string">&#x27;Sample&#x27;</span>)</span><br><span class="line">sorted_idx = x.T[<span class="number">0</span>].argsort()</span><br><span class="line"></span><br><span class="line">mp.plot(x[sorted_idx], pred_y[sorted_idx], c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;Linear&#x27;</span>)  <span class="comment"># 线性回归</span></span><br><span class="line">mp.plot(x[sorted_idx], pred_y2[sorted_idx], c=<span class="string">&#x27;limegreen&#x27;</span>, label=<span class="string">&#x27;Ridge&#x27;</span>)  <span class="comment"># 岭回归</span></span><br><span class="line">mp.plot(x[sorted_idx], pred_y3[sorted_idx], c=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Lasso&#x27;</span>)  <span class="comment"># Lasso回归</span></span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>以下是执行结果：</p>
<p><img src="https://image.discover304.top/ai/linear_9.png" alt="linear_9"></p>
<h2 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h2><p>可以使用Python提供的功能对模型对象进行保存.使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">pickle.dump(模型对象, 文件对象)   </span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model_obj = pickle.load(文件对象)</span><br></pre></td></tr></table></figure>

<p>保存训练模型应该在训练完成或评估完成之后，完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型保存示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm <span class="comment"># 线性模型</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0.5</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>], [<span class="number">1.1</span>], [<span class="number">1.4</span>]])  <span class="comment"># 输入集</span></span><br><span class="line">y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归器</span></span><br><span class="line">model = lm.LinearRegression()</span><br><span class="line"><span class="comment"># 用已知输入、输出数据集训练回归器</span></span><br><span class="line">model.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存训练后的模型</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;linear_model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(model, f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;保存模型完成.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>执行完成后，可以看到与源码相同目录下多了一个名称为linear_model.pkl的文件，这就是保存的训练模型.使用该模型代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型加载示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm  <span class="comment"># 线性模型</span></span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm  <span class="comment"># 模型性能评价模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0.5</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>], [<span class="number">1.1</span>], [<span class="number">1.4</span>]])  <span class="comment"># 输入集</span></span><br><span class="line">y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;linear_model.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    model = pickle.load(f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;加载模型完成.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据加载的模型预测输出</span></span><br><span class="line">pred_y = model.predict(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Linear Regression&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Linear Regression&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">mp.scatter(x, y, c=<span class="string">&#x27;blue&#x27;</span>, alpha=<span class="number">0.8</span>, s=<span class="number">60</span>, label=<span class="string">&#x27;Sample&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.plot(x, pred_y, c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;Regression&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>执行结果和训练模型预测结果一样.</p>
<h2 id="线性回归总结"><a href="#线性回归总结" class="headerlink" title="线性回归总结"></a>线性回归总结</h2><p>（1）什么是线性模型：线性模型是自然界最简单的模型之一，反映自变量、因变量之间的等比例增长关系</p>
<p>（2）什么时候使用线性回归：线性模型只能用于满足线性分布规律的数据中</p>
<p>（3）如何实现线性回归：给定一组样本，给定初始的w和b，通过梯度下降法求最优的w和b</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a><a class="post-meta__tags" href="/tags/%E8%AE%B0%E5%BD%95/">记录</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://image.discover304.top/ai/AI-cover-black-white.webp" data-sites="facebook,twitter,wechat,weibo,qzone,qq,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/25/2021q4/103-1-ml-dt/"><img class="prev-cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" onerror="onerror=null;src='/img/404.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">【机器学习】第二部分下：决策树回归</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/25/2021q4/103-0-ml/"><img class="next-cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" onerror="onerror=null;src='/img/404.png'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">【机器学习】第一部分：概述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/11/25/2021q4/103-0-ml/" title="【机器学习】第一部分：概述"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第一部分：概述</div></div></a></div><div><a href="/2021/11/25/2021q4/103-2-ml-bays/" title="【机器学习】第三部分肆：朴素贝叶斯"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第三部分肆：朴素贝叶斯</div></div></a></div><div><a href="/2021/11/25/2021q4/103-2-ml-svm/" title="【机器学习】第三部分叁：支持向量机（SVM）"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第三部分叁：支持向量机（SVM）</div></div></a></div><div><a href="/2021/11/25/2021q4/103-2-ml-lo/" title="【机器学习】第三部分壹：逻辑回归"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第三部分壹：逻辑回归</div></div></a></div><div><a href="/2021/11/25/2021q4/103-2-ml-dt/" title="【机器学习】第三部分贰：决策树分类"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第三部分贰：决策树分类</div></div></a></div><div><a href="/2021/11/25/2021q4/103-4-ml-pca/" title="【机器学习】第五部分：降维问题"><img class="cover" src="https://image.discover304.top/ai/AI-cover-black-white.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【机器学习】第五部分：降维问题</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">✨YangSier✨</div><div class="author-info__description">Love Everything You Like.</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">261</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">90</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">29</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://space.bilibili.com/98639326"><i class="fab fa-bilibili"></i><span>Bilibili Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YangSierCode000" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/Discover304" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a><a class="social-icon" href="https://www.zhihu.com/people/discover-56-86-75" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="mailto:hobart.yang@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://jq.qq.com/?_wv=1027&amp;k=EaGddTQg" target="_blank" title="QQ"><i class="fa-brands fa-qq"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">✨动态更新：<p style="text-align:center">享受精彩大学生活中。</p>✨聊天划水QQ群：<p style="text-align:center"><a target="_blank" rel="noopener" href="https://jq.qq.com/?_wv=1027&k=EaGddTQg"><strong>兔叽の魔术工房</strong></a><br>942-848-525</p>✨我们的口号是：<p style="text-align:center; color:#39C5BB">人工降神，机械飞升！</p><a target="_blank" rel="noopener" href='https://space.bilibili.com/98639326'><img src='/img/mikulittletrans.png'></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-text">线性模型定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">梯度下降法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">实现线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%B7%B1%E7%BC%96%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">自己编码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87sklearn-API%E5%AE%9E%E7%8E%B0"><span class="toc-text">通过sklearn API实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-text">模型评价指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-text">多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-text">什么是多项式回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-text">多项式模型定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">与线性回归的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0"><span class="toc-text">多项式回归实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">过拟合与欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%AC%A0%E6%8B%9F%E5%90%88%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">什么是欠拟合、过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%AC%A0%E6%8B%9F%E5%90%88%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">如何处理欠拟合、过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%8F%98%E7%A7%8D"><span class="toc-text">线性回归模型变种</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%AE%9A%E4%B9%89"><span class="toc-text">正则化定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-text">Lasso回归与岭回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-text">模型保存与加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%80%BB%E7%BB%93"><span class="toc-text">线性回归总结</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/183-rl-call/" title="ICLR2024 Emergent Communication With Conversational Repair"><img src="https://api.btstu.cn/sjbz/api.php?lx=dongman&amp;format=images" onerror="this.onerror=null;this.src='/img/404.png'" alt="ICLR2024 Emergent Communication With Conversational Repair"/></a><div class="content"><a class="title" href="/2024/06/25/183-rl-call/" title="ICLR2024 Emergent Communication With Conversational Repair">ICLR2024 Emergent Communication With Conversational Repair</a><time datetime="2024-06-25T07:30:48.635Z" title="Updated 2024-06-25 15:30:48">2024-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/05/181-2/" title="PMR Basic Assumptions for Efficient Model Representation"><img src="https://api.btstu.cn/sjbz/api.php?lx=dongman&amp;format=images" onerror="this.onerror=null;this.src='/img/404.png'" alt="PMR Basic Assumptions for Efficient Model Representation"/></a><div class="content"><a class="title" href="/2024/02/05/181-2/" title="PMR Basic Assumptions for Efficient Model Representation">PMR Basic Assumptions for Efficient Model Representation</a><time datetime="2024-05-31T09:58:47.688Z" title="Updated 2024-05-31 17:58:47">2024-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/17/2021q4/118-ros-note-code-naming/" title="【机器人】ROS1程序运行指北：启动、重命名、launch"><img src="https://image.discover304.top/bai_gu_gu_head.jpg?imageView2/2/h/300" onerror="this.onerror=null;this.src='/img/404.png'" alt="【机器人】ROS1程序运行指北：启动、重命名、launch"/></a><div class="content"><a class="title" href="/2021/12/17/2021q4/118-ros-note-code-naming/" title="【机器人】ROS1程序运行指北：启动、重命名、launch">【机器人】ROS1程序运行指北：启动、重命名、launch</a><time datetime="2024-03-10T04:03:46.235Z" title="Updated 2024-03-10 12:03:46">2024-03-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/17/2021q4/118-ros-note-code-serv/" title="【机器人】ROS1工程案例：服务和动作"><img src="https://image.discover304.top/bai_gu_gu_head.jpg?imageView2/2/h/300" onerror="this.onerror=null;this.src='/img/404.png'" alt="【机器人】ROS1工程案例：服务和动作"/></a><div class="content"><a class="title" href="/2021/12/17/2021q4/118-ros-note-code-serv/" title="【机器人】ROS1工程案例：服务和动作">【机器人】ROS1工程案例：服务和动作</a><time datetime="2024-03-10T04:03:46.235Z" title="Updated 2024-03-10 12:03:46">2024-03-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/17/2021q4/118-ros-note-code-type/" title="【机器人】ROS1工程案例：自定义消息类型"><img src="https://image.discover304.top/bai_gu_gu_head.jpg?imageView2/2/h/300" onerror="this.onerror=null;this.src='/img/404.png'" alt="【机器人】ROS1工程案例：自定义消息类型"/></a><div class="content"><a class="title" href="/2021/12/17/2021q4/118-ros-note-code-type/" title="【机器人】ROS1工程案例：自定义消息类型">【机器人】ROS1工程案例：自定义消息类型</a><time datetime="2024-03-10T04:03:46.235Z" title="Updated 2024-03-10 12:03:46">2024-03-10</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://image.discover304.top/ai/AI-cover-black-white.webp)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ✨YangSier✨</div><div><a target="_blank" href="https://beian.miit.gov.cn/" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;"> 冀ICP备2021025381号-1</p></a></div><div><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=13060602001430" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><img src="/img/beian.png" style="float:left;"/><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;">冀公网安备 13060602001430号</p></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.spacingElementById('content-inner')
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.spacingElementById('content-inner')
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'neutral',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'A9RWVELPcIotgfbpp9KLGXQM-gzGzoHsz',
      appKey: 'MLgPQW5h0DPgE8jNkeREKubU',
      placeholder: '欢迎留言呀。（网址是选填，可以留空）',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: true,
      serverURLs: 'https://a9rwvelp.lc-cn-n1-shared.com',
      emojiCDN: 'https://cdn.jsdelivr.net/gh/GamerNoTitle/ValineCDN@master/',
      emojiMaps: {"QQ1":"QQ/aini.gif","QQ2":"QQ/aixin.gif","QQ3":"QQ/aoman.gif","QQ4":"QQ/baiyan.gif","QQ5":"QQ/bangbangtang.gif","QQ6":"QQ/baojin.gif","QQ7":"QQ/baoquan.gif","QQ8":"QQ/bishi.gif","QQ9":"QQ/bizui.gif","QQ11":"QQ/cahan.gif","QQ12":"QQ/caidao.gif","QQ13":"QQ/chi.gif","QQ14":"QQ/ciya.gif","QQ15":"QQ/dabing.gif","QQ16":"QQ/daku.gif","QQ17":"QQ/dan.gif","QQ18":"QQ/deyi.gif","QQ19":"QQ/doge.gif","QQ20":"QQ/fadai.gif","QQ21":"QQ/fanu.gif","QQ22":"QQ/fendou.gif","QQ23":"QQ/ganga.gif","QQ24":"QQ/gouyin.gif","QQ25":"QQ/guzhang.gif","QQ26":"QQ/haixiu.gif","QQ27":"QQ/hanxiao.gif","QQ28":"QQ/haobang.gif","QQ29":"QQ/haqian.gif","QQ30":"QQ/hecai.gif","QQ31":"QQ/hexie.gif","QQ32":"QQ/huaixiao.gif","QQ33":"QQ/jie.gif","QQ34":"QQ/jingkong.gif","QQ35":"QQ/jingxi.gif","QQ36":"QQ/jingya.gif","QQ37":"QQ/juhua.gif","QQ38":"QQ/keai.gif","QQ39":"QQ/kelian.gif","QQ40":"QQ/koubi.gif","QQ41":"QQ/ku.gif","QQ42":"QQ/kuaikule.gif","QQ43":"QQ/kulou.gif","QQ44":"QQ/kun.gif","QQ45":"QQ/lanqiu.gif","QQ46":"QQ/leiben.gif","QQ47":"QQ/lenghan.gif","QQ48":"QQ/liuhan.gif","QQ49":"QQ/liulei.gif","QQ50":"QQ/nanguo.gif","QQ51":"QQ/OK.gif","QQ52":"QQ/penxue.gif","QQ53":"QQ/piezui.gif","QQ54":"QQ/pijiu.gif","QQ55":"QQ/qiang.gif","QQ56":"QQ/qiaoda.gif","QQ57":"QQ/qinqin.gif","QQ58":"QQ/qiudale.gif","QQ59":"QQ/quantou.gif","QQ60":"QQ/saorao.gif","QQ61":"QQ/se.gif","QQ62":"QQ/shengli.gif","QQ63":"QQ/shouqiang.gif","QQ64":"QQ/shuai.gif","QQ65":"QQ/shui.gif","QQ66":"QQ/tiaopi.gif","QQ67":"QQ/touxiao.gif","QQ68":"QQ/tu.gif","QQ69":"QQ/tuosai.gif","QQ70":"QQ/weiqu.gif","QQ71":"QQ/weixiao.gif","QQ72":"QQ/woshou.gif","QQ73":"QQ/wozuimei.gif","QQ74":"QQ/wunai.gif","QQ75":"QQ/xia.gif","QQ76":"QQ/xiaojiujie.gif","QQ77":"QQ/xiaoku.gif","QQ78":"QQ/xiaoyanger.gif","QQ79":"QQ/xieyanxiao.gif","QQ80":"QQ/xigua.gif","QQ81":"QQ/xu.gif","QQ82":"QQ/yangtuo.gif","QQ83":"QQ/yinxian.gif","QQ84":"QQ/yiwen.gif","QQ85":"QQ/youhengheng.gif","QQ86":"QQ/youling.gif","QQ87":"QQ/yun.gif","QQ88":"QQ/zaijian.gif","QQ89":"QQ/zhayanjian.gif","QQ90":"QQ/zhemo.gif","QQ91":"QQ/zhouma.gif","QQ92":"QQ/zhuakuang.gif","QQ93":"QQ/zuohengheng.gif","bilibiliHotKey1":"bilibiliHotKey/1.jpg","bilibiliHotKey2":"bilibiliHotKey/10.jpg","bilibiliHotKey3":"bilibiliHotKey/11.jpg","bilibiliHotKey4":"bilibiliHotKey/12.jpg","bilibiliHotKey5":"bilibiliHotKey/13.jpg","bilibiliHotKey6":"bilibiliHotKey/14.jpg","bilibiliHotKey7":"bilibiliHotKey/15.jpg","bilibiliHotKey8":"bilibiliHotKey/16.jpg","bilibiliHotKey9":"bilibiliHotKey/17.jpg","bilibiliHotKey10":"bilibiliHotKey/18.jpg","bilibiliHotKey11":"bilibiliHotKey/19.jpg","bilibiliHotKey12":"bilibiliHotKey/2.jpg","bilibiliHotKey13":"bilibiliHotKey/20.jpg","bilibiliHotKey14":"bilibiliHotKey/21.jpg","bilibiliHotKey15":"bilibiliHotKey/22.jpg","bilibiliHotKey16":"bilibiliHotKey/23.jpg","bilibiliHotKey17":"bilibiliHotKey/24.jpg","bilibiliHotKey18":"bilibiliHotKey/25.jpg","bilibiliHotKey19":"bilibiliHotKey/26.jpg","bilibiliHotKey20":"bilibiliHotKey/27.jpg","bilibiliHotKey21":"bilibiliHotKey/28.jpg","bilibiliHotKey22":"bilibiliHotKey/29.jpg","bilibiliHotKey23":"bilibiliHotKey/3.jpg","bilibiliHotKey24":"bilibiliHotKey/30.jpg","bilibiliHotKey25":"bilibiliHotKey/31.jpg","bilibiliHotKey26":"bilibiliHotKey/32.jpg","bilibiliHotKey27":"bilibiliHotKey/4.jpg","bilibiliHotKey28":"bilibiliHotKey/5.jpg","bilibiliHotKey29":"bilibiliHotKey/6.jpg","bilibiliHotKey30":"bilibiliHotKey/7.jpg","bilibiliHotKey31":"bilibiliHotKey/8.jpg","bilibiliHotKey32":"bilibiliHotKey/9.jpg","Menhera-chan1":"Menhera-chan/1.jpg","Menhera-chan2":"Menhera-chan/10.jpg","Menhera-chan3":"Menhera-chan/100.jpg","Menhera-chan4":"Menhera-chan/101.jpg","Menhera-chan5":"Menhera-chan/102.jpg","Menhera-chan6":"Menhera-chan/103.jpg","Menhera-chan7":"Menhera-chan/104.jpg","Menhera-chan8":"Menhera-chan/105.jpg","Menhera-chan9":"Menhera-chan/106.jpg","Menhera-chan10":"Menhera-chan/107.jpg","Menhera-chan11":"Menhera-chan/108.jpg","Menhera-chan12":"Menhera-chan/109.jpg","Menhera-chan13":"Menhera-chan/11.jpg","Menhera-chan14":"Menhera-chan/110.jpg","Menhera-chan15":"Menhera-chan/111.jpg","Menhera-chan16":"Menhera-chan/112.jpg","Menhera-chan17":"Menhera-chan/113.jpg","Menhera-chan18":"Menhera-chan/114.jpg","Menhera-chan19":"Menhera-chan/115.jpg","Menhera-chan20":"Menhera-chan/116.jpg","Menhera-chan21":"Menhera-chan/117.jpg","Menhera-chan22":"Menhera-chan/118.jpg","Menhera-chan23":"Menhera-chan/119.jpg","Menhera-chan24":"Menhera-chan/12.jpg","Menhera-chan25":"Menhera-chan/120.jpg","Menhera-chan26":"Menhera-chan/13.jpg","Menhera-chan27":"Menhera-chan/14.jpg","Menhera-chan28":"Menhera-chan/15.jpg","Menhera-chan29":"Menhera-chan/16.jpg","Menhera-chan30":"Menhera-chan/17.jpg","Menhera-chan31":"Menhera-chan/18.jpg","Menhera-chan32":"Menhera-chan/19.jpg","Menhera-chan33":"Menhera-chan/2.jpg","Menhera-chan34":"Menhera-chan/20.jpg","Menhera-chan35":"Menhera-chan/21.jpg","Menhera-chan36":"Menhera-chan/22.jpg","Menhera-chan37":"Menhera-chan/23.jpg","Menhera-chan38":"Menhera-chan/24.jpg","Menhera-chan39":"Menhera-chan/25.jpg","Menhera-chan40":"Menhera-chan/26.jpg","Menhera-chan41":"Menhera-chan/27.jpg","Menhera-chan42":"Menhera-chan/28.jpg","Menhera-chan43":"Menhera-chan/29.jpg","Menhera-chan44":"Menhera-chan/3.jpg","Menhera-chan45":"Menhera-chan/30.jpg","Menhera-chan46":"Menhera-chan/31.jpg","Menhera-chan47":"Menhera-chan/32.jpg","Menhera-chan48":"Menhera-chan/33.jpg","Menhera-chan49":"Menhera-chan/34.jpg","Menhera-chan50":"Menhera-chan/35.jpg","Menhera-chan51":"Menhera-chan/36.jpg","Menhera-chan52":"Menhera-chan/37.jpg","Menhera-chan53":"Menhera-chan/38.jpg","Menhera-chan54":"Menhera-chan/39.jpg","Menhera-chan55":"Menhera-chan/4.jpg","Menhera-chan56":"Menhera-chan/40.jpg","Menhera-chan57":"Menhera-chan/41.jpg","Menhera-chan58":"Menhera-chan/42.jpg","Menhera-chan59":"Menhera-chan/43.jpg","Menhera-chan60":"Menhera-chan/44.jpg","Menhera-chan61":"Menhera-chan/45.jpg","Menhera-chan62":"Menhera-chan/46.jpg","Menhera-chan63":"Menhera-chan/47.jpg","Menhera-chan64":"Menhera-chan/48.jpg","Menhera-chan65":"Menhera-chan/49.jpg","Menhera-chan66":"Menhera-chan/5.jpg","Menhera-chan67":"Menhera-chan/50.jpg","Menhera-chan68":"Menhera-chan/51.jpg","Menhera-chan69":"Menhera-chan/52.jpg","Menhera-chan70":"Menhera-chan/53(1).jpg","Menhera-chan71":"Menhera-chan/53.jpg","Menhera-chan72":"Menhera-chan/54.jpg","Menhera-chan73":"Menhera-chan/55.jpg","Menhera-chan74":"Menhera-chan/56.jpg","Menhera-chan75":"Menhera-chan/57.jpg","Menhera-chan76":"Menhera-chan/58.jpg","Menhera-chan77":"Menhera-chan/59.jpg","Menhera-chan78":"Menhera-chan/6.jpg","Menhera-chan79":"Menhera-chan/60.jpg","Menhera-chan80":"Menhera-chan/61.jpg","Menhera-chan81":"Menhera-chan/62.jpg","Menhera-chan82":"Menhera-chan/63.jpg","Menhera-chan83":"Menhera-chan/64.jpg","Menhera-chan84":"Menhera-chan/65.jpg","Menhera-chan85":"Menhera-chan/66.jpg","Menhera-chan86":"Menhera-chan/67.jpg","Menhera-chan87":"Menhera-chan/68.jpg","Menhera-chan88":"Menhera-chan/69.jpg","Menhera-chan89":"Menhera-chan/7.jpg","Menhera-chan90":"Menhera-chan/70.jpg","Menhera-chan91":"Menhera-chan/71.jpg","Menhera-chan92":"Menhera-chan/72.jpg","Menhera-chan93":"Menhera-chan/73.jpg","Menhera-chan94":"Menhera-chan/74.jpg","Menhera-chan95":"Menhera-chan/75.jpg","Menhera-chan96":"Menhera-chan/76.jpg","Menhera-chan97":"Menhera-chan/77.jpg","Menhera-chan98":"Menhera-chan/78.jpg","Menhera-chan99":"Menhera-chan/79.jpg","Menhera-chan100":"Menhera-chan/8.jpg","Menhera-chan101":"Menhera-chan/80.jpg","Menhera-chan102":"Menhera-chan/81.jpg","Menhera-chan103":"Menhera-chan/82.jpg","Menhera-chan104":"Menhera-chan/83.jpg","Menhera-chan105":"Menhera-chan/84.jpg","Menhera-chan106":"Menhera-chan/85.jpg","Menhera-chan107":"Menhera-chan/86.jpg","Menhera-chan108":"Menhera-chan/87.jpg","Menhera-chan109":"Menhera-chan/88.jpg","Menhera-chan110":"Menhera-chan/89.jpg","Menhera-chan111":"Menhera-chan/9.jpg","Menhera-chan112":"Menhera-chan/90.jpg","Menhera-chan113":"Menhera-chan/91.jpg","Menhera-chan114":"Menhera-chan/92.jpg","Menhera-chan115":"Menhera-chan/93.jpg","Menhera-chan116":"Menhera-chan/94.jpg","Menhera-chan117":"Menhera-chan/95.jpg","Menhera-chan118":"Menhera-chan/96.jpg","Menhera-chan119":"Menhera-chan/97.jpg","Menhera-chan120":"Menhera-chan/98.jpg","Menhera-chan121":"Menhera-chan/99.jpg","Sweetie-Bunny1":"Sweetie-Bunny/12311678.png","Sweetie-Bunny2":"Sweetie-Bunny/12311679.png","Sweetie-Bunny3":"Sweetie-Bunny/12311680.png","Sweetie-Bunny4":"Sweetie-Bunny/12311681.png","Sweetie-Bunny5":"Sweetie-Bunny/12311682.png","Sweetie-Bunny6":"Sweetie-Bunny/12311683.png","Sweetie-Bunny7":"Sweetie-Bunny/12311684.png","Sweetie-Bunny8":"Sweetie-Bunny/12311685.png","Sweetie-Bunny9":"Sweetie-Bunny/12311686.png","Sweetie-Bunny10":"Sweetie-Bunny/12311687.png","Sweetie-Bunny11":"Sweetie-Bunny/12311688.png","Sweetie-Bunny12":"Sweetie-Bunny/12311689.png","Sweetie-Bunny13":"Sweetie-Bunny/12311690.png","Sweetie-Bunny14":"Sweetie-Bunny/12311691.png","Sweetie-Bunny15":"Sweetie-Bunny/12311692.png","Sweetie-Bunny16":"Sweetie-Bunny/12311693.png","Sweetie-Bunny17":"Sweetie-Bunny/12311694.png","Sweetie-Bunny18":"Sweetie-Bunny/12311695.png","Sweetie-Bunny19":"Sweetie-Bunny/12311696.png","Sweetie-Bunny20":"Sweetie-Bunny/12311697.png","Sweetie-Bunny21":"Sweetie-Bunny/12311698.png","Sweetie-Bunny22":"Sweetie-Bunny/12311699.png","Sweetie-Bunny23":"Sweetie-Bunny/12311700.png","Sweetie-Bunny24":"Sweetie-Bunny/12311701.png","Sweetie-Bunny25":"Sweetie-Bunny/12311702.png","Sweetie-Bunny26":"Sweetie-Bunny/12311703.png","Sweetie-Bunny27":"Sweetie-Bunny/12311704.png","Sweetie-Bunny28":"Sweetie-Bunny/12311705.png","Sweetie-Bunny29":"Sweetie-Bunny/12311706.png","Sweetie-Bunny30":"Sweetie-Bunny/12311707.png","Sweetie-Bunny31":"Sweetie-Bunny/12311708.png","Sweetie-Bunny32":"Sweetie-Bunny/12311709.png","Sweetie-Bunny33":"Sweetie-Bunny/12311710.png","Sweetie-Bunny34":"Sweetie-Bunny/12311711.png","Sweetie-Bunny35":"Sweetie-Bunny/12311712.png","Sweetie-Bunny36":"Sweetie-Bunny/12311713.png","Sweetie-Bunny37":"Sweetie-Bunny/12311714.png","Sweetie-Bunny38":"Sweetie-Bunny/12311715.png","Sweetie-Bunny39":"Sweetie-Bunny/12311716.png","Sweetie-Bunny40":"Sweetie-Bunny/12311717.png","Majotabi1":"Majotabi/367516718.png","Majotabi2":"Majotabi/367516719.png","Majotabi3":"Majotabi/367516720.png","Majotabi4":"Majotabi/367516721.png","Majotabi5":"Majotabi/367516722.png","Majotabi6":"Majotabi/367516723.png","Majotabi7":"Majotabi/367516724.png","Majotabi8":"Majotabi/367516725.png","Majotabi9":"Majotabi/367516726.png","Majotabi10":"Majotabi/367516727.png","Majotabi11":"Majotabi/367516728.png","Majotabi12":"Majotabi/367516729.png","Majotabi13":"Majotabi/367516730.png","Majotabi14":"Majotabi/367516731.png","Majotabi15":"Majotabi/367516732.png","Majotabi16":"Majotabi/367516733.png","Majotabi17":"Majotabi/367516734.png","Majotabi18":"Majotabi/367516735.png","Majotabi19":"Majotabi/367516736.png","Majotabi20":"Majotabi/367516737.png","Majotabi21":"Majotabi/367516738.png","Majotabi22":"Majotabi/367516739.png","Majotabi23":"Majotabi/367516740.png","Majotabi24":"Majotabi/367516741.png","Majotabi25":"Majotabi/367516742.png","Majotabi26":"Majotabi/367516743.png","Majotabi27":"Majotabi/367516744.png","Majotabi28":"Majotabi/367516745.png","Majotabi29":"Majotabi/367516746.png","Majotabi30":"Majotabi/367516747.png","Majotabi31":"Majotabi/367516748.png","Majotabi32":"Majotabi/367516749.png","Majotabi33":"Majotabi/367516750.png","Majotabi34":"Majotabi/367516751.png","Majotabi35":"Majotabi/367516752.png","Majotabi36":"Majotabi/367516753.png","Majotabi37":"Majotabi/367516754.png","Majotabi38":"Majotabi/367516755.png","Majotabi39":"Majotabi/367516756.png","Majotabi40":"Majotabi/367516757.png","Snow-Miku1":"Snow-Miku/3583066@2x.png","Snow-Miku2":"Snow-Miku/3583067@2x.png","Snow-Miku3":"Snow-Miku/3583068@2x.png","Snow-Miku4":"Snow-Miku/3583069@2x.png","Snow-Miku5":"Snow-Miku/3583070@2x.png","Snow-Miku6":"Snow-Miku/3583071@2x.png","Snow-Miku7":"Snow-Miku/3583072@2x.png","Snow-Miku8":"Snow-Miku/3583073@2x.png","Snow-Miku9":"Snow-Miku/3583074@2x.png","Snow-Miku10":"Snow-Miku/3583075@2x.png","Snow-Miku11":"Snow-Miku/3583076@2x.png","Snow-Miku12":"Snow-Miku/3583077@2x.png","Snow-Miku13":"Snow-Miku/3583078@2x.png","Snow-Miku14":"Snow-Miku/3583079@2x.png","Snow-Miku15":"Snow-Miku/3583080@2x.png","Snow-Miku16":"Snow-Miku/3583081@2x.png","Snow-Miku17":"Snow-Miku/3583082@2x.png","Snow-Miku18":"Snow-Miku/3583083@2x.png","Snow-Miku19":"Snow-Miku/3583084@2x.png","Snow-Miku20":"Snow-Miku/3583085@2x.png","Snow-Miku21":"Snow-Miku/3583086@2x.png","Snow-Miku22":"Snow-Miku/3583087@2x.png","Snow-Miku23":"Snow-Miku/3583088@2x.png","Snow-Miku24":"Snow-Miku/3583089@2x.png","Snow-Miku25":"Snow-Miku/3583090@2x.png","Snow-Miku26":"Snow-Miku/3583091@2x.png","Snow-Miku27":"Snow-Miku/3583092@2x.png","Snow-Miku28":"Snow-Miku/3583093@2x.png","Snow-Miku29":"Snow-Miku/3583094@2x.png","Snow-Miku30":"Snow-Miku/3583095@2x.png","Snow-Miku31":"Snow-Miku/3583096@2x.png","Snow-Miku32":"Snow-Miku/3583097@2x.png","Snow-Miku33":"Snow-Miku/3583098@2x.png","Snow-Miku34":"Snow-Miku/3583099@2x.png","Snow-Miku35":"Snow-Miku/3583100@2x.png","Snow-Miku36":"Snow-Miku/3583101@2x.png","Snow-Miku37":"Snow-Miku/3583102@2x.png","Snow-Miku38":"Snow-Miku/3583103@2x.png","Snow-Miku39":"Snow-Miku/3583104@2x.png","Snow-Miku40":"Snow-Miku/3583105@2x.png"},
      enableQQ: true,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign({}, initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script>(function(){
  const bp = document.createElement('script');
  const curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  bp.dataset.pjax = ''
  const s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})()</script></div></body></html>