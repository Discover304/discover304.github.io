<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>【深度学习】自然语言处理 | Yang's Harbor</title><meta name="keywords" content="学习,记录,Python,笔记,深度学习"><meta name="author" content="✨YangSier✨,hobart.yang@qq.com"><meta name="copyright" content="✨YangSier✨"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="自然语言处理（NLP）讲义一、NLP概述1. NLP的定义NLP（Nature Language Processing，自然语言处理）是计算机学科及人工智能领域一个重要的子学科，它主要研究计算机如何处理、理解及应用人类语言。所谓自然语言，指人说的话、人写的文章，是人类在长期进化过程中形成的一套复杂的符号系统（类似于C&#x2F;Java等计算机语言则称为人造语言）。以下是关于自然语言处理常见的定义">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习】自然语言处理">
<meta property="og:url" content="https://discover304.top/2021/12/13/2021q4/113-5-dl-nlp/index.html">
<meta property="og:site_name" content="Yang&#39;s Harbor">
<meta property="og:description" content="自然语言处理（NLP）讲义一、NLP概述1. NLP的定义NLP（Nature Language Processing，自然语言处理）是计算机学科及人工智能领域一个重要的子学科，它主要研究计算机如何处理、理解及应用人类语言。所谓自然语言，指人说的话、人写的文章，是人类在长期进化过程中形成的一套复杂的符号系统（类似于C&#x2F;Java等计算机语言则称为人造语言）。以下是关于自然语言处理常见的定义">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E">
<meta property="article:published_time" content="2021-12-13T01:01:16.000Z">
<meta property="article:modified_time" content="2021-12-25T06:38:38.000Z">
<meta property="article:author" content="✨YangSier✨">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="记录">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://discover304.top/2021/12/13/2021q4/113-5-dl-nlp/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta name="google-site-verification" content="ilqpfk3vkgzDNNikz_V37-DOvRyi5wv4Hoi_eyBqvTg"/><meta name="msvalidate.01" content="49D9A50CCF9744E17274791468EDB517"/><meta name="baidu-site-verification" content="code-V24KosyVh1"/><meta name="360-site-verification" content="bd8859c3d74dfa3e8aeee9db30c94bd2"/><meta name="yandex-verification" content="f28ec9bbd50c56f5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8030f6052f2fed6a4704d96619f090d6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="/css/font.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":200,"languages":{"author":"Author: ✨YangSier✨","link":"Link: ","source":"Source: Yang's Harbor","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#ffc910","bgDark":"#02c3f6","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-25 14:38:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Yang's Harbor" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">253</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">88</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-link"></i><span> Connection</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i><span> Friends</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://aierlab.com"><i class="fa-fw fas fa-sitemap"></i><span> GroupSite</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-wrench"></i><span> Tools</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/diary"><i class="fa-fw fas fa-file-text"></i><span> Diary</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wandb.ai/"><i class="fa-fw fas fa-newspaper"></i><span> WandB</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Article</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yang's Harbor</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-link"></i><span> Connection</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i><span> Friends</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://aierlab.com"><i class="fa-fw fas fa-sitemap"></i><span> GroupSite</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-wrench"></i><span> Tools</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/diary"><i class="fa-fw fas fa-file-text"></i><span> Diary</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wandb.ai/"><i class="fa-fw fas fa-newspaper"></i><span> WandB</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Article</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【深度学习】自然语言处理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-12-13T01:01:16.000Z" title="Created 2021-12-13 09:01:16">2021-12-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-12-25T06:38:38.000Z" title="Updated 2021-12-25 14:38:38">2021-12-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NoteBook/">NoteBook</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NoteBook/PythonNote/">PythonNote</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">20.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>77min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="自然语言处理（NLP）讲义"><a href="#自然语言处理（NLP）讲义" class="headerlink" title="自然语言处理（NLP）讲义"></a>自然语言处理（NLP）讲义</h1><h2 id="一、NLP概述"><a href="#一、NLP概述" class="headerlink" title="一、NLP概述"></a>一、NLP概述</h2><h3 id="1-NLP的定义"><a href="#1-NLP的定义" class="headerlink" title="1. NLP的定义"></a>1. NLP的定义</h3><p>NLP（Nature Language Processing，自然语言处理）是计算机学科及人工智能领域一个重要的子学科，它主要研究计算机如何处理、理解及应用人类语言。所谓自然语言，指人说的话、人写的文章，是人类在长期进化过程中形成的一套复杂的符号系统（类似于C&#x2F;Java等计算机语言则称为人造语言）。以下是关于自然语言处理常见的定义：</p>
<ul>
<li>自然语言处理是计算机科学与语言中关于计算机与人类语言转换的领域。——中文维基百科</li>
<li>自然语言处理是人工智能领域中一个重要的方向。它研究实现人与计算机之间用自然语言进行有效沟通的各种理论和方法。——百度百科</li>
<li>自然语言处理研究在人与人交际中及人与计算机交际中的语言问题的一门学科。NLP要研制表示语言能力和语言应用的模型，建立计算机框架来实现这些语言模型，提出相应的方法来不断完善这种模型，并根据语言模型设计各种实用系统，以及对这些系统的评测技术。——Bill Manaris，《从人机交互的角度看自然语言处理》</li>
</ul>
<p>自然语言处理还有其它一些名称，例如：自然语言理解（Natural Language Understanding），计算机语言学（Computational Linguistics），人类语言技术（Human Language Technology）等等。</p>
<h3 id="2-NLP的主要任务"><a href="#2-NLP的主要任务" class="headerlink" title="2. NLP的主要任务"></a>2. NLP的主要任务</h3><p>NLP的主要任务可以分为两大类，一类是基于现有文本或语料的分析，另一类是生成新的文本或语料。</p>
<p><img src="https://image.discover304.top/dl/nlp/NLP_task.png" alt="LP_task"></p>
<h4 id="1）分词"><a href="#1）分词" class="headerlink" title="1）分词"></a>1）分词</h4><p>该任务将文本或语料分隔成更小的语言单元（例如，单词）。对于拉丁语系，词语之间有空格分隔，对于中文、日文等语言，分词就是一项重要的基本任务，分词直接影响对文本语义的理解。例如：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本：吉林市长春药店</span><br><span class="line">分词<span class="number">1</span>：吉林市<span class="regexp">/长春/</span>药店</span><br><span class="line">分词<span class="number">2</span>：吉林<span class="regexp">/市长/</span>春药/店</span><br></pre></td></tr></table></figure>

<h4 id="2）词义消歧"><a href="#2）词义消歧" class="headerlink" title="2）词义消歧"></a>2）词义消歧</h4><p>词义消歧是识别单词正确含义的任务。例如，在句子“The dog <u>barked</u> at the mailman”（狗对邮递员吠叫）和“Tree <u>bark</u> is sometimes used as a medicine”（树皮有时用作药物）中，单词bark有两种不同的含义。词义消歧对于诸如问答之类的任务至关重要。</p>
<h4 id="3）命名实体识别（NER）"><a href="#3）命名实体识别（NER）" class="headerlink" title="3）命名实体识别（NER）"></a>3）命名实体识别（NER）</h4><p>NER尝试从给定的文本主体或文本语料库中提取实体（例如，人物、位置和组织）。例如，句子：</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">John gave Mary <span class="literal">two</span> apples <span class="keyword">at</span> school <span class="keyword">on</span> <span class="title">Monday</span></span><br></pre></td></tr></table></figure>

<p>将转换为：</p>
<p><img src="https://image.discover304.top/dl/nlp/NER.png" alt="NER"></p>
<h4 id="4）词性标记（PoS）"><a href="#4）词性标记（PoS）" class="headerlink" title="4）词性标记（PoS）"></a>4）词性标记（PoS）</h4><p>PoS标记是将单词分配到各自对应词性的任务。它既可以是名词、动词、形容词、副词、介词等基本词、也可以是专有名词、普通名词、短语动词、动词等。</p>
<h4 id="5）文本分类"><a href="#5）文本分类" class="headerlink" title="5）文本分类"></a>5）文本分类</h4><p>文本分类有许多应用场景，例如垃圾邮件检测、新闻文章分类（例如，政治、科技和运动）和产品评论评级（即正向或负向）。我们可以用标记数据（即人工对评论标上正面或负面的标签）训练一个分类模型来实现这项任务。</p>
<h4 id="6）语言生成"><a href="#6）语言生成" class="headerlink" title="6）语言生成"></a>6）语言生成</h4><p>可以利用NLP模型来生成新的文本或语料，例如机器写作（天气预报、新闻报道、模仿唐诗），生成文本摘要等。以下是一段机器合成的”诗”：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">向塞唯何近，空令极是辞。向睹一我扇，猛绶临来惊。</span><br><span class="line">向面炎交好，荷莎正若隳。即住长非乱，休分去此垂。</span><br><span class="line">却定何人改，松仙绕绮霞。偶笑寒栖咽，长闻暖顶时。</span><br><span class="line">失个亦垂谏，守身丈韦鸿。忆及他年事，应愁一故名。</span><br><span class="line">坐忆山高道，为随夏郭间。到乱唯无己，千方得命赊。</span><br></pre></td></tr></table></figure>

<h4 id="5）问答（QA）系统"><a href="#5）问答（QA）系统" class="headerlink" title="5）问答（QA）系统"></a>5）问答（QA）系统</h4><p>QA技术具有很高的商业价值，这些技术是聊天机器人和VA（例如，Google Assistant和Apple Siri）的基础。许多公司已经采用聊天机器人来提供客户支持。以下是一段与聊天机器人的对话：</p>
<p><img src="https://image.discover304.top/dl/nlp/chat_robot.png" alt="chat_robot"></p>
<h4 id="6）机器翻译（MT）"><a href="#6）机器翻译（MT）" class="headerlink" title="6）机器翻译（MT）"></a>6）机器翻译（MT）</h4><p>机器翻译（Machine Translation，MT）指将文本由一种语言翻译成另一种语言，本质是根据一个序列，生成语义最相近的另一种语言序列。</p>
<p><img src="https://image.discover304.top/dl/nlp/MT.png" alt="MT"></p>
<h3 id="3-NLP的发展历程"><a href="#3-NLP的发展历程" class="headerlink" title="3. NLP的发展历程"></a>3. NLP的发展历程</h3><p>NLP的发展轨迹为：基于规则 → 基于统计 → 基于深度学习，其发展大致经历了4个阶段：1956年以前的萌芽期；1957<del>1970年的快速发展期；1971</del>1993年的低速发展期；1994年至今的复苏融合期。</p>
<h4 id="1）萌芽期（1956年以前）"><a href="#1）萌芽期（1956年以前）" class="headerlink" title="1）萌芽期（1956年以前）"></a>1）萌芽期（1956年以前）</h4><ul>
<li>1946年：第一台电子计算机诞生</li>
<li>1948年：Shannon把离散马尔可夫过程的概率模型应用于描述语言的自动机。接着，他又把热力学中“熵”(entropy)的概念引用于语言处理的概率算法中</li>
<li>1956年：Chomsky又提出了上下文无关语法，并把它运用到自然语言处理中</li>
</ul>
<h4 id="2）快速发展期（1957-1970）"><a href="#2）快速发展期（1957-1970）" class="headerlink" title="2）快速发展期（1957~1970）"></a>2）快速发展期（1957~1970）</h4><p>自然语言处理在这一时期很快融入了人工智能的研究领域中。由于有基于规则和基于概率这两种不同方法的存在，自然语言处理的研究在这一时期分为了两大阵营。一个是基于规则方法的符号派(symbolic)，另一个是采用概率方法的随机派(stochastic)。这一时期，两种方法的研究都取得了长足的发展。从50年代中期开始到60年代中期，以Chomsky为代表的符号派学者开始了形式语言理论和生成句法的研究，60年代末又进行了形式逻辑系统的研究。而随机派学者采用基于贝叶斯方法的统计学研究方法，在这一时期也取得了很大的进步。</p>
<p>这一时期的重要研究成果包括1959年宾夕法尼亚大学研制成功的TDAP系统，布朗美国英语语料库的建立等。1967年美国心理学家Neisser提出认知心理学的概念，直接把自然语言处理与人类的认知联系起来了。</p>
<h4 id="3）低速发展期（1971-1993）"><a href="#3）低速发展期（1971-1993）" class="headerlink" title="3）低速发展期（1971~1993）"></a>3）低速发展期（1971~1993）</h4><p>随着研究的深入，由于人们看到基于自然语言处理的应用并不能在短时间内得到解决，而一连串的新问题又不断地涌现，于是，许多人对自然语言处理的研究丧失了信心。从70年代开始，自然语言处理的研究进入了低谷时期。<br>但尽管如此，一些研究人员依旧坚持继续着他们的研究。由于他们的出色工作，自然语言处理在这一低谷时期同样取得了一些成果。70年代，基于隐马尔可夫模型(Hidden Markov Model, HMM)的统计方法在语音识别领域获得成功。80年代初，话语分析(Discourse Analysis)也取得了重大进展。之后，由于自然语言处理研究者对于过去的研究进行了反思，有限状态模型和经验主义研究方法也开始复苏。</p>
<h4 id="4）复苏融合期（1994年至今）"><a href="#4）复苏融合期（1994年至今）" class="headerlink" title="4）复苏融合期（1994年至今）"></a>4）复苏融合期（1994年至今）</h4><p>90年代中期以后，有两件事从根本上促进了自然语言处理研究的复苏与发展。一件事是90年代中期以来，计算机的速度和存储量大幅增加，为自然语言处理改善了物质基础，使得语音和语言处理的商品化开发成为可能；另一件事是1994年Internet商业化和同期网络技术的发展使得基于自然语言的信息检索和信息抽取的需求变得更加突出。以下列举除了2000年之后NLP领域的几个里程碑事件：</p>
<ul>
<li><p>2001年：神经语言模型</p>
</li>
<li><p>2008年：多任务学习</p>
</li>
<li><p>2013年： Word嵌入</p>
</li>
<li><p>2013年：NLP的神经网络</p>
</li>
<li><p>2014年：序列到序列模型</p>
</li>
<li><p>2015年：注意力机制</p>
</li>
<li><p>2015年：基于记忆的神经网络</p>
</li>
<li><p>2018年：预训练语言模型</p>
</li>
</ul>
<h3 id="4-NLP的困难与挑战"><a href="#4-NLP的困难与挑战" class="headerlink" title="4. NLP的困难与挑战"></a>4. NLP的困难与挑战</h3><h4 id="1）语言歧义"><a href="#1）语言歧义" class="headerlink" title="1）语言歧义"></a>1）语言歧义</h4><p><strong>不同分词导致的歧义</strong></p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例如：自动化研究所取得的成就</span><br><span class="line">理解一：自动化 <span class="regexp">/ 研究 /</span> 所 <span class="regexp">/ 取得 /</span> 的 / 成就</span><br><span class="line">理解二：自动化 <span class="regexp">/ 研究所 /</span> 取得 <span class="regexp">/ 的 /</span> 成就</span><br></pre></td></tr></table></figure>

<p><strong>词性歧义</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">动物保护警察</span><br></pre></td></tr></table></figure>

<p>“保护”理解成动词、名词，语义不一样</p>
<p><strong>结构歧义</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">喜欢乡下的孩子</span><br><span class="line">关于鲁迅的文章</span><br></pre></td></tr></table></figure>

<p><strong>语音歧义</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">节假日期间，所有博物馆全部（不）对外开放</span><br></pre></td></tr></table></figure>

<h4 id="2）不同语言结构差异"><a href="#2）不同语言结构差异" class="headerlink" title="2）不同语言结构差异"></a>2）不同语言结构差异</h4><p><img src="https://image.discover304.top/dl/nlp/translate_err.png" alt="translate_err"></p>
<h4 id="3）未知语言不可预测性"><a href="#3）未知语言不可预测性" class="headerlink" title="3）未知语言不可预测性"></a>3）未知语言不可预测性</h4><p>语言在不断演化，每年都有为数不少的新词语、新语料出现，给一些NLP处理任务造成困难。以下列举了几个2021年网络上出现的新词语：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">双减</span><br><span class="line">元宇宙</span><br><span class="line">绝绝子</span><br><span class="line">躺平</span><br></pre></td></tr></table></figure>

<h4 id="4）语言表达的复杂性"><a href="#4）语言表达的复杂性" class="headerlink" title="4）语言表达的复杂性"></a>4）语言表达的复杂性</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">甲：你这是什么意思？</span><br><span class="line">乙：没什么意思，意思意思。</span><br><span class="line">甲：你这就不够意思了。</span><br><span class="line">乙：小意思，小意思。</span><br><span class="line">甲：你这人真有意思。</span><br><span class="line">乙：其实也没有别的意思。</span><br><span class="line">甲：那我就不好意思了。</span><br></pre></td></tr></table></figure>

<h4 id="5）机器处理语言缺乏背景与常识"><a href="#5）机器处理语言缺乏背景与常识" class="headerlink" title="5）机器处理语言缺乏背景与常识"></a>5）机器处理语言缺乏背景与常识</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中国国家队比赛最没悬念的是乒乓球和足球，他们一个谁也打不过，另一个谁也打不过</span><br><span class="line">如果希拉里当选，她就是全世界唯一一个干过美国总统和干过美国总统的女人，克林顿也将成为全世界唯一一个干过美国总统和干过美国总统的男人</span><br></pre></td></tr></table></figure>



<h3 id="5-NLP相关知识构成"><a href="#5-NLP相关知识构成" class="headerlink" title="5. NLP相关知识构成"></a>5. NLP相关知识构成</h3><p><img src="https://image.discover304.top/dl/nlp/NLP_structure.png" alt="NLP_structure"></p>
<h3 id="6-语料库"><a href="#6-语料库" class="headerlink" title="6. 语料库"></a>6. 语料库</h3><h4 id="1）什么是语料库"><a href="#1）什么是语料库" class="headerlink" title="1）什么是语料库"></a>1）什么是语料库</h4><p>语料库（corpus）是指存放语言材料的仓库。现代的语料库是指存放在计算机里的原始语料文本或经过加工后带有语言学信息标注的语料文本。以语言的真实材料为基础来呈现语言知识，反映语言单位的用法和意义，基本以知识的原型形态表现——语言的原貌。</p>
<h4 id="2）语料库的特征"><a href="#2）语料库的特征" class="headerlink" title="2）语料库的特征"></a>2）语料库的特征</h4><ul>
<li>语料库中存放的是实际中真实出现过的语言材料</li>
<li>语料库是以计算机为载体承载语言知识的基础资源，但不等于语言知识</li>
<li>真实语料需要经过分析、处理和加工，才能成为有用的资源</li>
</ul>
<h4 id="3）语料库的作用"><a href="#3）语料库的作用" class="headerlink" title="3）语料库的作用"></a>3）语料库的作用</h4><ul>
<li>支持语言学研究和语言教学研究</li>
<li>支持NLP系统的开发</li>
</ul>
<h4 id="4）常用语料库介绍"><a href="#4）常用语料库介绍" class="headerlink" title="4）常用语料库介绍"></a>4）常用语料库介绍</h4><ul>
<li><p>北京大学计算机语言所语料库标记（中文），地址：http://opendata.pku.edu.cn/dataverse/icl</p>
</li>
<li><p>London-Lund英语口语语料库,地址：http://www.helsinki.fi/varieng/CoRD/copora.LLC/</p>
</li>
<li><p>腾讯中文语料库。包含800多万个中文词汇，其中每个词对应一个200维的向量，覆盖很多现代词汇，包括最近一两年出现的新词。采用了更大规模的数据和更好算法。https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz</p>
</li>
<li><p>中文维基百科语料库。维基百科是最常用且权威的开放网络数据集之一，作为极少数人工编辑、内容丰富、格式规范的文本语料，各类语言的维基百科在NLP中广泛应用。</p>
</li>
</ul>
<h2 id="二、传统NLP处理技术"><a href="#二、传统NLP处理技术" class="headerlink" title="二、传统NLP处理技术"></a>二、传统NLP处理技术</h2><h3 id="1-中文分词"><a href="#1-中文分词" class="headerlink" title="1. 中文分词"></a>1. 中文分词</h3><p>中文分词是一项重要的基本任务，分词直接影响对文本语义的理解。分词主要有基于规则的分词、基于统计的分词和混合分词。基于规则的分词主要是通过维护词典，在切分语句时，将语句的每个子字符串与词表中的词语进行匹配，找到则切分，找不到则不切分；基于统计的分词，主要是基于统计规则和语言模型，输出一个概率最大的分词序列（由于所需的知识尚未讲解，此处暂不讨论）；混合分词就是各种分词方式混合使用，从而提高分词准确率。下面介绍基于规则的分词。</p>
<h4 id="1）正向最大匹配法"><a href="#1）正向最大匹配法" class="headerlink" title="1）正向最大匹配法"></a>1）正向最大匹配法</h4><p>正向最大匹配法（Forward Maximum Matching，FMM）是按照从前到后的顺序对语句进行切分，其步骤为：</p>
<ul>
<li>从左向右取待分汉语句的m个字作为匹配字段，m为词典中最长词的长度；</li>
<li>查找词典进行匹配；</li>
<li>若匹配成功，则将该字段作为一个词切分出去；</li>
<li>若匹配不成功，则将该字段最后一个字去掉，剩下的字作为新匹配字段，进行再次匹配；</li>
<li>重复上述过程，直到切分所有词为止。</li>
</ul>
<h4 id="2）逆向最大匹配法"><a href="#2）逆向最大匹配法" class="headerlink" title="2）逆向最大匹配法"></a>2）逆向最大匹配法</h4><p>逆向最大匹配法（Reverse Maximum Matching， RMM）基本原理与FMM基本相同，不同的是分词的方向与FMM相反。RMM是从待分词句子的末端开始，也就是从右向左开始匹配扫描，每次取末端m个字作为匹配字段，匹配失败，则去掉匹配字段前面的一个字，继续匹配。</p>
<h4 id="3）双向最大匹配法"><a href="#3）双向最大匹配法" class="headerlink" title="3）双向最大匹配法"></a>3）双向最大匹配法</h4><p>双向最大匹配法（Bi-directional Maximum Matching，Bi-MM）是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。双向最大匹配的规则是：</p>
<ul>
<li><p>如果正反向分词结果词数不同，则取分词数量少的那个；</p>
</li>
<li><p>分词结果相同，没有歧义，返回任意一个；分词结果不同，返回其中单字数量较少的那个。</p>
</li>
</ul>
<p>【示例1】正向最大匹配分词法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正向最大匹配分词示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MM</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.window_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cut</span>(<span class="params">self, text</span>):</span><br><span class="line">        result = [] <span class="comment"># 分词结果</span></span><br><span class="line">        start = <span class="number">0</span> <span class="comment"># 起始位置</span></span><br><span class="line">        text_len = <span class="built_in">len</span>(text) <span class="comment"># 文本长度</span></span><br><span class="line"></span><br><span class="line">        dic = [<span class="string">&quot;吉林&quot;</span>, <span class="string">&quot;吉林市&quot;</span>, <span class="string">&quot;市长&quot;</span>, <span class="string">&quot;长春&quot;</span>, <span class="string">&quot;春药&quot;</span>, <span class="string">&quot;药店&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> text_len &gt; start:</span><br><span class="line">            <span class="keyword">for</span> size <span class="keyword">in</span> <span class="built_in">range</span>(self.window_size + start, start, -<span class="number">1</span>): <span class="comment"># 取最大长度，逐步比较减小</span></span><br><span class="line">                piece = text[start:size] <span class="comment"># 切片</span></span><br><span class="line">                <span class="keyword">if</span> piece <span class="keyword">in</span> dic: <span class="comment"># 在字典中</span></span><br><span class="line">                    result.append(piece) <span class="comment"># 添加到列表</span></span><br><span class="line">                    start += <span class="built_in">len</span>(piece)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>: <span class="comment"># 没在字典中，什么都不做</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(piece) == <span class="number">1</span>:</span><br><span class="line">                        result.append(piece) <span class="comment"># 单个字成词</span></span><br><span class="line">                        start += <span class="built_in">len</span>(piece)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    text = <span class="string">&quot;吉林市长春药店&quot;</span></span><br><span class="line">    tk = MM() <span class="comment"># 实例化对象</span></span><br><span class="line">    result = tk.cut(text)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">&#x27;吉林市</span>&#x27;, <span class="symbol">&#x27;长春</span>&#x27;, <span class="symbol">&#x27;药店</span>&#x27;]</span><br></pre></td></tr></table></figure>



<p>【示例2】逆向最大匹配分词法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逆向最大匹配分词示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RMM</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.window_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cut</span>(<span class="params">self, text</span>):</span><br><span class="line">        result = [] <span class="comment"># 分词结果</span></span><br><span class="line">        start = <span class="built_in">len</span>(text) <span class="comment"># 起始位置</span></span><br><span class="line">        text_len = <span class="built_in">len</span>(text) <span class="comment"># 文本长度</span></span><br><span class="line"></span><br><span class="line">        dic = [<span class="string">&quot;吉林&quot;</span>, <span class="string">&quot;吉林市&quot;</span>, <span class="string">&quot;市长&quot;</span>, <span class="string">&quot;长春&quot;</span>, <span class="string">&quot;春药&quot;</span>, <span class="string">&quot;药店&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> size <span class="keyword">in</span> <span class="built_in">range</span>(self.window_size, <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">                piece = text[start-size:start] <span class="comment"># 切片</span></span><br><span class="line">                <span class="keyword">if</span> piece <span class="keyword">in</span> dic: <span class="comment"># 在字典中</span></span><br><span class="line">                    result.append(piece) <span class="comment"># 添加到列表</span></span><br><span class="line">                    start -= <span class="built_in">len</span>(piece)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>: <span class="comment"># 没在字典中</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(piece) == <span class="number">1</span>:</span><br><span class="line">                        result.append(piece) <span class="comment"># 单个字成词</span></span><br><span class="line">                        start -= <span class="built_in">len</span>(piece)</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">        result.reverse()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    text = <span class="string">&quot;吉林市长春药店&quot;</span></span><br><span class="line">    tk = RMM() <span class="comment"># 实例化对象</span></span><br><span class="line">    result = tk.cut(text)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">&#x27;吉林市</span>&#x27;, <span class="symbol">&#x27;长春</span>&#x27;, <span class="symbol">&#x27;药店</span>&#x27;]</span><br></pre></td></tr></table></figure>



<p>【示例3】Jieba库分词</p>
<p>Jieba是一款开源的、功能丰富、使用简单的中文分词工具库，它提供了三种分词模式：</p>
<ul>
<li>精确模式：试图将句子最精确地分词，适合文本分析</li>
<li>全模式：把句子中所有可以成词的词语分割出来，速度快，但有重复词和歧义</li>
<li>搜索引擎模式：在精确模式基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词</li>
</ul>
<p>使用Jieba库之前，需要进行安装：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">pip</span> install jieba==<span class="number">0</span>.<span class="number">42</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>分词示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jieba分词示例</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;吉林市长春药店&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全模式</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> seg_list:</span><br><span class="line">    <span class="built_in">print</span>(word, end=<span class="string">&quot;/&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精确模式</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> seg_list:</span><br><span class="line">    <span class="built_in">print</span>(word, end=<span class="string">&quot;/&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索引擎模式</span></span><br><span class="line">seg_list = jieba.cut_for_search(text)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> seg_list:</span><br><span class="line">    <span class="built_in">print</span>(word, end=<span class="string">&quot;/&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">吉林<span class="regexp">/吉林市/</span>市长<span class="regexp">/长春/</span>春药<span class="regexp">/药店/</span></span><br><span class="line">吉林市<span class="regexp">/长春/</span>药店/</span><br><span class="line">吉林<span class="regexp">/吉林市/</span>长春<span class="regexp">/药店/</span></span><br></pre></td></tr></table></figure>



<p>【示例4】文本高频词汇提取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过tf-idf提取高频词汇</span></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件内容</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;gbk&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        content = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            line = line.strip()</span><br><span class="line">            content += line</span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计词频，返回最高前10位词频列表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tf</span>(<span class="params">words, topk=<span class="number">10</span></span>):</span><br><span class="line">    tf_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> tf_dict.items():</span><br><span class="line">            tf_dict[w] = tf_dict.get(w, <span class="number">0</span>) + <span class="number">1</span>  <span class="comment"># 获取词频并加1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 倒序排列</span></span><br><span class="line">    new_list = <span class="built_in">sorted</span>(tf_dict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_list[:topk]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stop_words</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 样本文件</span></span><br><span class="line">    fname = <span class="string">&quot;d:\\NLP_DATA\\chap_3\\news\\C000008\\11.txt&quot;</span></span><br><span class="line">    <span class="comment"># 读取文件内容</span></span><br><span class="line">    corpus = get_content(fname)</span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    tmp_list = <span class="built_in">list</span>(jieba.cut(corpus))</span><br><span class="line">    <span class="comment"># 去除停用词</span></span><br><span class="line">    stop_words = get_stop_words(<span class="string">&quot;d:\\NLP_DATA\\chap_3\\stop_words.utf8&quot;</span>)</span><br><span class="line">    split_words = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> tmp_list:</span><br><span class="line">        <span class="keyword">if</span> tmp <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            split_words.append(tmp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(&quot;样本:\n&quot;, corpus)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n 分词结果: \n&quot;</span> + <span class="string">&quot;/&quot;</span>.join(split_words))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计高频词</span></span><br><span class="line">    tf_list = get_tf(split_words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n top10词 \n:&quot;</span>, <span class="built_in">str</span>(tf_list))</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分词结果:</span><br><span class="line">焦点<span class="regexp">/个股/</span>苏宁<span class="regexp">/电器/</span><span class="number">002024</span><span class="regexp">/该股/</span>早市<span class="regexp">/涨停/</span>开盘<span class="regexp">/其后/</span>获利盘<span class="regexp">/抛/</span>压下<span class="regexp">/略有/</span>回落<span class="regexp">/强大/</span>买盘<span class="regexp">/推动/</span>下该<span class="regexp">/股/</span>已经<span class="regexp">/再次/</span>封于<span class="regexp">/涨停/</span>主力<span class="regexp">/资金/</span>积极<span class="regexp">/拉升/</span>意愿<span class="regexp">/相当/</span>强烈<span class="regexp">/盘面/</span>解析<span class="regexp">/技术/</span>层面<span class="regexp">/早市/</span>指数<span class="regexp">/小幅/</span>探低<span class="regexp">/迅速/</span>回升<span class="regexp">/中石化/</span>强势<span class="regexp">/上扬/</span>带动<span class="regexp">/指数/</span>已经<span class="regexp">/成功/</span>翻红<span class="regexp">/多头/</span>实力<span class="regexp">/之强/</span>令人<span class="regexp">/瞠目结舌/</span>市场<span class="regexp">/高度/</span>繁荣<span class="regexp">/情形/</span>投资者<span class="regexp">/需谨慎/</span>操作<span class="regexp">/必竟/</span>持续<span class="regexp">/上攻/</span>已经<span class="regexp">/消耗/</span>大量<span class="regexp">/多头/</span>动能<span class="regexp">/盘中/</span>热点<span class="regexp">/来看/</span>相比<span class="regexp">/周二/</span>略有<span class="regexp">/退温/</span>依然<span class="regexp">/看到/</span>目前<span class="regexp">/热点/</span>效应<span class="regexp">/外扩散/</span>迹象<span class="regexp">/相当/</span>明显<span class="regexp">/高度/</span>活跌<span class="regexp">/板块/</span>已经<span class="regexp">/前期/</span>有色金属<span class="regexp">/金融/</span>地产股<span class="regexp">/向外/</span>扩大<span class="regexp">/军工/</span>概念<span class="regexp">/航天航空/</span>操作<span class="regexp">/思路/</span>短线<span class="regexp">/依然/</span>需<span class="regexp">/规避/</span>一下<span class="regexp">/技术性/</span>回调<span class="regexp">/风险/</span>盘中<span class="regexp">/切记/</span>不可/追高</span><br><span class="line"></span><br><span class="line">top10词:</span><br><span class="line"> [(<span class="string">&#x27;已经&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;早市&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;涨停&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;略有&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;相当&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;指数&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;多头&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;高度&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;操作&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;盘中&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>



<h3 id="2-词性标注"><a href="#2-词性标注" class="headerlink" title="2. 词性标注"></a>2. 词性标注</h3><h4 id="1）什么是词性标注"><a href="#1）什么是词性标注" class="headerlink" title="1）什么是词性标注"></a>1）什么是词性标注</h4><p>词性是词语的基本语法属性，通常也称为词类。词性标注是判定给定文本或语料中每个词语的词性。有很多词语在不同语境中表现为不同的词性，这就为词性标注带来很大的困难。另一方面，从整体上看，大多数词语，尤其是实词，一般只有一到两个词性，其中一个词性的使用频率远远大于另一个。</p>
<h4 id="2）词性标注的原理"><a href="#2）词性标注的原理" class="headerlink" title="2）词性标注的原理"></a>2）词性标注的原理</h4><p>词性标注最主要方法同分词一样，将其作为一个序列生成问题来处理。使用序列模型，根据输入的文本，生成一个对应的词性序列。</p>
<h4 id="3）词性标注规范"><a href="#3）词性标注规范" class="headerlink" title="3）词性标注规范"></a>3）词性标注规范</h4><p>词性标注要有一定的标注规范，如将名词、形容词、动词表示为”n”, “adj”, “v”等。中文领域尚无统一的标注标准，较为主流的有北大词性标注集和宾州词性标注集。以下是北大词性标注集部分词性表示：</p>
<p><img src="https://image.discover304.top/dl/nlp/pku_pos_1.jpg" alt="pku_pos_1"></p>
<p><img src="https://image.discover304.top/dl/nlp/pku_pos_2.jpg" alt="pku_pos_2"></p>
<h4 id="4）Jieba库词性标注"><a href="#4）Jieba库词性标注" class="headerlink" title="4）Jieba库词性标注"></a>4）Jieba库词性标注</h4><p>Jieba库提供了词性标注功能，采用结合规则和统计的方式，具体为在词性标注的过程中，词典匹配和HMM共同作用。词性标注流程如下：</p>
<p>第一步：根据正则表达式判断文本是否为汉字；</p>
<p>第二步：如果判断为汉字，构建HMM模型计算最大概率，在词典中查找分出的词性，若在词典中未找到，则标记为”未知”；</p>
<p>第三步：若不如何上面的正则表达式，则继续通过正则表达式进行判断，分别赋予”未知”、”数词“或”英文”。</p>
<p>【示例】Jieba库实现词性标注</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> psg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pos</span>(<span class="params">text</span>):</span><br><span class="line">    results = psg.cut(text)</span><br><span class="line">    <span class="keyword">for</span> w, t <span class="keyword">in</span> results:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s/%s&quot;</span> % (w, t), end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;呼伦贝尔大草原&quot;</span></span><br><span class="line">pos(text)</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;梅兰芳大剧院里星期六晚上有演出&quot;</span></span><br><span class="line">pos(text)</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">呼伦贝尔/nr 大/a 草原/n </span><br><span class="line">梅兰芳/nr 大/a 剧院/n 里/f 星期六/t 晚上/t 有/v 演出/v </span><br></pre></td></tr></table></figure>



<h3 id="3-命名实体识别（NER）"><a href="#3-命名实体识别（NER）" class="headerlink" title="3. 命名实体识别（NER）"></a>3. 命名实体识别（NER）</h3><p>命名实体识别（Named Entities Recognition，NER）也是自然语言处理的一个基础任务，是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。其目的是识别语料中人名、地名、组织机构名等命名实体，实体类型包括3大类（实体类、时间类和数字类）和7小类（人名、地名、组织机构名、时间、日期、货币和百分比）。中文命名实体识别主要有以下难点：</p>
<p>（1）各类命名实体的数量众多。</p>
<p>（2）命名实体的构成规律复杂。</p>
<p>（2）嵌套情况复杂。</p>
<p>（4）长度不确定。</p>
<p>命名实体识别方法有：</p>
<p>（1）基于规则的命名实体识别。规则加词典是早期命名实体识别中最行之有效的方式。其依赖手工规则的系统，结合命名实体库，对每条规则进行权重赋值，然后通过实体与规则的相符情况来进行类型判断。这种方式可移植性差、更新维护困难等问题。</p>
<p>（2）基于统计的命名实体识别。基于统计的命名实体识别方法有：隐马尔可夫模型、最大熵模型、条件随机场等。其主要思想是基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决。基于统计的方法对语料库的依赖比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少，这是该方法的一大制约。</p>
<p>（3）基于深度学习的方法。利用深度学习模型，预测词（或字）是否为命名实体，并预测出起始、结束位置。</p>
<p>（4）混合方法。将前面介绍的方法混合使用。</p>
<p>命名实体识别在深度学习部分有专门案例进行探讨和演示。</p>
<h3 id="4-关键词提取"><a href="#4-关键词提取" class="headerlink" title="4. 关键词提取"></a>4. 关键词提取</h3><p>关键词提取是提取出代表文章重要内容的一组词，对文本聚类、分类、自动摘要起到重要作用。此外，关键词提取还能使人们便捷地浏览和获取信息。现实中大量文本不包含关键词，自动提取关检测技术具有重要意义和价值。关键词提取包括有监督学习、无监督学习方法两类。</p>
<p>有监督关键词提取。该方法主要通过分类方式进行，通过构建一个较为丰富完整的词表，然后通过判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果。该方法能获取较高的精度，但需要对大量样本进行标注，人工成本过高。另外，现在每天都有大量新的信息出现，固定词表很难将新信息内容表达出来，但人工实时维护词表成本过高。所以，有监督学习关键词提取方法有较明显的缺陷。</p>
<p>无监督关键词提取。相对于有监督关键词提取，无监督方法对数据要求低得多，既不需要人工维护词表，也不需要人工标注语料辅助训练。因此，在实际应用中更受青睐。这里主要介绍无监督关键词提取算法，包括TF-IDF算法，TextRank算法和主题模型算法。</p>
<h4 id="1）TF-IDF算法"><a href="#1）TF-IDF算法" class="headerlink" title="1）TF-IDF算法"></a>1）TF-IDF算法</h4><p>TF-IDF（Term Frequency-Inverse Document Frequency，词频-逆文档频率）是一种基于传统的统计计算方法，常用于评估一个文档集中一个词对某份文档的重要程度。其基本思想是：一个词语在文档中出现的次数越多、出现的文档越少，语义贡献度越大（对文档区分能力越强）。TF-IDF表达式由两部分构成，词频、逆文档频率。词频定义为：<br>$$<br>TF_{ij} &#x3D; \frac{n_{ji}}{\sum_k n_{kj}}<br>$$<br>其中，$n_{ij}$表示词语i在文档j中出现的次数，分母$\sum_k n_{kj}$表示所有文档总次数。逆文档频率定义为：<br>$$<br>IDF_i &#x3D; log(\frac{|D|}{|D_i| + 1})<br>$$<br>其中，$|D|$为文档总数，$D_i$为文档中出现词i的文档数量，分母加1是避免分母为0的情况（称为拉普拉斯平滑），TF-IDF算法是将TF和IDF综合使用，表达式为：<br>$$<br>TF-IDF &#x3D; TF_{ij} \times IDF_i &#x3D;\frac{n_{ji}}{\sum_k n_{kj}} \times log(\frac{|D|}{|D_i| + 1})<br>$$<br>由公式可知，词频越大，该值越大；出现的文档数越多（说明该词越通用），逆文档频率越接近0，语义贡献度越低。例如有以下文本：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">世界献血日，学校团体、献血服务志愿者等可到血液中心参观检验加工过程，我们会对检验结果进行公示，同时血液的价格也将进行公示。</span><br></pre></td></tr></table></figure>

<p>以上文本词语总数为30，计算几个词的词频：<br>$$<br>TF_{献血} &#x3D; 2 &#x2F; 30 \approx 0.067 \<br>TF_{血液} &#x3D; 2 &#x2F; 30 \approx 0.067 \<br>TF_{进行} &#x3D; 2 &#x2F; 30 \approx 0.067 \<br>TF_{公示} &#x3D; 2 &#x2F; 30 \approx 0.067<br>$$<br>假设出现献血、血液、进行、公示文档数量分别为10、15、100、50，根据TF-IDF计算公式，得：<br>$$<br>TF-IDF_{献血} &#x3D; 0.067 * log(1000&#x2F;10) &#x3D; 0.067 * 2 &#x3D; 0.134\<br>TF-IDF_{血液} &#x3D; 0.067 * log(1000&#x2F;15) &#x3D; 0.067 * 1.824 &#x3D; 0.1222 \<br>TF-IDF_{进行} &#x3D; 0.067 * log(1000&#x2F;100) &#x3D; 0.067 * 1 &#x3D; 0.067 \<br>TF-IDF_{公示} &#x3D; 0.067 * log(1000&#x2F;50) &#x3D; 0.067 * 1.30 &#x3D; 0.08717<br>$$<br>“献血”、“血液”的TF-IDF值最高，所以为最适合这篇文档的关键词。</p>
<h4 id="2）TextRank算法"><a href="#2）TextRank算法" class="headerlink" title="2）TextRank算法"></a>2）TextRank算法</h4><p>与TF-IDF不一样，TextRank算法可以脱离于语料库，仅对单篇文档进行分析就可以提取该文档的关键词，这也是TextRank算法的一个重要特点。TextRank算法最早用于文档的自动摘要，基于句子维度的分析，利用算法对每个句子进行打分，挑选出分数最高的n个句子作为文档的关键句，以达到自动摘要的效果。</p>
<p>TextRank算法的基本思想来源于Google的PageRank算法，该算法是Google创始人拉里·佩奇和希尔盖·布林于1997年构建早期的搜索系统原型时提出的链接分析法，用于评价搜索系统各覆盖网页重要性的一种方法。随着Google的成功，该算法也称为其它搜索引擎和学术界十分关注的计算模型。</p>
<p><img src="https://image.discover304.top/dl/nlp/PageRank2.png" alt="PageRank"></p>
<p>PageRank基本思想有两条：</p>
<ul>
<li>链接数量。一个网页被越多的其它网页链接，说明这个网页越重要</li>
<li>链接质量。一个网页被一个越高权值的网页链接，也能表名这个网页越重要</li>
</ul>
<p>基于上述思想，一个网页的PageRank计算公式可以表示为：<br>$$<br>S(V_i) &#x3D; \sum_{j \in In(V_i)} \Bigg( \frac{1}{Out(V_j)} \times S(V_j) \Bigg)<br>$$<br>其中，$In(V_i)$为$V_i$的入链集合，$Out(V_j)$为$V_j$的出链集合，$|Out(V_j)|$为出链的数量。因为每个网页要将它自身的分数平均贡献给每个出链，则$\Bigg( \frac{1}{Out(V_j)} \times S(V_j) \Bigg)$即为$V_i$贡献给$V_j$的分数。将所有入链贡献给它的分数全部加起来，就是$V_i$自身的得分。算法开始时，将所有页面的得分均初始化为1。</p>
<p>对于一些孤立页面，可能链入、链出的页面数量为0，为了避免这种情况，对公式进行了改造，加入了一个阻尼系数$d$，这样，即使孤立页面也有一个得分。改造后的公式如下：<br>$$<br>S(V_i) &#x3D; (1 - d) + d \times \sum_{j \in In(V_i)} \Bigg( \frac{1}{Out(V_j)} \times S(V_j) \Bigg)<br>$$<br>以上就是PageRank的理论，也是TextRank的理论基础，不同于的是TextRank不需要与文档中的所有词进行链接，而是采用一个窗口大小，在窗口中的词互相都有链接关系。例如对下面的文本进行窗口划分：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">世界献血日，学校团体、献血服务志愿者等可到血液中心参观检验加工过程，我们会对检验结果进行公示，同时血液的价格也将进行公示。</span><br></pre></td></tr></table></figure>

<p>如果将窗口大小设置为5，则可得到如下计算窗口：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[世界，献血，日，学校，团体]</span></span><br><span class="line"><span class="string">[献血，日，学校，团体，献血]</span></span><br><span class="line"><span class="string">[日，学校，团体，献血，服务]</span></span><br><span class="line"><span class="string">[学校，团体，献血，服务，志愿者]</span></span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>每个窗口内所有词之间都有链接关系，如[世界]和[献血，日，学校，团体]之间有链接关系。得到了链接关系，就可以套用TextRank公式，计算每个词的得分，最后选择得分最高的N个词作为文档的关键词。</p>
<h4 id="3）关键词提取示例"><a href="#3）关键词提取示例" class="headerlink" title="3）关键词提取示例"></a>3）关键词提取示例</h4><p>本案例演示了通过自定义TF-IDF、调用TextRank API实现关键字提取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> psg</span><br><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models</span><br><span class="line"><span class="keyword">from</span> jieba <span class="keyword">import</span> analyse</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停用词表加载方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stopword_list</span>():</span><br><span class="line">    <span class="comment"># 停用词表存储路径，每一行为一个词，按行读取进行加载</span></span><br><span class="line">    <span class="comment"># 进行编码转换确保匹配准确率</span></span><br><span class="line">    stop_word_path = <span class="string">&#x27;../data/stopword.txt&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(stop_word_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    stopword_list = [sw.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>) <span class="keyword">for</span> sw <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">return</span> stopword_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">word_filter</span>(<span class="params">seg_list</span>):</span><br><span class="line">    filter_list = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> seg_list:</span><br><span class="line">        <span class="comment"># 过滤停用词表中的词，以及长度为&lt;2的词</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word <span class="keyword">in</span> stopword_list <span class="keyword">and</span> <span class="built_in">len</span>(word) &gt; <span class="number">1</span>:</span><br><span class="line">            filter_list.append(word)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filter_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载，pos为是否词性标注的参数，corpus_path为数据集路径</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">corpus_path</span>):</span><br><span class="line">    <span class="comment"># 调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词</span></span><br><span class="line">    doc_list = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(corpus_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>):  <span class="comment"># 循环读取一行(一行即一个文档)</span></span><br><span class="line">        content = line.strip()  <span class="comment"># 去空格</span></span><br><span class="line">        seg_list = jieba.cut(content)  <span class="comment"># 分词</span></span><br><span class="line">        filter_list = word_filter(seg_list)  <span class="comment"># 去除停用词</span></span><br><span class="line">        doc_list.append(filter_list)  <span class="comment"># 将分词后的内容添加到列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> doc_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># idf值统计方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_idf</span>(<span class="params">doc_list</span>):</span><br><span class="line">    idf_dic = &#123;&#125;</span><br><span class="line">    tt_count = <span class="built_in">len</span>(doc_list)  <span class="comment"># 总文档数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个词出现的文档数</span></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list:</span><br><span class="line">        doc_set = <span class="built_in">set</span>(doc)  <span class="comment"># 将词推入集合去重</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> doc_set:  <span class="comment"># 词语在文档中</span></span><br><span class="line">            idf_dic[word] = idf_dic.get(word, <span class="number">0.0</span>) + <span class="number">1.0</span>  <span class="comment"># 文档数加1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按公式转换为idf值，分母加1进行平滑处理</span></span><br><span class="line">    <span class="keyword">for</span> word, doc_cnt <span class="keyword">in</span> idf_dic.items():</span><br><span class="line">        idf_dic[word] = math.log(tt_count / (<span class="number">1.0</span> + doc_cnt))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值</span></span><br><span class="line">    default_idf = math.log(tt_count / (<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idf_dic, default_idf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TfIdf</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, idf_dic, default_idf, word_list, keyword_num</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        TfIdf类构造方法</span></span><br><span class="line"><span class="string">        :param idf_dic: 训练好的idf字典</span></span><br><span class="line"><span class="string">        :param default_idf: 默认idf值</span></span><br><span class="line"><span class="string">        :param word_list: 待提取文本</span></span><br><span class="line"><span class="string">        :param keyword_num: 关键词数量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.word_list = word_list</span><br><span class="line">        self.idf_dic, self.default_idf = idf_dic, default_idf <span class="comment"># 逆文档频率</span></span><br><span class="line">        self.tf_dic = self.get_tf_dic()  <span class="comment"># 词频</span></span><br><span class="line">        self.keyword_num = keyword_num</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计tf值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_tf_dic</span>(<span class="params">self</span>):</span><br><span class="line">        tf_dic = &#123;&#125;  <span class="comment"># 词频字典</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> self.word_list:</span><br><span class="line">            tf_dic[word] = tf_dic.get(word, <span class="number">0.0</span>) + <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        total = <span class="built_in">len</span>(self.word_list)  <span class="comment"># 词语总数</span></span><br><span class="line">        <span class="keyword">for</span> word, word_cnt <span class="keyword">in</span> tf_dic.items():</span><br><span class="line">            tf_dic[word] = <span class="built_in">float</span>(word_cnt) / total</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf_dic</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按公式计算tf-idf</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_tfidf</span>(<span class="params">self</span>):</span><br><span class="line">        tfidf_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> self.word_list:</span><br><span class="line">            idf = self.idf_dic.get(word, self.default_idf)</span><br><span class="line">            tf = self.tf_dic.get(word, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            tfidf = tf * idf  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">            tfidf_dic[word] = tfidf</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据tf-idf排序，去排名前keyword_num的词作为关键词</span></span><br><span class="line">        s_list = <span class="built_in">sorted</span>(tfidf_dic.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># print(s_list)</span></span><br><span class="line">        top_list = s_list[:self.keyword_num]  <span class="comment"># 切出前N个</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> top_list:</span><br><span class="line">            <span class="built_in">print</span>(k + <span class="string">&quot;, &quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tfidf_extract</span>(<span class="params">word_list, keyword_num=<span class="number">20</span></span>):</span><br><span class="line">    doc_list = load_data(<span class="string">&#x27;../data/corpus.txt&#x27;</span>)  <span class="comment"># 读取文件内容</span></span><br><span class="line">    <span class="comment"># print(doc_list)</span></span><br><span class="line">    idf_dic, default_idf = train_idf(doc_list) <span class="comment"># 计算逆文档频率</span></span><br><span class="line"></span><br><span class="line">    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)</span><br><span class="line">    tfidf_model.get_tfidf()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">textrank_extract</span>(<span class="params">text, keyword_num=<span class="number">20</span></span>):</span><br><span class="line">    keywords = analyse.textrank(text, keyword_num)</span><br><span class="line">    <span class="comment"># 输出抽取出的关键词</span></span><br><span class="line">    <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">        <span class="built_in">print</span>(keyword + <span class="string">&quot;, &quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">global</span> stopword_list</span><br><span class="line"></span><br><span class="line">    text = <span class="string">&quot;&quot;&quot;在中国共产党百年华诞的重要时刻，在“两个一百年”奋斗目标历史交汇关键节点，</span></span><br><span class="line"><span class="string">    党的十九届六中全会的召开具有重大历史意义。全会审议通过的《决议》全面系统总结了党的百年奋斗</span></span><br><span class="line"><span class="string">    重大成就和历史经验，特别是着重阐释了党的十八大以来党和国家事业取得的历史性成就、发生的历史性变革，</span></span><br><span class="line"><span class="string">    充分彰显了中国共产党的历史自觉与历史自信。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    stopword_list = get_stopword_list()</span><br><span class="line"></span><br><span class="line">    seg_list = jieba.cut(text)  <span class="comment"># 分词</span></span><br><span class="line">    filter_list = word_filter(seg_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TF-IDF提取关键词</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;TF-IDF模型结果：&#x27;</span>)</span><br><span class="line">    tfidf_extract(filter_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TextRank提取关键词</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;TextRank模型结果：&#x27;</span>)</span><br><span class="line">    textrank_extract(text)</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TF-IDF模型结果：</span><br><span class="line">历史, 中国共产党, 百年, 历史性, 华诞, 一百年, 奋斗目标, 交汇, 节点, 十九, 六中全会, 全会, 奋斗, 重大成就, 着重, 阐释, 十八, 党和国家, 成就, 变革, </span><br><span class="line"></span><br><span class="line">TextRank模型结果：</span><br><span class="line">历史, 历史性, 意义, 成就, 决议, 审议, 发生, 系统, 总结, 全面, 节点, 关键, 交汇, 召开, 具有, 全会, 取得, 事业, 自信, 变革, </span><br></pre></td></tr></table></figure>



<h3 id="6-综合案例"><a href="#6-综合案例" class="headerlink" title="6. 综合案例"></a>6. 综合案例</h3><h4 id="1）垃圾邮件分类"><a href="#1）垃圾邮件分类" class="headerlink" title="1）垃圾邮件分类"></a>1）垃圾邮件分类</h4><ul>
<li>数据集介绍：包含5000份正常邮件、5001份垃圾邮件的样本</li>
<li>文本特征处理方式：采用TF-IDF作为文本特征值</li>
<li>模型选择：朴素贝叶斯、支持向量机模型</li>
<li>基本流程：读取数据 → 去除停用词和特殊符号 → 计算TF-IDF特征值 → 模型训练 → 预测 → 打印结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 利用TF-IDF特征、朴素贝叶斯/支持向量机实现垃圾邮件分类</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> sklearn.model_selection <span class="keyword">as</span> ms</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">label_name_map = [<span class="string">&quot;垃圾邮件&quot;</span>, <span class="string">&quot;正常邮件&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_text</span>(<span class="params">text</span>):</span><br><span class="line">    tokens = jieba.cut(text)  <span class="comment"># 分词</span></span><br><span class="line">    tokens = [token.strip() <span class="keyword">for</span> token <span class="keyword">in</span> tokens]  <span class="comment"># 去空格</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_special_characters</span>(<span class="params">text</span>):</span><br><span class="line">    tokens = tokenize_text(text)</span><br><span class="line">    <span class="comment"># escape函数对字符进行转义处理</span></span><br><span class="line">    <span class="comment"># compile函数用于编译正则表达式，生成一个 Pattern 对象</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;[&#123;&#125;]&#x27;</span>.<span class="built_in">format</span>(re.escape(string.punctuation)))</span><br><span class="line">    <span class="comment"># filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表</span></span><br><span class="line">    <span class="comment"># sub函数进行正则匹配字符串替换</span></span><br><span class="line">    filtered_tokens = <span class="built_in">filter</span>(<span class="literal">None</span>, [pattern.sub(<span class="string">&#x27;&#x27;</span>, token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line">    filtered_text = <span class="string">&#x27; &#x27;</span>.join(filtered_tokens)</span><br><span class="line">    <span class="keyword">return</span> filtered_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_stopwords</span>(<span class="params">text</span>):</span><br><span class="line">    tokens = tokenize_text(text)  <span class="comment"># 分词、去空格</span></span><br><span class="line">    filtered_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> stopword_list]  <span class="comment"># 去除停用词</span></span><br><span class="line">    filtered_text = <span class="string">&#x27;&#x27;</span>.join(filtered_tokens)</span><br><span class="line">    <span class="keyword">return</span> filtered_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 规范化处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_corpus</span>(<span class="params">corpus</span>):</span><br><span class="line">    result = []  <span class="comment"># 处理结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> corpus:  <span class="comment"># 遍历每个词汇</span></span><br><span class="line">        text = remove_special_characters(text)  <span class="comment"># 去除标点符号</span></span><br><span class="line">        text = remove_stopwords(text)  <span class="comment"># 去除停用词</span></span><br><span class="line">        result.append(text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tfidf_extractor</span>(<span class="params">corpus</span>):</span><br><span class="line">    vectorizer = TfidfVectorizer(min_df=<span class="number">1</span>,</span><br><span class="line">                                 norm=<span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                                 smooth_idf=<span class="literal">True</span>,</span><br><span class="line">                                 use_idf=<span class="literal">True</span>)</span><br><span class="line">    features = vectorizer.fit_transform(corpus)</span><br><span class="line">    <span class="keyword">return</span> vectorizer, features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    获取数据</span></span><br><span class="line"><span class="string">    :return: 文本数据，对应的labels</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    corpus = []  <span class="comment"># 邮件内容</span></span><br><span class="line">    labels = []  <span class="comment"># 标签(0-垃圾邮件 1-正常邮件)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正常邮件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data/ham_data.txt&quot;</span>, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            corpus.append(line)</span><br><span class="line">            labels.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 垃圾邮件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data/spam_data.txt&quot;</span>, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            corpus.append(line)</span><br><span class="line">            labels.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corpus, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤空文档</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_empty_docs</span>(<span class="params">corpus, labels</span>):</span><br><span class="line">    filtered_corpus = []</span><br><span class="line">    filtered_labels = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> doc, label <span class="keyword">in</span> <span class="built_in">zip</span>(corpus, labels):</span><br><span class="line">        <span class="keyword">if</span> doc.strip():</span><br><span class="line">            filtered_corpus.append(doc)</span><br><span class="line">            filtered_labels.append(label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_corpus, filtered_labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算并打印分类指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_metrics</span>(<span class="params">true_labels, predicted_labels</span>):</span><br><span class="line">    <span class="comment"># Accuracy</span></span><br><span class="line">    accuracy = metrics.accuracy_score(true_labels, predicted_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Precision</span></span><br><span class="line">    precision = metrics.precision_score(true_labels,</span><br><span class="line">                                        predicted_labels,</span><br><span class="line">                                        average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recall</span></span><br><span class="line">    recall = metrics.recall_score(true_labels,</span><br><span class="line">                                  predicted_labels,</span><br><span class="line">                                  average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># F1</span></span><br><span class="line">    f1 = metrics.f1_score(true_labels,</span><br><span class="line">                          predicted_labels,</span><br><span class="line">                          average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正确率: %.2f, 查准率: %.2f, 召回率: %.2f, F1: %.2f&quot;</span> % (accuracy, precision, recall, f1))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">global</span> stopword_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取停用词</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;dict/stop_words.utf8&quot;</span>, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        stopword_list = f.readlines()</span><br><span class="line"></span><br><span class="line">    corpus, labels = get_data()  <span class="comment"># 加载数据</span></span><br><span class="line">    corpus, labels = remove_empty_docs(corpus, labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;总的数据量:&quot;</span>, <span class="built_in">len</span>(labels))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印前N个样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;label:&quot;</span>, labels[i], <span class="string">&quot; 邮件内容:&quot;</span>, corpus[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数据进行划分</span></span><br><span class="line">    train_corpus, test_corpus, train_labels, test_labels = \</span><br><span class="line">        ms.train_test_split(corpus,</span><br><span class="line">                            labels,</span><br><span class="line">                            test_size=<span class="number">0.10</span>,</span><br><span class="line">                            random_state=<span class="number">36</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 规范化处理</span></span><br><span class="line">    norm_train_corpus = normalize_corpus(train_corpus)</span><br><span class="line">    norm_test_corpus = normalize_corpus(test_corpus)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># tfidf 特征</span></span><br><span class="line">    <span class="comment">## 先计算tf-idf</span></span><br><span class="line">    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)</span><br><span class="line">    <span class="comment">## 再用刚刚训练的tf-idf模型计算测试集tf-idf</span></span><br><span class="line">    tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)</span><br><span class="line">    <span class="comment"># print(tfidf_test_features)</span></span><br><span class="line">    <span class="comment"># print(tfidf_test_features)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于tfidf的多项式朴素贝叶斯模型</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;基于tfidf的贝叶斯模型&quot;</span>)</span><br><span class="line">    nb_model = MultinomialNB()  <span class="comment"># 多分类朴素贝叶斯模型</span></span><br><span class="line">    nb_model.fit(tfidf_train_features, train_labels)  <span class="comment"># 训练</span></span><br><span class="line">    mnb_pred = nb_model.predict(tfidf_test_features)  <span class="comment"># 预测</span></span><br><span class="line">    print_metrics(true_labels=test_labels, predicted_labels=mnb_pred)  <span class="comment"># 打印测试集下的分类指标</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于tfidf的支持向量机模型</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;基于tfidf的支持向量机模型&quot;</span>)</span><br><span class="line">    svm_model = SGDClassifier()</span><br><span class="line">    svm_model.fit(tfidf_train_features, train_labels)  <span class="comment"># 训练</span></span><br><span class="line">    svm_pred = svm_model.predict(tfidf_test_features)  <span class="comment"># 预测</span></span><br><span class="line">    print_metrics(true_labels=test_labels, predicted_labels=svm_pred)  <span class="comment"># 打印测试集下的分类指标</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印测试结果</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> text, label, pred_lbl <span class="keyword">in</span> <span class="built_in">zip</span>(test_corpus, test_labels, svm_pred):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;真实类别:&#x27;</span>, label_name_map[<span class="built_in">int</span>(label)], <span class="string">&#x27; 预测结果:&#x27;</span>, label_name_map[<span class="built_in">int</span>(pred_lbl)])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;邮件内容【&#x27;</span>, text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>), <span class="string">&#x27;】&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">10</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">基于tfidf的贝叶斯模型</span></span><br><span class="line"><span class="attribute">正确率</span><span class="punctuation">:</span> <span class="string">0.97, 查准率: 0.97, 召回率: 0.97, F1: 0.97</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">基于tfidf的支持向量机模型</span></span><br><span class="line"><span class="attribute">正确率</span><span class="punctuation">:</span> <span class="string">0.98, 查准率: 0.98, 召回率: 0.98, F1: 0.98</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">真实类别</span><span class="punctuation">:</span> <span class="string">正常邮件  预测结果: 正常邮件</span></span><br><span class="line"><span class="attribute">邮件内容【 分专业吧，也分导师吧 标  题</span><span class="punctuation">:</span> <span class="string">Re: 问一个：有人觉得自己博士能混毕业吗 当然很好混毕业了 : 博士读到快中期了，始终感觉什么都不会，文章也没发几篇好的，论文的架构也没有， : 一切跟刚上的时候没有区别。但是事实上我也很辛苦的找资料，做实验，还进公司实习过， : 现在感觉好失败，内心已经放弃了，打算混毕业，不知道过来人有什么高招，请指点一二。 -- 】</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">真实类别</span><span class="punctuation">:</span> <span class="string">垃圾邮件  预测结果: 垃圾邮件</span></span><br><span class="line"><span class="attribute">邮件内容【 您好！ 我公司有多余的发票可以向外代开！（国税、地税、运输、广告、海关缴款书）。 如果贵公司（厂）有需要请来电洽谈、咨询！ 联系电话</span><span class="punctuation">:</span> <span class="string">01351025****  陈先生 谢谢 顺祝商祺! 】</span></span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure>



<h2 id="三、文本表示"><a href="#三、文本表示" class="headerlink" title="三、文本表示"></a>三、文本表示</h2><h3 id="1-One-hot"><a href="#1-One-hot" class="headerlink" title="1. One-hot"></a>1. One-hot</h3><p>One-hot（独热）编码是一种最简单的文本表示方式。如果有一个大小为V的词表，对于第i个词$w_i$，可以用一个长度为V的向量来表示，其中第i个元素为1，其它为0.例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">减肥：[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">瘦身：[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">增重：[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>One-hot词向量构建简单，但也存在明显的弱点：</p>
<ul>
<li>维度过高。如果词数量较多，每个词需要使用更长的向量表示，造成维度灾难；</li>
<li>稀疏矩阵。每个词向量，其中只有一位为1，其它位均为零；</li>
<li>语义鸿沟。词语之间的相似度、相关程度无法度量。</li>
</ul>
<h3 id="2-词袋模型"><a href="#2-词袋模型" class="headerlink" title="2. 词袋模型"></a>2. 词袋模型</h3><p>词袋模型(Bag-of-words model，BOW)，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我把他揍了一顿，揍得鼻青眼肿</span><br><span class="line">他把我走了一顿，揍得鼻青眼肿</span><br></pre></td></tr></table></figure>

<p>构建一个词典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;我&quot;</span>:<span class="number">0</span>, <span class="string">&quot;把&quot;</span>:<span class="number">1</span>, <span class="string">&quot;他&quot;</span>:<span class="number">2</span>, <span class="string">&quot;揍&quot;</span>:<span class="number">3</span>, <span class="string">&quot;了&quot;</span>:<span class="number">4</span> <span class="string">&quot;一顿&quot;</span>:<span class="number">5</span>, <span class="string">&quot;鼻青眼肿&quot;</span>:<span class="number">6</span>, <span class="string">&quot;得&quot;</span>:<span class="number">7</span>&#125;</span><br></pre></td></tr></table></figure>

<p>再将句子向量化，维数和字典大小一致，第i维上的数值代表ID为i的词在句子里出现的频次，两个句子可以表示为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>词袋模型表示简单，但也存在较为明显的缺点：</p>
<ul>
<li>丢失了顺序和语义。顺序是极其重要的语义信息，词袋模型只统计词语出现的频率，忽略了词语的顺序。例如上述两个句子意思相反，但词袋模型表示却完全一致；</li>
<li>高维度和稀疏性。当语料增加时，词袋模型维度也会增加，需要更长的向量来表示。但大多数词语不会出现在一个文本中，所以导致矩阵稀疏。</li>
</ul>
<h3 id="3-TF-IDF"><a href="#3-TF-IDF" class="headerlink" title="3. TF-IDF"></a>3. TF-IDF</h3><p>TF-IDF（Term Frequency-Inverse Document Frequency，词频-逆文档频率）是一种基于传统的统计计算方法，常用于评估一个文档集中一个词对某份文档的重要程度。其基本思想是：一个词语在文档中出现的次数越多、出现的文档越少，语义贡献度越大（对文档区分能力越强）。其表达式为：<br>$$<br>TF-IDF &#x3D; TF_{ij} \times IDF_i &#x3D;\frac{n_{ji}}{\sum_k n_{kj}} \times log(\frac{|D|}{|D_i| + 1})<br>$$<br>该指标依然无法保留词语在文本中的位置关系。该指标前面有过详细讨论，此处不再赘述。</p>
<h3 id="4-共现矩阵"><a href="#4-共现矩阵" class="headerlink" title="4. 共现矩阵"></a>4. 共现矩阵</h3><p>共现（co-occurrence）矩阵指通过统计一个事先指定大小的窗口内的词语共现次数，以词语周边的共现词的次数做为当前词语的向量。具体来说，我们通过从大量的语料文本中构建一个共现矩阵来表示词语。例如，有语料如下：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">I</span> like deep learning.</span><br><span class="line"><span class="selector-tag">I</span> like NLP.</span><br><span class="line"><span class="selector-tag">I</span> enjoy flying.</span><br></pre></td></tr></table></figure>

<p>则共现矩阵表示为：</p>
<p><img src="https://image.discover304.top/dl/nlp/co_occurrence.png" alt="co_occurrence"></p>
<p>矩阵定义的词向量在一定程度上缓解了one-hot向量相似度为0的问题，但没有解决数据稀疏性和维度灾难的问题。</p>
<h3 id="5-N-Gram表示"><a href="#5-N-Gram表示" class="headerlink" title="5. N-Gram表示"></a>5. N-Gram表示</h3><p>N-Gram模型是一种基于统计语言模型，语言模型是一个基于概率的判别模型，它的输入是个句子（由词构成的顺序序列），输出是这句话的概率，即这些单词的联合概率。</p>
<p>N-Gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有Bi-gram（N&#x3D;2）和Tri-gram（N&#x3D;3）。例如：</p>
<p>句子：L love deep learning</p>
<p>Bi-gram: {I, love}, {love, deep}, {deep, learning}</p>
<p>Tri-gram: {I, love, deep}, {love deep learning}</p>
<p>N-Gram基本思想是将文本里面的内容按照字节进行大小为n的滑动窗口操作，形成了长度是n的字节片段序列。每一个字节片段称为一个gram，对所有gram的出现频度进行统计，并按照事先设置好的频度阈值进行过滤，形成关键gram列表，也就是这个文本向量的特征空间，列表中的每一种gram就是一个特征向量维度。</p>
<h3 id="6-词嵌入"><a href="#6-词嵌入" class="headerlink" title="6. 词嵌入"></a>6. 词嵌入</h3><h4 id="1）什么是词嵌入"><a href="#1）什么是词嵌入" class="headerlink" title="1）什么是词嵌入"></a>1）什么是词嵌入</h4><p>词嵌入（word embedding）是一种词的向量化表示方式，该方法将词语映射为一个实数向量，同时保留词语之间语义的相似性和相关性。例如：</p>
<table>
<thead>
<tr>
<th></th>
<th>Man</th>
<th>Women</th>
<th>King</th>
<th>Queen</th>
<th>Apple</th>
<th>Orange</th>
</tr>
</thead>
<tbody><tr>
<td>Gender</td>
<td>-1</td>
<td>1</td>
<td>-0.95</td>
<td>0.97</td>
<td>0.00</td>
<td>0.01</td>
</tr>
<tr>
<td>Royal</td>
<td>0.01</td>
<td>0.02</td>
<td>0.93</td>
<td>0.95</td>
<td>-0.01</td>
<td>0.00</td>
</tr>
<tr>
<td>Age</td>
<td>0.03</td>
<td>0.02</td>
<td>0.70</td>
<td>0.69</td>
<td>0.03</td>
<td>-0.02</td>
</tr>
<tr>
<td>Food</td>
<td>0.09</td>
<td>0.01</td>
<td>0.02</td>
<td>0.01</td>
<td>0.95</td>
<td>0.97</td>
</tr>
</tbody></table>
<p>我们用一个四维向量来表示man，Women，King，Queen，Apple，Orange等词语（在实际中使用更高维度的表示，例如100~300维），这些向量能进行语义的表示和计算。例如，用Man的向量减去Woman的向量值：<br>$$<br>e_{man} - e_{woman} &#x3D; \left[<br>\begin{matrix}<br>-1 \<br>0.01 \<br>0.03 \<br>0.09 \<br>\end{matrix}<br>\right] -\left[<br>\begin{matrix}<br>1 \<br>0.02 \<br>0.02 \<br>0.01 \<br>\end{matrix}<br>\right] &#x3D; \left[<br>\begin{matrix}<br>-2 \<br>-0.01 \<br>0.01 \<br>0.08 \<br>\end{matrix}<br>\right] \approx \left[<br>\begin{matrix}<br>-2 \<br>0 \<br>0 \<br>0 \<br>\end{matrix}<br>\right]<br>$$<br>类似地，如果用King的向量减去Queen的向量，得到相似的结果：<br>$$<br>e_{man} - e_{woman} &#x3D; \left[<br>\begin{matrix}<br>-0.95 \<br>0.93 \<br>0.70 \<br>0.02 \<br>\end{matrix}<br>\right] -\left[<br>\begin{matrix}<br>0.97 \<br>0.85 \<br>0.69 \<br>0.01 \<br>\end{matrix}<br>\right] &#x3D; \left[<br>\begin{matrix}<br>-1.92 \<br>-0.02 \<br>0.01 \<br>0.01 \<br>\end{matrix}<br>\right] \approx \left[<br>\begin{matrix}<br>-2 \<br>0 \<br>0 \<br>0 \<br>\end{matrix}<br>\right]<br>$$<br>我们可以通过某种降维算法，将向量映射到低纬度空间中，相似的词语位置较近，不相似的词语位置较远，这样能帮助我们更直观理解词嵌入对语义的表示。如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/word_embedding.png" alt="word_embedding"></p>
<p>实际任务中，词汇量较大，表示维度较高，因此，我们不能手动为大型文本语料库开发词向量，而需要设计一种方法来使用一些机器学习算法（例如，神经网络）自动找到好的词嵌入，以便有效地执行这项繁重的任务。</p>
<h4 id="2）词嵌入的优点"><a href="#2）词嵌入的优点" class="headerlink" title="2）词嵌入的优点"></a>2）词嵌入的优点</h4><ul>
<li>特征稠密；</li>
<li>能够表征词与词之间的相似度；</li>
<li>泛化能力更好，支持语义计算。</li>
</ul>
<h2 id="四、语言模型"><a href="#四、语言模型" class="headerlink" title="四、语言模型"></a>四、语言模型</h2><h3 id="1-什么是语言模型"><a href="#1-什么是语言模型" class="headerlink" title="1. 什么是语言模型"></a>1. 什么是语言模型</h3><p>语言模型在文本处理、信息检索、机器翻译、语音识别中承担这重要的任务。从通俗角度来说，语言模型就是通过给定的一个词语序列，预测下一个最可能的词语是什么。传统语言模型有N-gram模型、HMM（隐马尔可夫模型）等，进入深度学习时代后，著名的语言模型有神经网络语言模型（Neural Network Language Model，NNLM），循环神经网络（Recurrent Neural Networks，RNN）等。</p>
<p>语言模型从概率论专业角度来描述就是：为长度为m的字符串确定其概率分布$P(w_1, w_2, …, w_n)$，其中$w_1$到$w_n$依次表示文本中的各个词语。一般采用链式法则计算其概率值：<br>$$<br>P(w_1, w_2, …, w_n) &#x3D; P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)…P(w_m|w_1,w_2,…,w_{m-1})<br>$$<br>观察上式，可发现，当文本长度过长时计算量过大，所以有人提出N元模型（N-gram）降低计算复杂度。</p>
<h3 id="2-N-gram模型"><a href="#2-N-gram模型" class="headerlink" title="2. N-gram模型"></a>2. N-gram模型</h3><p>所谓N-gram（N元）模型，就是在计算概率时，忽略长度大于N的上下文词的影响。当N&#x3D;1时，称为一元模型（Uni-gram Mode），其表达式为：<br>$$<br>P(w_1, w_2, …, w_n) &#x3D; \prod_{i&#x3D;1}^m P(w_i)<br>$$<br>当N&#x3D;2时，称为二元模型（Bi-gram Model），其表达式为：<br>$$<br>P(w_1, w_2, …, w_n) &#x3D; \prod_{i&#x3D;1}^m P(w_i|w_{i-1})<br>$$<br>当N&#x3D;3时，称为三元模型（Tri-gram Model），其表达式为：<br>$$<br>P(w_1, w_2, …, w_n) &#x3D; \prod_{i&#x3D;1}^m P(w_i|w_{i-2}, w_{i-1})<br>$$<br>可见，N值越大，保留的词序信息（上下文）越丰富，但计算量也呈指数级增长。</p>
<h3 id="3-神经网络语言模型（NNLM）"><a href="#3-神经网络语言模型（NNLM）" class="headerlink" title="3. 神经网络语言模型（NNLM）"></a>3. 神经网络语言模型（NNLM）</h3><p> NNLM是利用神经网络对N元条件进行概率估计的一种方法，其基本结构如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/NNLM.png" alt="NNLM"></p>
<ul>
<li><p>输入：前N-1个词语的向量</p>
</li>
<li><p>输出：第N个词语的一组概率</p>
</li>
<li><p>目标函数：</p>
</li>
</ul>
<p>$$<br>f(w_t, t_{t-1}, …, w_{t-n+1}) &#x3D; p(p_t|w_1^{t-1})<br>$$</p>
<p>其中，$w_t$表示第t个词，$w_1^{t-1}$表示第1个到第t个词语组成的子序列，每个词语概率均大于0，所有词语概率之和等于1。该模型计算包括两部分：特征映射、计算条件概率</p>
<ul>
<li>特征映射：将输入映射为一个特征向量，映射矩阵$C \in R^{|V| \times m}$</li>
<li>计算条件概率分布：通过另一个函数，将特征向量转化为一个概率分布</li>
</ul>
<p>神经网络计算公式为：<br>$$<br>h &#x3D; tanh(Hx + b) \<br>y &#x3D; Uh + d<br>$$<br>H为隐藏层权重矩阵，U为隐藏层到输出层的权重矩阵。输出层加入softmax函数，将y转换为对应的概率。模型参数$\theta$，包括：<br>$$<br>\theta &#x3D; (b, d, H, U, C)<br>$$<br>以下是一个计算示例：设词典大小为1000，向量维度为5，N&#x3D;3，先将前N个词表示成独热向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">呼：[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">伦：[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">贝：[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>输入矩阵为：[3, 5]</p>
<p>权重矩阵：[1000, 5]</p>
<p>隐藏层：[3, 5] * [1000, 5] &#x3D; [3, 5]</p>
<p>输出层权重：[5, 1000]</p>
<p>输出矩阵：[3, 5] * [5, 1000] &#x3D; [3, 1000] &#x3D;&#x3D;&gt; [1, 1000]，表示预测属于1000个词的概率.</p>
<h3 id="4-Word2vec"><a href="#4-Word2vec" class="headerlink" title="4. Word2vec"></a>4. Word2vec</h3><p>Word2vec是Goolge发布的、应用最广泛的词嵌入表示学习技术，其主要作用是高效获取词语的词向量，目前被用作许多NLP任务的特征工程。Word2vec 可以根据给定的语料库，通过优化后的训练模型快速有效地将一个词语表达成向量形式，为自然语言处理领域的应用研究提供了新的工具，包含Skip-gram（跳字模型）和CBOW（连续词袋模型）来建立词语的词嵌入表示。Skip-gram的主要作用是根据当前词，预测背景词（前后的词）；CBOW的主要作用是根据背景词（前后的词）预测当前词。</p>
<h4 id="1）Skip-gram"><a href="#1）Skip-gram" class="headerlink" title="1）Skip-gram"></a>1）Skip-gram</h4><p>Skip-gram的主要作用是根据当前词，预测背景词（前后的词），其结构图如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/skip_gram_network.png" alt="skip_gram_network"></p>
<p>例如有如下语句：呼伦贝尔大草原</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">呼<span class="literal">_</span> <span class="literal">_</span> 尔<span class="literal">_</span> <span class="literal">_</span>原</span><br><span class="line"><span class="literal">_</span>伦<span class="literal">_</span> <span class="literal">_</span>大<span class="literal">_</span> <span class="literal">_</span></span><br><span class="line"><span class="literal">_</span> <span class="literal">_</span>贝<span class="literal">_</span> <span class="literal">_</span> 草<span class="literal">_</span></span><br><span class="line">呼<span class="literal">_</span> <span class="literal">_</span>尔<span class="literal">_</span> <span class="literal">_</span>原</span><br><span class="line"><span class="literal">_</span>伦<span class="literal">_</span> <span class="literal">_</span>大<span class="literal">_</span> <span class="literal">_</span></span><br></pre></td></tr></table></figure>

<p>预测出前后词的数量，称为window_size（以上示例中windows_size为2），实际是要将以下概率最大化：</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P<span class="comment">(伦|呼)</span>P<span class="comment">(贝|呼)</span></span><br><span class="line">P<span class="comment">(伦|尔)</span>P<span class="comment">(贝|尔)</span> P<span class="comment">(大|尔)</span>P<span class="comment">(草|尔)</span></span><br><span class="line">P<span class="comment">(大|原)</span>P<span class="comment">(草|原)</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>可以写出概率的一般化表达式，设有文本Text，由N个单词组成：<br>$$<br>Text &#x3D; {w_1, w_2, w_3, …, w_n}<br>$$<br>目标函数可以写作：<br>$$<br>argmax \prod_{c \in Text} \prod_{c \in c(w)} P(c|w; \theta)<br>$$<br>因为概率均为0~1之间的数字，连乘计算较为困难，所以转换为对数相加形式：<br>$$<br>argmax \sum_{c \in Text} \sum_{c \in c(w)} logP(c|w;\theta)<br>$$<br>再表示为softmax形式：<br>$$<br>argmax \sum_{c \in Text} \sum_{c \in c(w)} log \frac{e^{u_c \cdot v_w}}{\sum_{c’ \in vocab }e_{c’} \cdot v_w}<br>$$<br>其中，U为上下文单词矩阵，V为同样大小的中心词矩阵，因为每个词可以作为上下文词，同时也可以作为中心词，再将如上公式进一步转化：<br>$$<br>argmax \sum_{c \in Text} \sum_{c \in c(w)} u_c \cdot v_w - log \sum_{c’ \in vocab }e_{c’} \cdot v_w<br>$$<br>上式中，由于需要在整个词汇表中进行遍历，如果词汇表很大，计算效率会很低。所以，真正进行优化时，采用另一种优化形式。例如有如下语料库：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文本：呼伦贝尔大草原</span><br></pre></td></tr></table></figure>

<p>将window_size设置为1，构建正案例词典、负案例词典（一般来说，负样本词典比正样本词典大的多）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">正样本：D = &#123;(呼，伦)，(伦，呼)，(伦，贝)，(贝，伦),(贝，尔),(尔，贝)，(尔，大)，(大，尔)，(大，草)(草，大)，(草，原)，(原，草)&#125;</span><br><span class="line"></span><br><span class="line">负样本：D’= &#123;(呼，贝),(呼，尔),(呼，大)，(呼，草)，(呼，原)，(伦，尔),(伦，大),(伦，草),(伦，原),(贝，呼),(贝，大),(贝，草),(贝，原),(尔，呼),(尔，伦)(尔，草),(尔，原),(大，呼),(大，伦),(大，原)，(草，呼)，(草，伦)，(草，贝)，(原，呼)，(原，伦)，(原，贝)，(原，尔)，(原，大)&#125;</span><br></pre></td></tr></table></figure>

<p>词向量优化的目标函数定义为正样本、负样本公共概率最大化函数：<br>$$<br>argmax (\prod_{w,c \in D} log P(D&#x3D;1|w,c; \theta) \prod_{w, c \in D’} P(D&#x3D;0|w, c; \theta))  \<br>&#x3D; argmax (\prod_{w,c \in D} \frac{1}{1+exp(-U_c \cdot V_w)} \prod_{w, c \in D’} [1- \frac{1}{1+exp(-U_c \cdot V_w)}])  \<br>&#x3D; argmax(\sum_{w,c \in D} log \sigma (U_c \cdot V_w) + \sum_{w,c \in D’} log \sigma (-U_c \cdot V_w))<br>$$<br>在实际训练时，会从负样本集合中选取部分样本（称之为“负采样”）来进行计算，从而降低运算量.要训练词向量，还需要借助于语言模型.</p>
<h4 id="2）CBOW模型"><a href="#2）CBOW模型" class="headerlink" title="2）CBOW模型"></a>2）CBOW模型</h4><p>CBOW模型全程为Continous Bag of Words（连续词袋模型），其核心思想是用上下文来预测中心词，例如：</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">呼伦贝<span class="literal">_</span>大草原</span><br></pre></td></tr></table></figure>

<p>其模型结构示意图如下：</p>
<p><img src="https://image.discover304.top/dl/nlp/CBOW_network.png" alt="CBOW_network"></p>
<ul>
<li>输入：$C \times V$的矩阵，C表示上下文词语的个数，V表示词表大小</li>
<li>隐藏层：$V \times N$的权重矩阵，一般称为word-embedding，N表示每个词的向量长度，和输入矩阵相乘得到$C \times N$的矩阵。综合考虑上下文中所有词信息预测中心词，所以将$C \times N$矩阵叠加，得到$1 \times N$的向量</li>
<li>输出层：包含一个$N \times V$的权重矩阵，隐藏层向量和该矩阵相乘，输出$1 \times V$的向量，经过softmax转换为概率，对应每个词表中词语的概率</li>
</ul>
<h4 id="3）示例：训练词向量"><a href="#3）示例：训练词向量" class="headerlink" title="3）示例：训练词向量"></a>3）示例：训练词向量</h4><p>数据集：来自中文wiki文章</p>
<p>代码：建议在AIStudio下执行</p>
<ul>
<li>安装gensim</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install gensim==3.8.1 # 如果不在AIStudio下执行去掉前面的叹号</span><br></pre></td></tr></table></figure>

<ul>
<li>用于解析XML，读取XML文件中的数据，并写入到新的文本文件中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> WikiCorpus</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 获取输入数据路径</span></span><br><span class="line">inp = <span class="string">&#x27;data/data104767/articles.xml.bz2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建新的文本文件（输出文件）</span></span><br><span class="line">outp = <span class="built_in">open</span>(<span class="string">&#x27;wiki.zh.text&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 调用gensim读取xml压缩文件</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">wiki = WikiCorpus(inp, lemmatize=<span class="literal">False</span>, dictionary=&#123;&#125;)</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> wiki.get_texts():</span><br><span class="line">     outp.write( <span class="string">&quot; &quot;</span>.join(text) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">     count += <span class="number">1</span></span><br><span class="line">     <span class="keyword">if</span> count % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;count:&quot;</span>, count)</span><br><span class="line">     <span class="keyword">if</span> count % <span class="number">20000</span> == <span class="number">0</span>:</span><br><span class="line">         <span class="keyword">break</span></span><br><span class="line">outp.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finished Saved &quot;</span> +<span class="built_in">str</span>(count) + <span class="string">&quot; articles&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>生成分词文件</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line"><span class="keyword">import</span> codecs <span class="comment"># python封装的文件的工具包</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_wiki_text</span>(<span class="params">origin_file,target_file</span>):</span><br><span class="line">    <span class="keyword">with</span> codecs.<span class="built_in">open</span>(origin_file, <span class="string">&#x27;r&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> inp, codecs.<span class="built_in">open</span>(target_file,<span class="string">&#x27;w&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> outp:</span><br><span class="line">        line = inp.readline()</span><br><span class="line">        num = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            line_seg = <span class="string">&quot; &quot;</span>.join(jieba.cut(line))</span><br><span class="line">            outp.writelines(line_seg)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            line = inp.readline()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;----&#x27;</span>, num, <span class="string">&#x27; articles----&#x27;</span>)</span><br><span class="line">    inp.close()</span><br><span class="line">    outp.close()</span><br><span class="line"></span><br><span class="line">process_wiki_text(<span class="string">&#x27;wiki.zh.text&#x27;</span>,<span class="string">&#x27;wiki.zh.text.seg&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入工具库</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="comment"># cpu开启多线程执行</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="comment"># 按照行的方式读取文件内容（分词文件）</span></span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence </span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line"><span class="comment"># format: 指定输出的格式和内容，format可以输出很多有用信息，</span></span><br><span class="line"><span class="comment"># %(asctime)s: 打印日志的时间</span></span><br><span class="line"><span class="comment"># %(levelname)s: 打印日志级别名称</span></span><br><span class="line"><span class="comment"># %(message)s: 打印日志信息</span></span><br><span class="line">logging.basicConfig(<span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s: %(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line">logging.root.setLevel(level=logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.输入文件</span></span><br><span class="line">inp = <span class="string">&#x27;wiki.zh.text.seg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.输出文件</span></span><br><span class="line">outp1 = <span class="string">&#x27;wiki.zh.text.model&#x27;</span>   <span class="comment"># 模型</span></span><br><span class="line">outp2 = <span class="string">&#x27;wiki.zh.text.vector&#x27;</span>  <span class="comment"># 权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型的训练和保存</span></span><br><span class="line">model = Word2Vec(</span><br><span class="line">    LineSentence(inp),</span><br><span class="line">    size=<span class="number">100</span>, <span class="comment"># 词向量的维度（25~1000）</span></span><br><span class="line">    window=<span class="number">3</span>,</span><br><span class="line">    min_count=<span class="number">5</span>, <span class="comment"># 如果语料库中单词出现的次数小于5，就忽略该单词</span></span><br><span class="line">    workers=multiprocessing.cpu_count()</span><br><span class="line">)</span><br><span class="line">model.save(outp1)</span><br><span class="line">model.wv.save_word2vec_format(outp2,binary=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>测试</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span>  Word2Vec</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载模型</span></span><br><span class="line">model = Word2Vec.load(<span class="string">&#x27;whik.zh.text.model&#x27;</span>)</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 变量单词和对应的向量</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> model.wv.index2word:</span><br><span class="line">    <span class="built_in">print</span>(word, model[word])</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> count==<span class="number">10</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;==============================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">result = model.most_similar(<span class="string">u&quot;铁路&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result:</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;==============================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">result2 = model.most_similar(<span class="string">u&quot;中药&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result2:</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br></pre></td></tr></table></figure>

<p>输出（训练过程略）：</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;高速铁路&#x27;, <span class="number">0.8310302495956421</span>)</span><br><span class="line">(&#x27;客运专线&#x27;, <span class="number">0.8245105743408203</span>)</span><br><span class="line">(&#x27;高铁&#x27;, <span class="number">0.8095601201057434</span>)</span><br><span class="line">(&#x27;城际&#x27;, <span class="number">0.802475094795227</span>)</span><br><span class="line">(&#x27;联络线&#x27;, <span class="number">0.7837506532669067</span>)</span><br><span class="line">(&#x27;成昆铁路&#x27;, <span class="number">0.7820425033569336</span>)</span><br><span class="line">(&#x27;支线&#x27;, <span class="number">0.7775323390960693</span>)</span><br><span class="line">(&#x27;通车&#x27;, <span class="number">0.7751388549804688</span>)</span><br><span class="line">(&#x27;沪&#x27;, <span class="number">0.7748854756355286</span>)</span><br><span class="line">(&#x27;京广&#x27;, <span class="number">0.7708789110183716</span>)</span><br><span class="line">==============================================</span><br><span class="line">(&#x27;草药&#x27;, <span class="number">0.9046826362609863</span>)</span><br><span class="line">(&#x27;中药材&#x27;, <span class="number">0.8511005640029907</span>)</span><br><span class="line">(&#x27;气功&#x27;, <span class="number">0.8384993672370911</span>)</span><br><span class="line">(&#x27;中医学&#x27;, <span class="number">0.8368280529975891</span>)</span><br><span class="line">(&#x27;调味&#x27;, <span class="number">0.8364394307136536</span>)</span><br><span class="line">(&#x27;冶炼&#x27;, <span class="number">0.8328938484191895</span>)</span><br><span class="line">(&#x27;药材&#x27;, <span class="number">0.8304706811904907</span>)</span><br><span class="line">(&#x27;有机合成&#x27;, <span class="number">0.8298543691635132</span>)</span><br><span class="line">(&#x27;针灸&#x27;, <span class="number">0.8297436833381653</span>)</span><br><span class="line">(&#x27;药用&#x27;, <span class="number">0.8281913995742798</span>)</span><br></pre></td></tr></table></figure>



<h3 id="5-循环神经网络（RNN）"><a href="#5-循环神经网络（RNN）" class="headerlink" title="5. 循环神经网络（RNN）"></a>5. 循环神经网络（RNN）</h3><p>前面提到的关于NLP的模型及应用，都未考虑词的顺序问题，而在自然语言中，词语顺序又是极其重要的特征。循环神经网络（Recurrent Neural Network，RNN）能够在原有神经网络的基础上增加记忆单元，处理任意长度的序列（理论上），并且在前后词语（或字）之间建立起依赖关系。相比于CNN，RNN更适合处理视频、语音、文本等与时序相关的问题。</p>
<h4 id="1）原生RNN"><a href="#1）原生RNN" class="headerlink" title="1）原生RNN"></a>1）原生RNN</h4><h5 id="①-RNN起源及发展"><a href="#①-RNN起源及发展" class="headerlink" title="① RNN起源及发展"></a>① RNN起源及发展</h5><p>1982年，物理学家约翰·霍普菲尔德（John Hopfield）利用电阻、电容和运算放大器等元件组成的模拟电路实现了对网络神经元的描述，该网络从输出到输入有反馈连接。1986年，迈克尔·乔丹（Michael Jordan，不是打篮球那哥们，而是著名人工智能学者、美国科学院院士、吴恩达的导师）借鉴了Hopfield网络的思想，正式将循环连接拓扑结构引入神经网络。1990年，杰弗里·埃尔曼（Jeffrey Elman）又在Jordan的研究基础上做了部分简化，正式提出了RNN模型（那时还叫Simple Recurrent Network，SRN）。</p>
<h5 id="②-RNN的结构"><a href="#②-RNN的结构" class="headerlink" title="② RNN的结构"></a>② RNN的结构</h5><p>RNN结构如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/RNN_1.png" alt="RNN_1"></p>
<p>上图中，左侧为不展开的画法，右侧为展开画法。内部结构如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/RNN_2.png" alt="RNN_2"></p>
<p>计算公式可表示为：<br>$$<br>s_t &#x3D; f(U \cdot x_t + W \cdot s_{t-1} + b) \<br>y_t &#x3D; g(V \cdot s_t + d)<br>$$<br>其中，$x_t$表示$t$时刻的输入；$s_t$表示$t$时刻隐藏状态；$f$和$g$表示激活函数；$U，V，W$分别表示输入层 → 隐藏层权重、隐藏层 → 输出层权重、隐藏层 → 隐藏层权重。对于任意时刻$t$，所有权重和偏置都共享，这极大减少了模型参数量。</p>
<p>计算时，首先利用前向传播算法，依次按照时间顺序进行计算，再利用反向传播算法进行误差传递，和普通BP（Back Propagation）网络唯一区别是，加入了时间顺序，计算方式有些微差别，称为BPTT（Back Propagation Through Time）算法。</p>
<h5 id="③-RNN的功能"><a href="#③-RNN的功能" class="headerlink" title="③ RNN的功能"></a>③ RNN的功能</h5><p>RNN善于处理跟序列相关的信息，如：语音识别，语言建模，翻译，图像字幕。它能根据近期的一些信息来执行&#x2F;判别&#x2F;预测当前任务。例如：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">白色的云朵漂浮在蓝色的<span class="emphasis">____</span></span><br><span class="line">天空中飞过来一只<span class="emphasis">___</span></span><br></pre></td></tr></table></figure>

<p>根据前面输入的一连串词语，可以预测第一个句子最后一个词为”天空”、第二个句子最后一个词为”鸟”的概率最高。</p>
<h5 id="④-RNN的缺陷"><a href="#④-RNN的缺陷" class="headerlink" title="④ RNN的缺陷"></a>④ RNN的缺陷</h5><p>因为计算的缘故，RNN容易出现梯度消失，导致它无法学习过往久远的信息，无法处理长序列、远期依赖任务。例如：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我生长在中国，祖上十代都是农民，家里三亩一分地。我是家里老三，我大哥叫大狗子，二哥叫二狗子，我叫三狗子，我弟弟叫狗窝子。我的母语是<span class="emphasis">_____</span></span><br></pre></td></tr></table></figure>

<p>要预测出句子最后的词语，需要根据句子开够的信息”我出生在中国”，才能确定母语是”中文”或”汉语”的概率最高。原生RNN在处理这类远期依赖任务时出现了困难，于是LSTM被提出。</p>
<h4 id="2）长短期记忆模型（LSTM）"><a href="#2）长短期记忆模型（LSTM）" class="headerlink" title="2）长短期记忆模型（LSTM）"></a>2）长短期记忆模型（LSTM）</h4><p>长短期记忆模型（Long Short Term Memory，LSTM）是RNN的变种，于1997年Schmidhuber和他的合作者Hochreiter提出，由于独特的设计结构，LSTM可以很好地解决梯度消失问题，能够处理更长的序列，更好解决远期依赖任务。LSTM非常适合构造大型深度神经网络。2009年，用改进版的LSTM，赢得了国际文档分析与识别大赛（ICDAR）手写识别大赛冠军；2014年，Yoshua Bengio的团队提出了一种更好的LSTM变体GRU（Gated Recurrent Unit，门控环单元）；2016年，Google利用LSTM来做语音识别和文字翻译；同年，苹果公司使用LSTM来优化Siri应用。</p>
<p>LSTM同样具有链式结构，它具有4个以特殊方式互相影响的神经网络层。其结构入下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/LSTM.png" alt="LSTM"></p>
<p>LSTM的核心是细胞状态，用贯穿细胞的水平线表示。细胞状态像传送带一样。它贯穿整个细胞却只有很少的分支，这样能保证信息不变的流过整个结构。同时，LSTM通过称为门（gate）的结构来对单元状态进行增加或删除，包含三扇门：</p>
<ul>
<li><p>遗忘门：决定哪些信息丢弃</p>
<p><img src="https://image.discover304.top/dl/nlp/LSTM_forget.png" alt="LSTM_forget"></p>
<p>表达式为：$f_t &#x3D; \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)$，当输出为1时表示完全保留，输出为0是表示完全丢弃</p>
</li>
<li><p>输入门：决定哪些信息输入进来</p>
<p><img src="https://image.discover304.top/dl/nlp/LSTM_input.png" alt="LSTM_input"></p>
<p>表达式为：<br>$$<br>i_t &#x3D; \sigma (W_i \cdot [h_{t-1}, x_t] + b_i) \<br>\tilde{C}<em>t &#x3D; tanh(W_c \cdot [h</em>{t-1}, x_t] + b_c)<br>$$</p>
<p>根据输入、遗忘门作用结果，可以对细胞状态进行更新，如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/LSTM_update.png" alt="LSTM_update"></p>
<p>状态更新表达式为：<br>$$<br>C_t &#x3D; f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t<br>$$<br>遗忘门找到了需要忘掉的信息$f_t$后，再将它与旧状态相乘，丢弃掉确定需要丢弃的信息。再将结果加上$i_t \cdot C_t$使细胞状态获得新的信息，这样就完成了细胞状态的更新。</p>
</li>
<li><p>输出门：决定输出哪些信息</p>
<p><img src="https://image.discover304.top/dl/nlp/LSTM_out.png" alt="LSTM_out"></p>
</li>
</ul>
<p>输出门表达式为：<br>$$<br>O_t &#x3D; \sigma (W_o \cdot [h_{t-1}, x_t] + b_o) \<br>h_t &#x3D; O_t \cdot tanh(C_t)<br>$$<br>在输出门中，通过一个Sigmoid层来确定哪部分的信息将输出，接着把细胞状态通过Tanh进行处理（得到一个在-1～1之间的值）并将它和Sigmoid门的输出相乘，得出最终想要输出的那部分。</p>
<h4 id="3）双向循环神经网络"><a href="#3）双向循环神经网络" class="headerlink" title="3）双向循环神经网络"></a>3）双向循环神经网络</h4><p>双向循环神经网络（BRNN）由两个循环神经网络组成，一个正向、一个反向，两个序列连接同一个输出层。正向RNN提取正向序列特征，反向RNN提取反向序列特征。例如有如下两个语句：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我喜欢苹果，比安卓用起来更流畅些</span><br><span class="line">我喜欢苹果，基本上每天都要吃一个</span><br></pre></td></tr></table></figure>

<p>根据后面的描述，我们可以得知，第一句中的”苹果”指的是苹果手机，第二句中的”苹果”指的是水果。双向循环神经网络结构如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/BiRNN.png" alt="BiRNN"></p>
<p>权重设置如下图所示：</p>
<p><img src="https://image.discover304.top/dl/nlp/BiRNN_2.png" alt="BiRNN_2"></p>
<p>计算表达式为：<br>$$<br>h_t &#x3D; f(w_1x_t + w_2h_{t-1}) \<br>h_t’ &#x3D; f(w_3x_t + w_5h’_{t+1}) \<br>o_t &#x3D; g(w_4h_t + w_6h’_t)<br>$$<br>其中，$h_t$为$t$时刻正向序列计算结果，$h’_t$为$t$时刻反向序列的计算结果，将正向序列、反向序列结果和各自权重矩阵相乘，相加后结果激活函数产生$t$时刻的输出。</p>
<p>通常情况下，双向循环神经网络能获得比单向网络更好的性能。</p>
<h2 id="五、NLP应用"><a href="#五、NLP应用" class="headerlink" title="五、NLP应用"></a>五、NLP应用</h2><h3 id="1-文本分类"><a href="#1-文本分类" class="headerlink" title="1. 文本分类"></a>1. 文本分类</h3><h4 id="1）什么是文本分类"><a href="#1）什么是文本分类" class="headerlink" title="1）什么是文本分类"></a>1）什么是文本分类</h4><p>文本分类就是根据文本内容将文本划分到不同类别，例如新闻系统中，每篇新闻报道会划归到不同的类别。</p>
<h4 id="2）文本分类的应用"><a href="#2）文本分类的应用" class="headerlink" title="2）文本分类的应用"></a>2）文本分类的应用</h4><ul>
<li>内容分类（新闻分类）</li>
<li>邮件过滤（例如垃圾邮件过滤）</li>
<li>用户分类（如商城消费级别、喜好）</li>
<li>评论、文章、对话的情感分类（正面、负面、中性）</li>
</ul>
<h4 id="3）文本分类案例"><a href="#3）文本分类案例" class="headerlink" title="3）文本分类案例"></a>3）文本分类案例</h4><ul>
<li>任务：建立文本分类模型，并对模型进行训练、评估，从而实现对中文新闻摘要类别正确划分</li>
<li>数据集：从网站上爬取56821条数据中文新闻摘要，包含10种类别，国际、文化、娱乐、体育、财经、汽车、教育、科技、房产、证券，各类别样本数量如下表所示：</li>
</ul>
<p><img src="https://image.discover304.top/dl/nlp/News_samples_classes.png" alt="News_samples_classes"></p>
<ul>
<li>模型选择：</li>
</ul>
<p><img src="https://image.discover304.top/dl/nlp/News_classify_network.png" alt="TextCNN"></p>
<ul>
<li>步骤：</li>
</ul>
<p><img src="https://image.discover304.top/dl/nlp/News_classify_flow.png" alt="News_classify_flow"></p>
<ul>
<li><p>代码</p>
<p>【预处理部分】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中文资讯分类示例</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> cpu_count</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.fluid <span class="keyword">as</span> fluid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义公共变量</span></span><br><span class="line">data_root = <span class="string">&quot;data/news_classify/&quot;</span> <span class="comment"># 数据集所在目录</span></span><br><span class="line">data_file = <span class="string">&quot;news_classify_data.txt&quot;</span> <span class="comment"># 原始样本文件名</span></span><br><span class="line">test_file = <span class="string">&quot;test_list.txt&quot;</span> <span class="comment"># 测试集文件名称</span></span><br><span class="line">train_file = <span class="string">&quot;train_list.txt&quot;</span> <span class="comment"># 训练集文件名称</span></span><br><span class="line">dict_file = <span class="string">&quot;dict_txt.txt&quot;</span> <span class="comment"># 编码后的字典文件</span></span><br><span class="line"></span><br><span class="line">data_file_path = data_root + data_file <span class="comment"># 样本文件完整路径</span></span><br><span class="line">dict_file_path = data_root + dict_file <span class="comment"># 字典文件完整路径</span></span><br><span class="line">test_file_path = data_root + test_file <span class="comment"># 测试集文件完整路径</span></span><br><span class="line">train_file_path = data_root + train_file <span class="comment"># 训练集文件完整路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成字典文件：把每个字编码成一个数字，并存入文件中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dict</span>():</span><br><span class="line">    dict_set = <span class="built_in">set</span>()  <span class="comment"># 集合，去重</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f: <span class="comment"># 打开原始样本文件</span></span><br><span class="line">        lines = f.readlines() <span class="comment"># 读取所有的行</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        title = line.split(<span class="string">&quot;_!_&quot;</span>)[-<span class="number">1</span>].replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>) <span class="comment">#取出标题部分，去除换行符</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> title: <span class="comment"># 取出标题部分每个字</span></span><br><span class="line">            dict_set.add(w) <span class="comment"># 将每个字存入集合进行去重</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历集合，每个字分配一个编码</span></span><br><span class="line">    dict_list = []</span><br><span class="line">    i = <span class="number">0</span> <span class="comment"># 计数器</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> dict_set:</span><br><span class="line">        dict_list.append([s, i]) <span class="comment"># 将&quot;文字,编码&quot;键值对添加到列表中</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    dict_txt = <span class="built_in">dict</span>(dict_list) <span class="comment"># 将列表转换为字典</span></span><br><span class="line">    end_dict = &#123;<span class="string">&quot;&lt;unk&gt;&quot;</span>: i&#125; <span class="comment"># 未知字符</span></span><br><span class="line">    dict_txt.update(end_dict) <span class="comment"># 将未知字符编码添加到字典中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将字典保存到文件中</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_file_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(<span class="built_in">str</span>(dict_txt))  <span class="comment"># 将字典转换为字符串并存入文件</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;生成字典完成.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对一行标题进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">line_encoding</span>(<span class="params">title, dict_txt, label</span>):</span><br><span class="line">    new_line = <span class="string">&quot;&quot;</span>  <span class="comment"># 返回的结果</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> title:</span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> dict_txt: <span class="comment"># 如果字已经在字典中</span></span><br><span class="line">            code = <span class="built_in">str</span>(dict_txt[w])  <span class="comment"># 取出对应的编码</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code = <span class="built_in">str</span>(dict_txt[<span class="string">&quot;&lt;unk&gt;&quot;</span>]) <span class="comment"># 取未知字符的编码</span></span><br><span class="line">        new_line = new_line + code + <span class="string">&quot;,&quot;</span> <span class="comment"># 将编码追加到新的字符串后</span></span><br><span class="line"></span><br><span class="line">    new_line = new_line[:-<span class="number">1</span>] <span class="comment"># 去掉最后一个逗号</span></span><br><span class="line">    new_line = new_line + <span class="string">&quot;\t&quot;</span> + label + <span class="string">&quot;\n&quot;</span> <span class="comment"># 拼接成一行，标题和标签用\t分隔</span></span><br><span class="line">    <span class="keyword">return</span> new_line</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对原始样本进行编码，对每个标题的每个字使用字典中编码的整数进行替换</span></span><br><span class="line"><span class="comment"># 产生编码后的句子，并且存入测试集、训练集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_data_list</span>():</span><br><span class="line">    <span class="comment"># 清空测试集、训练集文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(test_file_path, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(train_file_path, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开原始样本文件，取出标题部分，对标题进行编码</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f_dict:</span><br><span class="line">        <span class="comment"># 读取字典文件中的第一行(只有一行)，通过调用eval函数转换为字典对象</span></span><br><span class="line">        dict_txt = <span class="built_in">eval</span>(f_dict.readlines()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f_data:</span><br><span class="line">        lines = f_data.readlines()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取出标题并编码</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        words = line.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>).split(<span class="string">&quot;_!_&quot;</span>) <span class="comment"># 拆分每行</span></span><br><span class="line">        label = words[<span class="number">1</span>] <span class="comment"># 分类</span></span><br><span class="line">        title = words[<span class="number">3</span>] <span class="comment"># 标题</span></span><br><span class="line"></span><br><span class="line">        new_line = line_encoding(title, dict_txt, label)  <span class="comment"># 对标题进行编码</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>: <span class="comment"># 每10笔写一笔测试集文件</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(test_file_path, <span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(new_line)</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 写入训练集</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(train_file_path, <span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(new_line)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;生成测试集、训练集结束.&quot;</span>)</span><br><span class="line"></span><br><span class="line">create_dict()  <span class="comment"># 生成字典</span></span><br><span class="line">create_data_list() <span class="comment"># 生成训练集、测试集</span></span><br></pre></td></tr></table></figure>



<p>【模型定义与训练】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取字典文件，并返回字典长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dict_len</span>(<span class="params">dict_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = <span class="built_in">eval</span>(f.readlines()[<span class="number">0</span>])  <span class="comment"># 读取字典文件内容，并返回一个字典对象</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(line.keys())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义data_mapper，将reader读取的数据进行二次处理</span></span><br><span class="line"><span class="comment"># 将传入的字符串转换为整型并返回</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_mapper</span>(<span class="params">sample</span>):</span><br><span class="line">    data, label = sample  <span class="comment"># 将sample元组拆分到两个变量</span></span><br><span class="line">    <span class="comment"># 拆分句子，将每个编码转换为数字, 并存入一个列表中</span></span><br><span class="line">    val = [<span class="built_in">int</span>(w) <span class="keyword">for</span> w <span class="keyword">in</span> data.split(<span class="string">&quot;,&quot;</span>)]</span><br><span class="line">    <span class="keyword">return</span> val, <span class="built_in">int</span>(label)  <span class="comment"># 返回整数列表，标签(转换成整数)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义reader</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_reader</span>(<span class="params">train_file_path</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(train_file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()  <span class="comment"># 读取所有的行</span></span><br><span class="line">            np.random.shuffle(lines)  <span class="comment"># 打乱所有样本</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                data, label = line.split(<span class="string">&quot;\t&quot;</span>)  <span class="comment"># 拆分样本到两个变量中</span></span><br><span class="line">                <span class="keyword">yield</span> data, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> paddle.reader.xmap_readers(data_mapper,  <span class="comment"># reader读取的数据进行下一步处理函数</span></span><br><span class="line">                                      reader,  <span class="comment"># 读取样本的reader</span></span><br><span class="line">                                      cpu_count(),  <span class="comment"># 线程数</span></span><br><span class="line">                                      <span class="number">1024</span>)  <span class="comment"># 缓冲区大小</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取测试集reader</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_reader</span>(<span class="params">test_file_path</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(test_file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                data, label = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">                <span class="keyword">yield</span> data, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> paddle.reader.xmap_readers(data_mapper,</span><br><span class="line">                                      reader,</span><br><span class="line">                                      cpu_count(),</span><br><span class="line">                                      <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CNN_net</span>(<span class="params">data, dict_dim, class_dim=<span class="number">10</span>, emb_dim=<span class="number">128</span>, hid_dim=<span class="number">128</span>, hid_dim2=<span class="number">98</span></span>):</span><br><span class="line">    <span class="comment"># embedding(词嵌入层)：生成词向量，得到一个新的粘稠的实向量</span></span><br><span class="line">    <span class="comment"># 以使用较少的维度，表达更丰富的信息</span></span><br><span class="line">    emb = fluid.layers.embedding(<span class="built_in">input</span>=data, size=[dict_dim, emb_dim])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 并列两个卷积、池化层</span></span><br><span class="line">    conv1 = fluid.nets.sequence_conv_pool(<span class="built_in">input</span>=emb,  <span class="comment"># 输入，上一个词嵌入层的输出作为输入</span></span><br><span class="line">                                          num_filters=hid_dim,  <span class="comment"># 卷积核数量</span></span><br><span class="line">                                          filter_size=<span class="number">3</span>,  <span class="comment"># 卷积核大小</span></span><br><span class="line">                                          act=<span class="string">&quot;tanh&quot;</span>,  <span class="comment"># 激活函数</span></span><br><span class="line">                                          pool_type=<span class="string">&quot;sqrt&quot;</span>)  <span class="comment"># 池化类型</span></span><br><span class="line"></span><br><span class="line">    conv2 = fluid.nets.sequence_conv_pool(<span class="built_in">input</span>=emb,  <span class="comment"># 输入，上一个词嵌入层的输出作为输入</span></span><br><span class="line">                                          num_filters=hid_dim2,  <span class="comment"># 卷积核数量</span></span><br><span class="line">                                          filter_size=<span class="number">4</span>,  <span class="comment"># 卷积核大小</span></span><br><span class="line">                                          act=<span class="string">&quot;tanh&quot;</span>,  <span class="comment"># 激活函数</span></span><br><span class="line">                                          pool_type=<span class="string">&quot;sqrt&quot;</span>)  <span class="comment"># 池化类型</span></span><br><span class="line">    output = fluid.layers.fc(<span class="built_in">input</span>=[conv1, conv2],  <span class="comment"># 输入</span></span><br><span class="line">                             size=class_dim,  <span class="comment"># 输出类别数量</span></span><br><span class="line">                             act=<span class="string">&quot;softmax&quot;</span>)  <span class="comment"># 激活函数</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型、训练、评估、保存</span></span><br><span class="line">model_save_dir = <span class="string">&quot;model/news_classify/&quot;</span>  <span class="comment"># 模型保存路径</span></span><br><span class="line"></span><br><span class="line">words = fluid.layers.data(name=<span class="string">&quot;words&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&quot;int64&quot;</span>,</span><br><span class="line">                          lod_level=<span class="number">1</span>) <span class="comment"># 张量层级</span></span><br><span class="line">label = fluid.layers.data(name=<span class="string">&quot;label&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&quot;int64&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取字典长度</span></span><br><span class="line">dict_dim = get_dict_len(dict_file_path)</span><br><span class="line"><span class="comment"># 调用函数创建CNN</span></span><br><span class="line">model = CNN_net(words, dict_dim)</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">cost = fluid.layers.cross_entropy(<span class="built_in">input</span>=model, <span class="comment"># 预测结果</span></span><br><span class="line">                                  label=label) <span class="comment"># 真实结果</span></span><br><span class="line">avg_cost = fluid.layers.mean(cost) <span class="comment"># 求损失函数均值</span></span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line">acc = fluid.layers.accuracy(<span class="built_in">input</span>=model, <span class="comment"># 预测结果</span></span><br><span class="line">                            label=label) <span class="comment"># 真实结果</span></span><br><span class="line"><span class="comment"># 克隆program用于模型测试评估</span></span><br><span class="line"><span class="comment"># for_test如果为True，会少一些优化</span></span><br><span class="line">test_program = fluid.default_main_program().clone(for_test=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">optimizer.minimize(avg_cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义执行器</span></span><br><span class="line">place = fluid.CPUPlace()</span><br><span class="line">exe = fluid.Executor(place)</span><br><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">tr_reader = train_reader(train_file_path)</span><br><span class="line">batch_train_reader = paddle.batch(reader=tr_reader, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">ts_reader = test_reader(test_file_path)</span><br><span class="line">batch_test_reader = paddle.batch(reader=ts_reader, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">feeder = fluid.DataFeeder(place=place, feed_list=[words, label]) <span class="comment"># feeder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> pass_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_train_reader()):</span><br><span class="line">        train_cost, train_acc = exe.run(program=fluid.default_main_program(),</span><br><span class="line">                                        feed=feeder.feed(data), <span class="comment"># 喂入数据</span></span><br><span class="line">                                        fetch_list=[avg_cost, acc]) <span class="comment"># 要获取的结果</span></span><br><span class="line">        <span class="comment"># 打印</span></span><br><span class="line">        <span class="keyword">if</span> batch_id % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;pass_id:%d, batch_id:%d, cost:%f, acc:%f&quot;</span> %</span><br><span class="line">                  (pass_id, batch_id, train_cost[<span class="number">0</span>], train_acc[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每轮次训练完成后，进行模型评估</span></span><br><span class="line">    test_costs_list = [] <span class="comment"># 存放所有的损失值</span></span><br><span class="line">    test_accs_list = [] <span class="comment"># 存放准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_test_reader()):  <span class="comment"># 读取一个批次测试数据</span></span><br><span class="line">        test_cost, test_acc = exe.run(program=test_program, <span class="comment"># 执行test_program</span></span><br><span class="line">                                      feed=feeder.feed(data), <span class="comment"># 喂入测试数据</span></span><br><span class="line">                                      fetch_list=[avg_cost, acc])  <span class="comment"># 要获取的结果</span></span><br><span class="line">        test_costs_list.append(test_cost[<span class="number">0</span>]) <span class="comment"># 记录损失值</span></span><br><span class="line">        test_accs_list.append(test_acc[<span class="number">0</span>]) <span class="comment"># 记录准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平均准确率和损失值</span></span><br><span class="line">    avg_test_cost = <span class="built_in">sum</span>(test_costs_list) / <span class="built_in">len</span>(test_costs_list)</span><br><span class="line">    avg_test_acc = <span class="built_in">sum</span>(test_accs_list) / <span class="built_in">len</span>(test_accs_list)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;pass_id:%d, test_cost:%f, test_acc:%f&quot;</span> %</span><br><span class="line">          (pass_id, avg_test_cost, avg_test_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(model_save_dir):</span><br><span class="line">    os.makedirs(model_save_dir)</span><br><span class="line">fluid.io.save_inference_model(model_save_dir, <span class="comment"># 模型保存路径</span></span><br><span class="line">                              feeded_var_names=[words.name], <span class="comment"># 使用模型时需传入的参数</span></span><br><span class="line">                              target_vars=[model], <span class="comment"># 预测结果</span></span><br><span class="line">                              executor=exe) <span class="comment"># 执行器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型保存完成.&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>【推理预测】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">model_save_dir = <span class="string">&quot;model/news_classify/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">sentence</span>):</span><br><span class="line">    <span class="comment"># 读取字典中的内容</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        dict_txt = <span class="built_in">eval</span>(f.readlines()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    keys = dict_txt.keys()</span><br><span class="line">    ret = []  <span class="comment"># 编码结果</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sentence:  <span class="comment"># 遍历句子</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">in</span> keys:  <span class="comment"># 字不在字典中，取未知字符</span></span><br><span class="line">            s = <span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line">        ret.append(<span class="built_in">int</span>(dict_txt[s]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建执行器</span></span><br><span class="line">place = fluid.CPUPlace()</span><br><span class="line">exe = fluid.Executor(place)</span><br><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;加载模型&quot;</span>)</span><br><span class="line">infer_program, feeded_var_names, target_var = \</span><br><span class="line">    fluid.io.load_inference_model(dirname=model_save_dir, executor=exe)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试数据</span></span><br><span class="line">texts = []</span><br><span class="line">data1 = get_data(<span class="string">&quot;在获得诺贝尔文学奖7年之后，莫言15日晚间在山西汾阳贾家庄如是说&quot;</span>)</span><br><span class="line">data2 = get_data(<span class="string">&quot;综合&#x27;今日美国&#x27;、《世界日报》等当地媒体报道，芝加哥河滨警察局表示&quot;</span>)</span><br><span class="line">data3 = get_data(<span class="string">&quot;中国队无缘2020年世界杯&quot;</span>)</span><br><span class="line">data4 = get_data(<span class="string">&quot;中国人民银行今日发布通知，降低准备金率，预计释放4000亿流动性&quot;</span>)</span><br><span class="line">data5 = get_data(<span class="string">&quot;10月20日,第六届世界互联网大会正式开幕&quot;</span>)</span><br><span class="line">data6 = get_data(<span class="string">&quot;同一户型，为什么高层比低层要贵那么多？&quot;</span>)</span><br><span class="line">data7 = get_data(<span class="string">&quot;揭秘A股周涨5%资金动向：追捧2类股，抛售600亿香饽饽&quot;</span>)</span><br><span class="line">data8 = get_data(<span class="string">&quot;宋慧乔陷入感染危机，前夫宋仲基不戴口罩露面，身处国外神态轻松&quot;</span>)</span><br><span class="line">data9 = get_data(<span class="string">&quot;此盆栽花很好养，花美似牡丹，三季开花，南北都能养，很值得栽培&quot;</span>)<span class="comment">#不属于任何一个类别</span></span><br><span class="line"></span><br><span class="line">texts.append(data1)</span><br><span class="line">texts.append(data2)</span><br><span class="line">texts.append(data3)</span><br><span class="line">texts.append(data4)</span><br><span class="line">texts.append(data5)</span><br><span class="line">texts.append(data6)</span><br><span class="line">texts.append(data7)</span><br><span class="line">texts.append(data8)</span><br><span class="line">texts.append(data9)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每个句子词数量</span></span><br><span class="line">base_shape = [[<span class="built_in">len</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> texts]]</span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">tensor_words = fluid.create_lod_tensor(texts, base_shape, place)</span><br><span class="line"><span class="comment"># 执行预测</span></span><br><span class="line">result = exe.run(program=infer_program,</span><br><span class="line">                 feed=&#123;feeded_var_names[<span class="number">0</span>]: tensor_words&#125;, <span class="comment"># 待预测的数据</span></span><br><span class="line">                 fetch_list=target_var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(result)</span></span><br><span class="line"></span><br><span class="line">names = [<span class="string">&quot;文化&quot;</span>, <span class="string">&quot;娱乐&quot;</span>, <span class="string">&quot;体育&quot;</span>, <span class="string">&quot;财经&quot;</span>, <span class="string">&quot;房产&quot;</span>, <span class="string">&quot;汽车&quot;</span>, <span class="string">&quot;教育&quot;</span>, <span class="string">&quot;科技&quot;</span>, <span class="string">&quot;国际&quot;</span>, <span class="string">&quot;证券&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取最大值的索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(texts)):</span><br><span class="line">    lab = np.argsort(result)[<span class="number">0</span>][i][-<span class="number">1</span>]  <span class="comment"># 取出最大值的元素下标</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;预测结果：%d, 名称:%s, 概率:%f&quot;</span> % (lab, names[lab], result[<span class="number">0</span>][i][lab]))</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-文本情感分析"><a href="#2-文本情感分析" class="headerlink" title="2. 文本情感分析"></a>2. 文本情感分析</h3><p>1）目标：利用训练数据集，对模型训练，从而实现对中文评论语句情感分析。情绪分为正面、负面两种</p>
<p>2）数据集：中文关于酒店的评论，5265笔用户评论数据，其中2822笔正面评价、其余为负面评价</p>
<p>3）步骤：同上一案例</p>
<p>4）模型选择：</p>
<p><img src="https://image.discover304.top/dl/nlp/Text_emotion_network.png" alt="Text_emotion_network"></p>
<p>5）代码</p>
<p>【数据预处理】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中文情绪分析：数据预处理部分</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.dataset.imdb <span class="keyword">as</span> imdb</span><br><span class="line"><span class="keyword">import</span> paddle.fluid <span class="keyword">as</span> fluid</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> cpu_count</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理，将中文文字解析出来，并进行编码转换为数字，每一行文字存入数组</span></span><br><span class="line">mydict = &#123;&#125;  <span class="comment"># 存放出现的字及编码，格式： 好,1</span></span><br><span class="line">code = <span class="number">1</span></span><br><span class="line">data_file = <span class="string">&quot;data/hotel_discuss2.csv&quot;</span>  <span class="comment"># 原始样本路径</span></span><br><span class="line">dict_file = <span class="string">&quot;data/hotel_dict.txt&quot;</span> <span class="comment"># 字典文件路径</span></span><br><span class="line">encoding_file = <span class="string">&quot;data/hotel_encoding.txt&quot;</span> <span class="comment"># 编码后的样本文件路径</span></span><br><span class="line">puncts = <span class="string">&quot; \n&quot;</span>  <span class="comment"># 要剔除的标点符号列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        <span class="comment"># print(line)</span></span><br><span class="line">        trim_line = line.strip()</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> trim_line:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> puncts:  <span class="comment"># 符号不参与编码</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> mydict:  <span class="comment"># 已经在编码字典中</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">len</span>(ch) &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 当前文字没在字典中</span></span><br><span class="line">                mydict[ch] = code</span><br><span class="line">                code += <span class="number">1</span></span><br><span class="line">    code += <span class="number">1</span></span><br><span class="line">    mydict[<span class="string">&quot;&lt;unk&gt;&quot;</span>] = code  <span class="comment"># 未知字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环结束后，将字典存入字典文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(dict_file, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(mydict))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据字典保存完成！&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将字典文件中的数据加载到mydict字典中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dict</span>():</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        new_dict = <span class="built_in">eval</span>(lines[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> new_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对评论数据进行编码</span></span><br><span class="line">new_dict = load_dict()  <span class="comment"># 调用函数加载</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(encoding_file, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            label = line[<span class="number">0</span>]  <span class="comment"># 标签</span></span><br><span class="line">            remark = line[<span class="number">1</span>:-<span class="number">1</span>]  <span class="comment"># 评论</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> ch <span class="keyword">in</span> remark:</span><br><span class="line">                <span class="keyword">if</span> ch <span class="keyword">in</span> puncts:  <span class="comment"># 符号不参与编码</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    fw.write(<span class="built_in">str</span>(mydict[ch]))</span><br><span class="line">                    fw.write(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            fw.write(<span class="string">&quot;\t&quot;</span> + <span class="built_in">str</span>(label) + <span class="string">&quot;\n&quot;</span>)  <span class="comment"># 写入tab分隔符、标签、换行符</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据预处理完成&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>【模型定义与训练】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取字典的长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dict_len</span>(<span class="params">dict_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        new_dict = <span class="built_in">eval</span>(lines[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(new_dict.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据读取器train_reader和test_reader</span></span><br><span class="line"><span class="comment"># 返回评论列表和标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_mapper</span>(<span class="params">sample</span>):</span><br><span class="line">    dt, lbl = sample</span><br><span class="line">    val = [<span class="built_in">int</span>(word) <span class="keyword">for</span> word <span class="keyword">in</span> dt.split(<span class="string">&quot;,&quot;</span>) <span class="keyword">if</span> word.isdigit()]</span><br><span class="line">    <span class="keyword">return</span> val, <span class="built_in">int</span>(lbl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机从训练数据集文件中取出一行数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_reader</span>(<span class="params">train_list_path</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(train_list_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8-sig&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">            np.random.shuffle(lines)  <span class="comment"># 打乱数据</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                data, label = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">                <span class="keyword">yield</span> data, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回xmap_readers, 能够使用多线程方式读取数据</span></span><br><span class="line">    <span class="keyword">return</span> paddle.reader.xmap_readers(data_mapper,  <span class="comment"># 映射函数</span></span><br><span class="line">                                      reader,  <span class="comment"># 读取数据内容</span></span><br><span class="line">                                      cpu_count(),  <span class="comment"># 线程数量</span></span><br><span class="line">                                      <span class="number">1024</span>)  <span class="comment"># 读取数据队列大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LSTM网络</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm_net</span>(<span class="params">ipt, input_dim</span>):</span><br><span class="line">    ipt = fluid.layers.reshape(ipt, [-<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                               inplace=<span class="literal">True</span>) <span class="comment"># 是否替换，True则表示输入和返回是同一个对象</span></span><br><span class="line">    <span class="comment"># 词嵌入层</span></span><br><span class="line">    emb = fluid.layers.embedding(<span class="built_in">input</span>=ipt, size=[input_dim, <span class="number">128</span>], is_sparse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一个全连接层</span></span><br><span class="line">    fc1 = fluid.layers.fc(<span class="built_in">input</span>=emb, size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一分支：LSTM分支</span></span><br><span class="line">    lstm1, _ = fluid.layers.dynamic_lstm(<span class="built_in">input</span>=fc1, size=<span class="number">128</span>)</span><br><span class="line">    lstm2 = fluid.layers.sequence_pool(<span class="built_in">input</span>=lstm1, pool_type=<span class="string">&quot;max&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二分支</span></span><br><span class="line">    conv = fluid.layers.sequence_pool(<span class="built_in">input</span>=fc1, pool_type=<span class="string">&quot;max&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层：全连接</span></span><br><span class="line">    out = fluid.layers.fc([conv, lstm2], size=<span class="number">2</span>, act=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据，lod_level不为0指定输入数据为序列数据</span></span><br><span class="line">dict_len = get_dict_len(dict_file)  <span class="comment"># 获取数据字典长度</span></span><br><span class="line">rmk = fluid.layers.data(name=<span class="string">&quot;rmk&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&quot;int64&quot;</span>, lod_level=<span class="number">1</span>)</span><br><span class="line">label = fluid.layers.data(name=<span class="string">&quot;label&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&quot;int64&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义长短期记忆网络</span></span><br><span class="line">model = lstm_net(rmk, dict_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，情绪判断实际是一个分类任务，使用交叉熵作为损失函数</span></span><br><span class="line">cost = fluid.layers.cross_entropy(<span class="built_in">input</span>=model, label=label)</span><br><span class="line">avg_cost = fluid.layers.mean(cost)  <span class="comment"># 求损失值平均数</span></span><br><span class="line"><span class="comment"># layers.accuracy接口，用来评估预测准确率</span></span><br><span class="line">acc = fluid.layers.accuracy(<span class="built_in">input</span>=model, label=label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化方法</span></span><br><span class="line"><span class="comment"># Adagrad(自适应学习率，前期放大梯度调节，后期缩小梯度调节)</span></span><br><span class="line">optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">opt = optimizer.minimize(avg_cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line"><span class="comment"># place = fluid.CPUPlace()</span></span><br><span class="line">place = fluid.CUDAPlace(<span class="number">0</span>)</span><br><span class="line">exe = fluid.Executor(place)</span><br><span class="line">exe.run(fluid.default_startup_program())  <span class="comment"># 参数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义reader</span></span><br><span class="line">reader = train_reader(encoding_file)</span><br><span class="line">batch_train_reader = paddle.batch(reader, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据的维度，数据的顺序是一条句子数据对应一个标签</span></span><br><span class="line">feeder = fluid.DataFeeder(place=place, feed_list=[rmk, label])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pass_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">40</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_train_reader()):</span><br><span class="line">        train_cost, train_acc = exe.run(program=fluid.default_main_program(),</span><br><span class="line">                                        feed=feeder.feed(data),</span><br><span class="line">                                        fetch_list=[avg_cost, acc])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_id % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;pass_id: %d, batch_id: %d, cost: %0.5f, acc:%.5f&quot;</span> %</span><br><span class="line">                  (pass_id, batch_id, train_cost[<span class="number">0</span>], train_acc))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型训练完成......&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model_save_dir = <span class="string">&quot;model/chn_emotion_analyses.model&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(model_save_dir):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;create model path&quot;</span>)</span><br><span class="line">    os.makedirs(model_save_dir)</span><br><span class="line"></span><br><span class="line">fluid.io.save_inference_model(model_save_dir,  <span class="comment"># 保存路径</span></span><br><span class="line">                              feeded_var_names=[rmk.name],</span><br><span class="line">                              target_vars=[model],</span><br><span class="line">                              executor=exe)  <span class="comment"># Executor</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型保存完成, 保存路径: &quot;</span>, model_save_dir)</span><br></pre></td></tr></table></figure>



<p>【推理预测】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.fluid <span class="keyword">as</span> fluid</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> cpu_count</span><br><span class="line"></span><br><span class="line">data_file = <span class="string">&quot;data/hotel_discuss2.csv&quot;</span></span><br><span class="line">dict_file = <span class="string">&quot;data/hotel_dict.txt&quot;</span></span><br><span class="line">encoding_file = <span class="string">&quot;data/hotel_encoding.txt&quot;</span></span><br><span class="line">model_save_dir = <span class="string">&quot;model/chn_emotion_analyses.model&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dict</span>():</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dict_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        new_dict = <span class="built_in">eval</span>(lines[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> new_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据字典对字符串进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_by_dict</span>(<span class="params">remark, dict_encoded</span>):</span><br><span class="line">    remark = remark.strip()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(remark) &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    ret = []</span><br><span class="line">    <span class="keyword">for</span> ch <span class="keyword">in</span> remark:</span><br><span class="line">        <span class="keyword">if</span> ch <span class="keyword">in</span> dict_encoded:</span><br><span class="line">            ret.append(dict_encoded[ch])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret.append(dict_encoded[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码,预测</span></span><br><span class="line">lods = []</span><br><span class="line">new_dict = load_dict()</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;总体来说房间非常干净,卫浴设施也相当不错,交通也比较便利&quot;</span>, new_dict))</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;酒店交通方便，环境也不错，正好是我们办事地点的旁边，感觉性价比还可以&quot;</span>, new_dict))</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;设施还可以，服务人员态度也好，交通还算便利&quot;</span>, new_dict))</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;酒店服务态度极差，设施很差&quot;</span>, new_dict))</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;我住过的最不好的酒店,以后决不住了&quot;</span>, new_dict))</span><br><span class="line">lods.append(encode_by_dict(<span class="string">&quot;说实在的我很失望，我想这家酒店以后无论如何我都不会再去了&quot;</span>, new_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每句话的单词数量</span></span><br><span class="line">base_shape = [[<span class="built_in">len</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> lods]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成预测数据</span></span><br><span class="line">place = fluid.CPUPlace()</span><br><span class="line">infer_exe = fluid.Executor(place)</span><br><span class="line">infer_exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line">tensor_words = fluid.create_lod_tensor(lods, base_shape, place)</span><br><span class="line"></span><br><span class="line">infer_program, feed_target_names, fetch_targets = fluid.io.load_inference_model(dirname=model_save_dir, executor=infer_exe)</span><br><span class="line"><span class="comment"># tvar = np.array(fetch_targets, dtype=&quot;int64&quot;)</span></span><br><span class="line">results = infer_exe.run(program=infer_program,</span><br><span class="line">                  feed=&#123;feed_target_names[<span class="number">0</span>]: tensor_words&#125;,</span><br><span class="line">                  fetch_list=fetch_targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每句话的正负面预测概率</span></span><br><span class="line"><span class="keyword">for</span> i, r <span class="keyword">in</span> <span class="built_in">enumerate</span>(results[<span class="number">0</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;负面: %0.5f, 正面: %0.5f&quot;</span> % (r[<span class="number">0</span>], r[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="附录一：相关数学知识"><a href="#附录一：相关数学知识" class="headerlink" title="附录一：相关数学知识"></a>附录一：相关数学知识</h2><p>1）向量余弦相似度</p>
<p>余弦相似度使用来度量向量相似度的指标，当两个向量夹角越大相似度越低；当两个向量夹角越小，相似度越高。</p>
<p><img src="https://image.discover304.top/dl/nlp/vector.png" alt="vector"></p>
<p>在三角形中，余弦值计算方式为$cos \theta &#x3D; \frac{a^2 + b^2 - c^2}{2ab}$，向量夹角余弦计算公式为：<br>$$<br>cos \theta &#x3D; \frac{ab}{||a|| \times ||b||}<br>$$<br>分子为两个向量的内积，分母是两个向量模长的乘积。</p>
<p><img src="https://image.discover304.top/dl/nlp/vector_cos.png" alt="vector_cos"></p>
<p>其推导过程如下：<br>$$<br>cos \theta &#x3D; \frac{a^2 + b^2 - c^2}{2ab} \<br> &#x3D; \frac{\sqrt{x_1^2 + y_1^2} + \sqrt{x_2^2 + y_2^2 }+ \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}}{2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} \<br>&#x3D; \frac{2 x_1 x_2 + 2 y_1 y_2}{2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} &#x3D; \frac{ab}{||a|| \times ||b||}<br>$$<br>以上是二维向量的计算过程，推广到N维向量，分子部分依然是向量的内积，分母部分依然是两个向量模长的乘积。由此可计算文本的余弦相似度。</p>
<h2 id="附录二：参考文献"><a href="#附录二：参考文献" class="headerlink" title="附录二：参考文献"></a>附录二：参考文献</h2><p>1）《Python自然语言处理实践——核心技术与算法》  ，涂铭、刘祥、刘树春 著 ，机械工业出版社</p>
<p>2）《Tensorflow自然语言处理》，【澳】图珊·加内格达拉，机械工业出版社</p>
<p>3）《深度学习之美》，张玉宏，中国工信出版集团 &#x2F; 电子工业出版社</p>
<p>4）网络部分资源</p>
<h2 id="附录三：专业词汇列表"><a href="#附录三：专业词汇列表" class="headerlink" title="附录三：专业词汇列表"></a>附录三：专业词汇列表</h2><table>
<thead>
<tr>
<th>英文简写</th>
<th>英文全写</th>
<th>中文</th>
</tr>
</thead>
<tbody><tr>
<td>NLP</td>
<td>Nature Language Processing</td>
<td>自然语言处理</td>
</tr>
<tr>
<td>NER</td>
<td>Named Entities Recognition</td>
<td>命名实体识别</td>
</tr>
<tr>
<td>PoS</td>
<td>part-of-speech tagging</td>
<td>词性标记</td>
</tr>
<tr>
<td>MT</td>
<td>Machine Translation</td>
<td>机器翻译</td>
</tr>
<tr>
<td>TF-IDF</td>
<td>Term Frequency-Inverse Document Frequency</td>
<td>词频-逆文档频率</td>
</tr>
<tr>
<td>Text Rank</td>
<td></td>
<td>文本排名算法</td>
</tr>
<tr>
<td>One-hot</td>
<td></td>
<td>独热编码</td>
</tr>
<tr>
<td>BOW</td>
<td>Bag-of-Words Model</td>
<td>词袋模型</td>
</tr>
<tr>
<td>N-Gram</td>
<td></td>
<td>N元模型</td>
</tr>
<tr>
<td>word embedding</td>
<td></td>
<td>词嵌入</td>
</tr>
<tr>
<td>NNLM</td>
<td>Neural Network Language Model</td>
<td>神经网络语言模型</td>
</tr>
<tr>
<td>HMM</td>
<td>Hidden Markov Model</td>
<td>隐马尔可夫模型</td>
</tr>
<tr>
<td>RNN</td>
<td>Recurrent Neural Networks</td>
<td>循环神经网络</td>
</tr>
<tr>
<td>Skip-gram</td>
<td></td>
<td>跳字模型</td>
</tr>
<tr>
<td>CBOW</td>
<td>Continous Bag of Words</td>
<td>连续词袋模型</td>
</tr>
<tr>
<td>LSTM</td>
<td>Long Short Term Memory</td>
<td>长短期记忆模型</td>
</tr>
<tr>
<td>GRU</td>
<td>Gated Recurrent Unit</td>
<td>门控环单元</td>
</tr>
<tr>
<td>BRNN</td>
<td>Bi-recurrent neural network</td>
<td>双向循环神经网络</td>
</tr>
<tr>
<td>FMM</td>
<td>Forward Maximum Matching</td>
<td>正向最大匹配</td>
</tr>
<tr>
<td>RMM</td>
<td>Reverse Maximum Matching</td>
<td>逆向最大匹配</td>
</tr>
<tr>
<td>Bi-MM</td>
<td>Bi-directional Maximum Matching</td>
<td>双向最大匹配法</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:hobart.yang@qq.com">✨YangSier✨</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://discover304.top/2021/12/13/2021q4/113-5-dl-nlp/">https://discover304.top/2021/12/13/2021q4/113-5-dl-nlp/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0anime</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a><a class="post-meta__tags" href="/tags/%E8%AE%B0%E5%BD%95/">记录</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E" data-sites="facebook,twitter,wechat,weibo,qzone,qq,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/13/2021q4/116-autodrive-terms-collection/"><img class="prev-cover" src="https://image.discover304.top/autodrive/cover/car_black_white.jpg?imageView2/2/h/300" onerror="onerror=null;src='/img/404.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">【自动驾驶】技术笔记：概述</div></div></a></div><div class="next-post pull-right"><a href="/2021/12/13/2021q4/113-4-dl-face/"><img class="next-cover" src="https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E" onerror="onerror=null;src='/img/404.png'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">【深度学习】人脸检测与人脸识别</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/11/30/2021q4/107-1-dl-back/" title="【深度学习】基础 叁：反向传播算法"><img class="cover" src="https://image.discover304.top/ai/dl/machine-girl.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】基础 叁：反向传播算法</div></div></a></div><div><a href="/2021/11/30/2021q4/107-0-dl/" title="【深度学习】概述"><img class="cover" src="https://image.discover304.top/ai/dl/machine-girl.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】概述</div></div></a></div><div><a href="/2021/11/30/2021q4/107-1-dl-loss-gd/" title="【深度学习】基础 贰：损失函数与梯度下降"><img class="cover" src="https://image.discover304.top/ai/dl/machine-girl.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】基础 贰：损失函数与梯度下降</div></div></a></div><div><a href="/2021/11/30/2021q4/107-1-dl-cnn/" title="【深度学习】基础 肆：卷积神经网络"><img class="cover" src="https://image.discover304.top/ai/dl/machine-girl.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】基础 肆：卷积神经网络</div></div></a></div><div><a href="/2021/11/30/2021q4/107-1-dl-perceptron/" title="【深度学习】基础 壹：感知机与神经网络"><img class="cover" src="https://image.discover304.top/ai/dl/machine-girl.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】基础 壹：感知机与神经网络</div></div></a></div><div><a href="/2021/12/02/2021q4/107-2-dl/" title="【深度学习】图像操作：OpenCV"><img class="cover" src="https://image.discover304.top/ai/dl/02/dog-end-world.jpg?imageView2/2/h/300" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2021-12-25</div><div class="title">【深度学习】图像操作：OpenCV</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">✨YangSier✨</div><div class="author-info__description">Love Everything You Like.</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">253</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">88</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://space.bilibili.com/98639326"><i class="fab fa-bilibili"></i><span>Bilibili Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Discover304" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/Discover304" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a><a class="social-icon" href="https://www.zhihu.com/people/discover-56-86-75" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="mailto:hobart.yang@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://jq.qq.com/?_wv=1027&amp;k=EaGddTQg" target="_blank" title="QQ"><i class="fa-brands fa-qq"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">✨动态更新：<p style="text-align:center">享受精彩大学生活中。</p>✨聊天划水QQ群：<p style="text-align:center"><a target="_blank" rel="noopener" href="https://jq.qq.com/?_wv=1027&k=EaGddTQg"><strong>兔叽の魔术工房</strong></a><br>942-848-525</p>✨我们的口号是：<p style="text-align:center; color:#39C5BB">人工降神，机械飞升！</p><a target="_blank" rel="noopener" href='https://space.bilibili.com/98639326'><img src='/img/mikulittletrans.png'></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89%E8%AE%B2%E4%B9%89"><span class="toc-text">自然语言处理（NLP）讲义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81NLP%E6%A6%82%E8%BF%B0"><span class="toc-text">一、NLP概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-NLP%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-text">1. NLP的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-NLP%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BB%BB%E5%8A%A1"><span class="toc-text">2. NLP的主要任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E5%88%86%E8%AF%8D"><span class="toc-text">1）分词</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E6%AD%A7"><span class="toc-text">2）词义消歧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89"><span class="toc-text">3）命名实体识别（NER）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89%E8%AF%8D%E6%80%A7%E6%A0%87%E8%AE%B0%EF%BC%88PoS%EF%BC%89"><span class="toc-text">4）词性标记（PoS）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-text">5）文本分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6%EF%BC%89%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90"><span class="toc-text">6）语言生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%EF%BC%89%E9%97%AE%E7%AD%94%EF%BC%88QA%EF%BC%89%E7%B3%BB%E7%BB%9F"><span class="toc-text">5）问答（QA）系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6%EF%BC%89%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%EF%BC%88MT%EF%BC%89"><span class="toc-text">6）机器翻译（MT）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-NLP%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-text">3. NLP的发展历程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E8%90%8C%E8%8A%BD%E6%9C%9F%EF%BC%881956%E5%B9%B4%E4%BB%A5%E5%89%8D%EF%BC%89"><span class="toc-text">1）萌芽期（1956年以前）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E5%BF%AB%E9%80%9F%E5%8F%91%E5%B1%95%E6%9C%9F%EF%BC%881957-1970%EF%BC%89"><span class="toc-text">2）快速发展期（1957~1970）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E4%BD%8E%E9%80%9F%E5%8F%91%E5%B1%95%E6%9C%9F%EF%BC%881971-1993%EF%BC%89"><span class="toc-text">3）低速发展期（1971~1993）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89%E5%A4%8D%E8%8B%8F%E8%9E%8D%E5%90%88%E6%9C%9F%EF%BC%881994%E5%B9%B4%E8%87%B3%E4%BB%8A%EF%BC%89"><span class="toc-text">4）复苏融合期（1994年至今）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-NLP%E7%9A%84%E5%9B%B0%E9%9A%BE%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-text">4. NLP的困难与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E8%AF%AD%E8%A8%80%E6%AD%A7%E4%B9%89"><span class="toc-text">1）语言歧义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E4%B8%8D%E5%90%8C%E8%AF%AD%E8%A8%80%E7%BB%93%E6%9E%84%E5%B7%AE%E5%BC%82"><span class="toc-text">2）不同语言结构差异</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E6%9C%AA%E7%9F%A5%E8%AF%AD%E8%A8%80%E4%B8%8D%E5%8F%AF%E9%A2%84%E6%B5%8B%E6%80%A7"><span class="toc-text">3）未知语言不可预测性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89%E8%AF%AD%E8%A8%80%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-text">4）语言表达的复杂性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%EF%BC%89%E6%9C%BA%E5%99%A8%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E7%BC%BA%E4%B9%8F%E8%83%8C%E6%99%AF%E4%B8%8E%E5%B8%B8%E8%AF%86"><span class="toc-text">5）机器处理语言缺乏背景与常识</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-NLP%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E6%9E%84%E6%88%90"><span class="toc-text">5. NLP相关知识构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AF%AD%E6%96%99%E5%BA%93"><span class="toc-text">6. 语料库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E6%96%99%E5%BA%93"><span class="toc-text">1）什么是语料库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E8%AF%AD%E6%96%99%E5%BA%93%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-text">2）语料库的特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E8%AF%AD%E6%96%99%E5%BA%93%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">3）语料库的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E4%BB%8B%E7%BB%8D"><span class="toc-text">4）常用语料库介绍</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BC%A0%E7%BB%9FNLP%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF"><span class="toc-text">二、传统NLP处理技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D"><span class="toc-text">1. 中文分词</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E6%AD%A3%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95"><span class="toc-text">1）正向最大匹配法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E9%80%86%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95"><span class="toc-text">2）逆向最大匹配法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E5%8F%8C%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95"><span class="toc-text">3）双向最大匹配法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8"><span class="toc-text">2. 词性标注</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8"><span class="toc-text">1）什么是词性标注</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">2）词性标注的原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E8%A7%84%E8%8C%83"><span class="toc-text">3）词性标注规范</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89Jieba%E5%BA%93%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8"><span class="toc-text">4）Jieba库词性标注</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89"><span class="toc-text">3. 命名实体识别（NER）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96"><span class="toc-text">4. 关键词提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89TF-IDF%E7%AE%97%E6%B3%95"><span class="toc-text">1）TF-IDF算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89TextRank%E7%AE%97%E6%B3%95"><span class="toc-text">2）TextRank算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%A4%BA%E4%BE%8B"><span class="toc-text">3）关键词提取示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B"><span class="toc-text">6. 综合案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB"><span class="toc-text">1）垃圾邮件分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA"><span class="toc-text">三、文本表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-One-hot"><span class="toc-text">1. One-hot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. 词袋模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-TF-IDF"><span class="toc-text">3. TF-IDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5"><span class="toc-text">4. 共现矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-N-Gram%E8%A1%A8%E7%A4%BA"><span class="toc-text">5. N-Gram表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-text">6. 词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-text">1）什么是词嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-text">2）词嵌入的优点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">四、语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. 什么是语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-N-gram%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. N-gram模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88NNLM%EF%BC%89"><span class="toc-text">3. 神经网络语言模型（NNLM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Word2vec"><span class="toc-text">4. Word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89Skip-gram"><span class="toc-text">1）Skip-gram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89CBOW%E6%A8%A1%E5%9E%8B"><span class="toc-text">2）CBOW模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E7%A4%BA%E4%BE%8B%EF%BC%9A%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F"><span class="toc-text">3）示例：训练词向量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-text">5. 循环神经网络（RNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E5%8E%9F%E7%94%9FRNN"><span class="toc-text">1）原生RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A0-RNN%E8%B5%B7%E6%BA%90%E5%8F%8A%E5%8F%91%E5%B1%95"><span class="toc-text">① RNN起源及发展</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A1-RNN%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-text">② RNN的结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A2-RNN%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="toc-text">③ RNN的功能</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A3-RNN%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-text">④ RNN的缺陷</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E6%A8%A1%E5%9E%8B%EF%BC%88LSTM%EF%BC%89"><span class="toc-text">2）长短期记忆模型（LSTM）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3）双向循环神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81NLP%E5%BA%94%E7%94%A8"><span class="toc-text">五、NLP应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-text">1. 文本分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-text">1）什么是文本分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">2）文本分类的应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B"><span class="toc-text">3）文本分类案例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-text">2. 文本情感分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95%E4%B8%80%EF%BC%9A%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86"><span class="toc-text">附录一：相关数学知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95%E4%BA%8C%EF%BC%9A%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">附录二：参考文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95%E4%B8%89%EF%BC%9A%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87%E5%88%97%E8%A1%A8"><span class="toc-text">附录三：专业词汇列表</span></a></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/177-6-aro-other/" title="ARO Additional Knowledge"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Additional Knowledge"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/177-6-aro-other/" title="ARO Additional Knowledge">ARO Additional Knowledge</a><time datetime="2023-12-19T09:00:21.000Z" title="Created 2023-12-19 17:00:21">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/176-5-aro-controller/" title="ARO Robot Controller"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Robot Controller"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/176-5-aro-controller/" title="ARO Robot Controller">ARO Robot Controller</a><time datetime="2023-12-19T08:59:59.000Z" title="Created 2023-12-19 16:59:59">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/176-4-aro-dynamics/" title="ARO Robot Dynamics"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Robot Dynamics"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/176-4-aro-dynamics/" title="ARO Robot Dynamics">ARO Robot Dynamics</a><time datetime="2023-12-19T08:59:42.000Z" title="Created 2023-12-19 16:59:42">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/176-3-aro-planning/" title="ARO Robot Motion Planning"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Robot Motion Planning"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/176-3-aro-planning/" title="ARO Robot Motion Planning">ARO Robot Motion Planning</a><time datetime="2023-12-19T08:59:25.000Z" title="Created 2023-12-19 16:59:25">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/176-2-aro-kinematics/" title="ARO Robot Kinematics"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Robot Kinematics"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/176-2-aro-kinematics/" title="ARO Robot Kinematics">ARO Robot Kinematics</a><time datetime="2023-12-19T08:59:10.000Z" title="Created 2023-12-19 16:59:10">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/19/2023q4/176-1-aro-note/" title="ARO Robot Geometry"><img src="https://image.discover304.top/s16470012192023.png?imageView2/2/h/400" onerror="this.onerror=null;this.src='/img/404.png'" alt="ARO Robot Geometry"/></a><div class="content"><a class="title" href="/2023/12/19/2023q4/176-1-aro-note/" title="ARO Robot Geometry">ARO Robot Geometry</a><time datetime="2023-12-19T08:25:59.000Z" title="Created 2023-12-19 16:25:59">2023-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/05/2023q4/175-qiniu-fix-archieve/" title="Enhanced Workflow for Managing Files in Qiniu Cloud Storage"><img src="https://image.discover304.top/huashi6-shichen.jpeg?imageView2/2/h/300" onerror="this.onerror=null;this.src='/img/404.png'" alt="Enhanced Workflow for Managing Files in Qiniu Cloud Storage"/></a><div class="content"><a class="title" href="/2023/12/05/2023q4/175-qiniu-fix-archieve/" title="Enhanced Workflow for Managing Files in Qiniu Cloud Storage">Enhanced Workflow for Managing Files in Qiniu Cloud Storage</a><time datetime="2023-12-05T04:42:10.000Z" title="Created 2023-12-05 12:42:10">2023-12-05</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://img2.huashi6.com/images/resource/thumbnail/2021/12/05/1496_54694744582.jpg?imageMogr2/quality/100/interlace/1/thumbnail/1000x%3E)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By ✨YangSier✨</div><div><a target="_blank" href="https://beian.miit.gov.cn/" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;"> 冀ICP备2021025381号-1</p></a></div><div><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=13060602001430" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><img src="/img/beian.png" style="float:left;"/><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;">冀公网安备 13060602001430号</p></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.spacingElementById('content-inner')
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.spacingElementById('content-inner')
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'neutral',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'A9RWVELPcIotgfbpp9KLGXQM-gzGzoHsz',
      appKey: 'MLgPQW5h0DPgE8jNkeREKubU',
      placeholder: '欢迎留言呀。（网址是选填，可以留空）',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: true,
      serverURLs: 'https://a9rwvelp.lc-cn-n1-shared.com',
      emojiCDN: 'https://cdn.jsdelivr.net/gh/GamerNoTitle/ValineCDN@master/',
      emojiMaps: {"QQ1":"QQ/aini.gif","QQ2":"QQ/aixin.gif","QQ3":"QQ/aoman.gif","QQ4":"QQ/baiyan.gif","QQ5":"QQ/bangbangtang.gif","QQ6":"QQ/baojin.gif","QQ7":"QQ/baoquan.gif","QQ8":"QQ/bishi.gif","QQ9":"QQ/bizui.gif","QQ11":"QQ/cahan.gif","QQ12":"QQ/caidao.gif","QQ13":"QQ/chi.gif","QQ14":"QQ/ciya.gif","QQ15":"QQ/dabing.gif","QQ16":"QQ/daku.gif","QQ17":"QQ/dan.gif","QQ18":"QQ/deyi.gif","QQ19":"QQ/doge.gif","QQ20":"QQ/fadai.gif","QQ21":"QQ/fanu.gif","QQ22":"QQ/fendou.gif","QQ23":"QQ/ganga.gif","QQ24":"QQ/gouyin.gif","QQ25":"QQ/guzhang.gif","QQ26":"QQ/haixiu.gif","QQ27":"QQ/hanxiao.gif","QQ28":"QQ/haobang.gif","QQ29":"QQ/haqian.gif","QQ30":"QQ/hecai.gif","QQ31":"QQ/hexie.gif","QQ32":"QQ/huaixiao.gif","QQ33":"QQ/jie.gif","QQ34":"QQ/jingkong.gif","QQ35":"QQ/jingxi.gif","QQ36":"QQ/jingya.gif","QQ37":"QQ/juhua.gif","QQ38":"QQ/keai.gif","QQ39":"QQ/kelian.gif","QQ40":"QQ/koubi.gif","QQ41":"QQ/ku.gif","QQ42":"QQ/kuaikule.gif","QQ43":"QQ/kulou.gif","QQ44":"QQ/kun.gif","QQ45":"QQ/lanqiu.gif","QQ46":"QQ/leiben.gif","QQ47":"QQ/lenghan.gif","QQ48":"QQ/liuhan.gif","QQ49":"QQ/liulei.gif","QQ50":"QQ/nanguo.gif","QQ51":"QQ/OK.gif","QQ52":"QQ/penxue.gif","QQ53":"QQ/piezui.gif","QQ54":"QQ/pijiu.gif","QQ55":"QQ/qiang.gif","QQ56":"QQ/qiaoda.gif","QQ57":"QQ/qinqin.gif","QQ58":"QQ/qiudale.gif","QQ59":"QQ/quantou.gif","QQ60":"QQ/saorao.gif","QQ61":"QQ/se.gif","QQ62":"QQ/shengli.gif","QQ63":"QQ/shouqiang.gif","QQ64":"QQ/shuai.gif","QQ65":"QQ/shui.gif","QQ66":"QQ/tiaopi.gif","QQ67":"QQ/touxiao.gif","QQ68":"QQ/tu.gif","QQ69":"QQ/tuosai.gif","QQ70":"QQ/weiqu.gif","QQ71":"QQ/weixiao.gif","QQ72":"QQ/woshou.gif","QQ73":"QQ/wozuimei.gif","QQ74":"QQ/wunai.gif","QQ75":"QQ/xia.gif","QQ76":"QQ/xiaojiujie.gif","QQ77":"QQ/xiaoku.gif","QQ78":"QQ/xiaoyanger.gif","QQ79":"QQ/xieyanxiao.gif","QQ80":"QQ/xigua.gif","QQ81":"QQ/xu.gif","QQ82":"QQ/yangtuo.gif","QQ83":"QQ/yinxian.gif","QQ84":"QQ/yiwen.gif","QQ85":"QQ/youhengheng.gif","QQ86":"QQ/youling.gif","QQ87":"QQ/yun.gif","QQ88":"QQ/zaijian.gif","QQ89":"QQ/zhayanjian.gif","QQ90":"QQ/zhemo.gif","QQ91":"QQ/zhouma.gif","QQ92":"QQ/zhuakuang.gif","QQ93":"QQ/zuohengheng.gif","bilibiliHotKey1":"bilibiliHotKey/1.jpg","bilibiliHotKey2":"bilibiliHotKey/10.jpg","bilibiliHotKey3":"bilibiliHotKey/11.jpg","bilibiliHotKey4":"bilibiliHotKey/12.jpg","bilibiliHotKey5":"bilibiliHotKey/13.jpg","bilibiliHotKey6":"bilibiliHotKey/14.jpg","bilibiliHotKey7":"bilibiliHotKey/15.jpg","bilibiliHotKey8":"bilibiliHotKey/16.jpg","bilibiliHotKey9":"bilibiliHotKey/17.jpg","bilibiliHotKey10":"bilibiliHotKey/18.jpg","bilibiliHotKey11":"bilibiliHotKey/19.jpg","bilibiliHotKey12":"bilibiliHotKey/2.jpg","bilibiliHotKey13":"bilibiliHotKey/20.jpg","bilibiliHotKey14":"bilibiliHotKey/21.jpg","bilibiliHotKey15":"bilibiliHotKey/22.jpg","bilibiliHotKey16":"bilibiliHotKey/23.jpg","bilibiliHotKey17":"bilibiliHotKey/24.jpg","bilibiliHotKey18":"bilibiliHotKey/25.jpg","bilibiliHotKey19":"bilibiliHotKey/26.jpg","bilibiliHotKey20":"bilibiliHotKey/27.jpg","bilibiliHotKey21":"bilibiliHotKey/28.jpg","bilibiliHotKey22":"bilibiliHotKey/29.jpg","bilibiliHotKey23":"bilibiliHotKey/3.jpg","bilibiliHotKey24":"bilibiliHotKey/30.jpg","bilibiliHotKey25":"bilibiliHotKey/31.jpg","bilibiliHotKey26":"bilibiliHotKey/32.jpg","bilibiliHotKey27":"bilibiliHotKey/4.jpg","bilibiliHotKey28":"bilibiliHotKey/5.jpg","bilibiliHotKey29":"bilibiliHotKey/6.jpg","bilibiliHotKey30":"bilibiliHotKey/7.jpg","bilibiliHotKey31":"bilibiliHotKey/8.jpg","bilibiliHotKey32":"bilibiliHotKey/9.jpg","Menhera-chan1":"Menhera-chan/1.jpg","Menhera-chan2":"Menhera-chan/10.jpg","Menhera-chan3":"Menhera-chan/100.jpg","Menhera-chan4":"Menhera-chan/101.jpg","Menhera-chan5":"Menhera-chan/102.jpg","Menhera-chan6":"Menhera-chan/103.jpg","Menhera-chan7":"Menhera-chan/104.jpg","Menhera-chan8":"Menhera-chan/105.jpg","Menhera-chan9":"Menhera-chan/106.jpg","Menhera-chan10":"Menhera-chan/107.jpg","Menhera-chan11":"Menhera-chan/108.jpg","Menhera-chan12":"Menhera-chan/109.jpg","Menhera-chan13":"Menhera-chan/11.jpg","Menhera-chan14":"Menhera-chan/110.jpg","Menhera-chan15":"Menhera-chan/111.jpg","Menhera-chan16":"Menhera-chan/112.jpg","Menhera-chan17":"Menhera-chan/113.jpg","Menhera-chan18":"Menhera-chan/114.jpg","Menhera-chan19":"Menhera-chan/115.jpg","Menhera-chan20":"Menhera-chan/116.jpg","Menhera-chan21":"Menhera-chan/117.jpg","Menhera-chan22":"Menhera-chan/118.jpg","Menhera-chan23":"Menhera-chan/119.jpg","Menhera-chan24":"Menhera-chan/12.jpg","Menhera-chan25":"Menhera-chan/120.jpg","Menhera-chan26":"Menhera-chan/13.jpg","Menhera-chan27":"Menhera-chan/14.jpg","Menhera-chan28":"Menhera-chan/15.jpg","Menhera-chan29":"Menhera-chan/16.jpg","Menhera-chan30":"Menhera-chan/17.jpg","Menhera-chan31":"Menhera-chan/18.jpg","Menhera-chan32":"Menhera-chan/19.jpg","Menhera-chan33":"Menhera-chan/2.jpg","Menhera-chan34":"Menhera-chan/20.jpg","Menhera-chan35":"Menhera-chan/21.jpg","Menhera-chan36":"Menhera-chan/22.jpg","Menhera-chan37":"Menhera-chan/23.jpg","Menhera-chan38":"Menhera-chan/24.jpg","Menhera-chan39":"Menhera-chan/25.jpg","Menhera-chan40":"Menhera-chan/26.jpg","Menhera-chan41":"Menhera-chan/27.jpg","Menhera-chan42":"Menhera-chan/28.jpg","Menhera-chan43":"Menhera-chan/29.jpg","Menhera-chan44":"Menhera-chan/3.jpg","Menhera-chan45":"Menhera-chan/30.jpg","Menhera-chan46":"Menhera-chan/31.jpg","Menhera-chan47":"Menhera-chan/32.jpg","Menhera-chan48":"Menhera-chan/33.jpg","Menhera-chan49":"Menhera-chan/34.jpg","Menhera-chan50":"Menhera-chan/35.jpg","Menhera-chan51":"Menhera-chan/36.jpg","Menhera-chan52":"Menhera-chan/37.jpg","Menhera-chan53":"Menhera-chan/38.jpg","Menhera-chan54":"Menhera-chan/39.jpg","Menhera-chan55":"Menhera-chan/4.jpg","Menhera-chan56":"Menhera-chan/40.jpg","Menhera-chan57":"Menhera-chan/41.jpg","Menhera-chan58":"Menhera-chan/42.jpg","Menhera-chan59":"Menhera-chan/43.jpg","Menhera-chan60":"Menhera-chan/44.jpg","Menhera-chan61":"Menhera-chan/45.jpg","Menhera-chan62":"Menhera-chan/46.jpg","Menhera-chan63":"Menhera-chan/47.jpg","Menhera-chan64":"Menhera-chan/48.jpg","Menhera-chan65":"Menhera-chan/49.jpg","Menhera-chan66":"Menhera-chan/5.jpg","Menhera-chan67":"Menhera-chan/50.jpg","Menhera-chan68":"Menhera-chan/51.jpg","Menhera-chan69":"Menhera-chan/52.jpg","Menhera-chan70":"Menhera-chan/53(1).jpg","Menhera-chan71":"Menhera-chan/53.jpg","Menhera-chan72":"Menhera-chan/54.jpg","Menhera-chan73":"Menhera-chan/55.jpg","Menhera-chan74":"Menhera-chan/56.jpg","Menhera-chan75":"Menhera-chan/57.jpg","Menhera-chan76":"Menhera-chan/58.jpg","Menhera-chan77":"Menhera-chan/59.jpg","Menhera-chan78":"Menhera-chan/6.jpg","Menhera-chan79":"Menhera-chan/60.jpg","Menhera-chan80":"Menhera-chan/61.jpg","Menhera-chan81":"Menhera-chan/62.jpg","Menhera-chan82":"Menhera-chan/63.jpg","Menhera-chan83":"Menhera-chan/64.jpg","Menhera-chan84":"Menhera-chan/65.jpg","Menhera-chan85":"Menhera-chan/66.jpg","Menhera-chan86":"Menhera-chan/67.jpg","Menhera-chan87":"Menhera-chan/68.jpg","Menhera-chan88":"Menhera-chan/69.jpg","Menhera-chan89":"Menhera-chan/7.jpg","Menhera-chan90":"Menhera-chan/70.jpg","Menhera-chan91":"Menhera-chan/71.jpg","Menhera-chan92":"Menhera-chan/72.jpg","Menhera-chan93":"Menhera-chan/73.jpg","Menhera-chan94":"Menhera-chan/74.jpg","Menhera-chan95":"Menhera-chan/75.jpg","Menhera-chan96":"Menhera-chan/76.jpg","Menhera-chan97":"Menhera-chan/77.jpg","Menhera-chan98":"Menhera-chan/78.jpg","Menhera-chan99":"Menhera-chan/79.jpg","Menhera-chan100":"Menhera-chan/8.jpg","Menhera-chan101":"Menhera-chan/80.jpg","Menhera-chan102":"Menhera-chan/81.jpg","Menhera-chan103":"Menhera-chan/82.jpg","Menhera-chan104":"Menhera-chan/83.jpg","Menhera-chan105":"Menhera-chan/84.jpg","Menhera-chan106":"Menhera-chan/85.jpg","Menhera-chan107":"Menhera-chan/86.jpg","Menhera-chan108":"Menhera-chan/87.jpg","Menhera-chan109":"Menhera-chan/88.jpg","Menhera-chan110":"Menhera-chan/89.jpg","Menhera-chan111":"Menhera-chan/9.jpg","Menhera-chan112":"Menhera-chan/90.jpg","Menhera-chan113":"Menhera-chan/91.jpg","Menhera-chan114":"Menhera-chan/92.jpg","Menhera-chan115":"Menhera-chan/93.jpg","Menhera-chan116":"Menhera-chan/94.jpg","Menhera-chan117":"Menhera-chan/95.jpg","Menhera-chan118":"Menhera-chan/96.jpg","Menhera-chan119":"Menhera-chan/97.jpg","Menhera-chan120":"Menhera-chan/98.jpg","Menhera-chan121":"Menhera-chan/99.jpg","Sweetie-Bunny1":"Sweetie-Bunny/12311678.png","Sweetie-Bunny2":"Sweetie-Bunny/12311679.png","Sweetie-Bunny3":"Sweetie-Bunny/12311680.png","Sweetie-Bunny4":"Sweetie-Bunny/12311681.png","Sweetie-Bunny5":"Sweetie-Bunny/12311682.png","Sweetie-Bunny6":"Sweetie-Bunny/12311683.png","Sweetie-Bunny7":"Sweetie-Bunny/12311684.png","Sweetie-Bunny8":"Sweetie-Bunny/12311685.png","Sweetie-Bunny9":"Sweetie-Bunny/12311686.png","Sweetie-Bunny10":"Sweetie-Bunny/12311687.png","Sweetie-Bunny11":"Sweetie-Bunny/12311688.png","Sweetie-Bunny12":"Sweetie-Bunny/12311689.png","Sweetie-Bunny13":"Sweetie-Bunny/12311690.png","Sweetie-Bunny14":"Sweetie-Bunny/12311691.png","Sweetie-Bunny15":"Sweetie-Bunny/12311692.png","Sweetie-Bunny16":"Sweetie-Bunny/12311693.png","Sweetie-Bunny17":"Sweetie-Bunny/12311694.png","Sweetie-Bunny18":"Sweetie-Bunny/12311695.png","Sweetie-Bunny19":"Sweetie-Bunny/12311696.png","Sweetie-Bunny20":"Sweetie-Bunny/12311697.png","Sweetie-Bunny21":"Sweetie-Bunny/12311698.png","Sweetie-Bunny22":"Sweetie-Bunny/12311699.png","Sweetie-Bunny23":"Sweetie-Bunny/12311700.png","Sweetie-Bunny24":"Sweetie-Bunny/12311701.png","Sweetie-Bunny25":"Sweetie-Bunny/12311702.png","Sweetie-Bunny26":"Sweetie-Bunny/12311703.png","Sweetie-Bunny27":"Sweetie-Bunny/12311704.png","Sweetie-Bunny28":"Sweetie-Bunny/12311705.png","Sweetie-Bunny29":"Sweetie-Bunny/12311706.png","Sweetie-Bunny30":"Sweetie-Bunny/12311707.png","Sweetie-Bunny31":"Sweetie-Bunny/12311708.png","Sweetie-Bunny32":"Sweetie-Bunny/12311709.png","Sweetie-Bunny33":"Sweetie-Bunny/12311710.png","Sweetie-Bunny34":"Sweetie-Bunny/12311711.png","Sweetie-Bunny35":"Sweetie-Bunny/12311712.png","Sweetie-Bunny36":"Sweetie-Bunny/12311713.png","Sweetie-Bunny37":"Sweetie-Bunny/12311714.png","Sweetie-Bunny38":"Sweetie-Bunny/12311715.png","Sweetie-Bunny39":"Sweetie-Bunny/12311716.png","Sweetie-Bunny40":"Sweetie-Bunny/12311717.png","Majotabi1":"Majotabi/367516718.png","Majotabi2":"Majotabi/367516719.png","Majotabi3":"Majotabi/367516720.png","Majotabi4":"Majotabi/367516721.png","Majotabi5":"Majotabi/367516722.png","Majotabi6":"Majotabi/367516723.png","Majotabi7":"Majotabi/367516724.png","Majotabi8":"Majotabi/367516725.png","Majotabi9":"Majotabi/367516726.png","Majotabi10":"Majotabi/367516727.png","Majotabi11":"Majotabi/367516728.png","Majotabi12":"Majotabi/367516729.png","Majotabi13":"Majotabi/367516730.png","Majotabi14":"Majotabi/367516731.png","Majotabi15":"Majotabi/367516732.png","Majotabi16":"Majotabi/367516733.png","Majotabi17":"Majotabi/367516734.png","Majotabi18":"Majotabi/367516735.png","Majotabi19":"Majotabi/367516736.png","Majotabi20":"Majotabi/367516737.png","Majotabi21":"Majotabi/367516738.png","Majotabi22":"Majotabi/367516739.png","Majotabi23":"Majotabi/367516740.png","Majotabi24":"Majotabi/367516741.png","Majotabi25":"Majotabi/367516742.png","Majotabi26":"Majotabi/367516743.png","Majotabi27":"Majotabi/367516744.png","Majotabi28":"Majotabi/367516745.png","Majotabi29":"Majotabi/367516746.png","Majotabi30":"Majotabi/367516747.png","Majotabi31":"Majotabi/367516748.png","Majotabi32":"Majotabi/367516749.png","Majotabi33":"Majotabi/367516750.png","Majotabi34":"Majotabi/367516751.png","Majotabi35":"Majotabi/367516752.png","Majotabi36":"Majotabi/367516753.png","Majotabi37":"Majotabi/367516754.png","Majotabi38":"Majotabi/367516755.png","Majotabi39":"Majotabi/367516756.png","Majotabi40":"Majotabi/367516757.png","Snow-Miku1":"Snow-Miku/3583066@2x.png","Snow-Miku2":"Snow-Miku/3583067@2x.png","Snow-Miku3":"Snow-Miku/3583068@2x.png","Snow-Miku4":"Snow-Miku/3583069@2x.png","Snow-Miku5":"Snow-Miku/3583070@2x.png","Snow-Miku6":"Snow-Miku/3583071@2x.png","Snow-Miku7":"Snow-Miku/3583072@2x.png","Snow-Miku8":"Snow-Miku/3583073@2x.png","Snow-Miku9":"Snow-Miku/3583074@2x.png","Snow-Miku10":"Snow-Miku/3583075@2x.png","Snow-Miku11":"Snow-Miku/3583076@2x.png","Snow-Miku12":"Snow-Miku/3583077@2x.png","Snow-Miku13":"Snow-Miku/3583078@2x.png","Snow-Miku14":"Snow-Miku/3583079@2x.png","Snow-Miku15":"Snow-Miku/3583080@2x.png","Snow-Miku16":"Snow-Miku/3583081@2x.png","Snow-Miku17":"Snow-Miku/3583082@2x.png","Snow-Miku18":"Snow-Miku/3583083@2x.png","Snow-Miku19":"Snow-Miku/3583084@2x.png","Snow-Miku20":"Snow-Miku/3583085@2x.png","Snow-Miku21":"Snow-Miku/3583086@2x.png","Snow-Miku22":"Snow-Miku/3583087@2x.png","Snow-Miku23":"Snow-Miku/3583088@2x.png","Snow-Miku24":"Snow-Miku/3583089@2x.png","Snow-Miku25":"Snow-Miku/3583090@2x.png","Snow-Miku26":"Snow-Miku/3583091@2x.png","Snow-Miku27":"Snow-Miku/3583092@2x.png","Snow-Miku28":"Snow-Miku/3583093@2x.png","Snow-Miku29":"Snow-Miku/3583094@2x.png","Snow-Miku30":"Snow-Miku/3583095@2x.png","Snow-Miku31":"Snow-Miku/3583096@2x.png","Snow-Miku32":"Snow-Miku/3583097@2x.png","Snow-Miku33":"Snow-Miku/3583098@2x.png","Snow-Miku34":"Snow-Miku/3583099@2x.png","Snow-Miku35":"Snow-Miku/3583100@2x.png","Snow-Miku36":"Snow-Miku/3583101@2x.png","Snow-Miku37":"Snow-Miku/3583102@2x.png","Snow-Miku38":"Snow-Miku/3583103@2x.png","Snow-Miku39":"Snow-Miku/3583104@2x.png","Snow-Miku40":"Snow-Miku/3583105@2x.png"},
      enableQQ: true,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign({}, initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script>(function(){
  const bp = document.createElement('script');
  const curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  bp.dataset.pjax = ''
  const s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})()</script></div></body></html>